{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW3T4AoC7bJU"
   },
   "source": [
    "# Transformer For TimeSeries Forecasting Multivariate series 1 Testset (co2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ukYMhHXMFj9"
   },
   "source": [
    "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_5_keras_transformers.ipynb\n",
    "\n",
    "https://keras.io/examples/timeseries/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QA2gkAI7yqR"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IezRVqfF7X8G"
   },
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nh7_9uwtHn9j",
    "outputId": "8103551c-4f4c-4a62-c27b-3450c49eb87d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.models import load_model\n",
    "#%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFmhe3p171U-"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "B2HbOIMdHRFf",
    "outputId": "66102f85-bea9-4d0e-98f6-661f355c21a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement_co2</th>\n",
       "      <th>co2</th>\n",
       "      <th>coal_co2</th>\n",
       "      <th>flaring_co2</th>\n",
       "      <th>gas_co2</th>\n",
       "      <th>land_use_change_co2</th>\n",
       "      <th>oil_co2</th>\n",
       "      <th>other_industry_co2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1880-01-01</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>853.705000</td>\n",
       "      <td>838.340000</td>\n",
       "      <td>256.309556</td>\n",
       "      <td>1814.698707</td>\n",
       "      <td>3100.721000</td>\n",
       "      <td>15.364000</td>\n",
       "      <td>82.069576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880-02-01</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>856.096667</td>\n",
       "      <td>840.575167</td>\n",
       "      <td>256.309556</td>\n",
       "      <td>1814.698707</td>\n",
       "      <td>3100.405500</td>\n",
       "      <td>15.520500</td>\n",
       "      <td>82.069576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880-03-01</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>858.488333</td>\n",
       "      <td>842.810333</td>\n",
       "      <td>256.309556</td>\n",
       "      <td>1814.698707</td>\n",
       "      <td>3100.090000</td>\n",
       "      <td>15.677000</td>\n",
       "      <td>82.069576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880-04-01</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>860.880000</td>\n",
       "      <td>845.045500</td>\n",
       "      <td>256.309556</td>\n",
       "      <td>1814.698707</td>\n",
       "      <td>3099.774500</td>\n",
       "      <td>15.833500</td>\n",
       "      <td>82.069576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880-05-01</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>863.271667</td>\n",
       "      <td>847.280667</td>\n",
       "      <td>256.309556</td>\n",
       "      <td>1814.698707</td>\n",
       "      <td>3099.459000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>82.069576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>1660.90700</td>\n",
       "      <td>37116.969833</td>\n",
       "      <td>14711.253333</td>\n",
       "      <td>413.545333</td>\n",
       "      <td>7799.983333</td>\n",
       "      <td>3978.615333</td>\n",
       "      <td>11622.042333</td>\n",
       "      <td>296.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>1663.82825</td>\n",
       "      <td>37118.690375</td>\n",
       "      <td>14778.339500</td>\n",
       "      <td>414.290500</td>\n",
       "      <td>7830.445000</td>\n",
       "      <td>3968.722500</td>\n",
       "      <td>11675.821500</td>\n",
       "      <td>296.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-01</th>\n",
       "      <td>1666.74950</td>\n",
       "      <td>37120.410917</td>\n",
       "      <td>14845.425667</td>\n",
       "      <td>415.035667</td>\n",
       "      <td>7860.906667</td>\n",
       "      <td>3958.829667</td>\n",
       "      <td>11729.600667</td>\n",
       "      <td>296.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>1669.67075</td>\n",
       "      <td>37122.131458</td>\n",
       "      <td>14912.511833</td>\n",
       "      <td>415.780833</td>\n",
       "      <td>7891.368333</td>\n",
       "      <td>3948.936833</td>\n",
       "      <td>11783.379833</td>\n",
       "      <td>296.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01</th>\n",
       "      <td>1672.59200</td>\n",
       "      <td>37123.852000</td>\n",
       "      <td>14979.598000</td>\n",
       "      <td>416.526000</td>\n",
       "      <td>7921.830000</td>\n",
       "      <td>3939.044000</td>\n",
       "      <td>11837.159000</td>\n",
       "      <td>296.146000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1693 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cement_co2           co2      coal_co2  flaring_co2      gas_co2  \\\n",
       "year                                                                           \n",
       "1880-01-01     0.00000    853.705000    838.340000   256.309556  1814.698707   \n",
       "1880-02-01     0.00000    856.096667    840.575167   256.309556  1814.698707   \n",
       "1880-03-01     0.00000    858.488333    842.810333   256.309556  1814.698707   \n",
       "1880-04-01     0.00000    860.880000    845.045500   256.309556  1814.698707   \n",
       "1880-05-01     0.00000    863.271667    847.280667   256.309556  1814.698707   \n",
       "...                ...           ...           ...          ...          ...   \n",
       "2020-09-01  1660.90700  37116.969833  14711.253333   413.545333  7799.983333   \n",
       "2020-10-01  1663.82825  37118.690375  14778.339500   414.290500  7830.445000   \n",
       "2020-11-01  1666.74950  37120.410917  14845.425667   415.035667  7860.906667   \n",
       "2020-12-01  1669.67075  37122.131458  14912.511833   415.780833  7891.368333   \n",
       "2021-01-01  1672.59200  37123.852000  14979.598000   416.526000  7921.830000   \n",
       "\n",
       "            land_use_change_co2       oil_co2  other_industry_co2  \n",
       "year                                                               \n",
       "1880-01-01          3100.721000     15.364000           82.069576  \n",
       "1880-02-01          3100.405500     15.520500           82.069576  \n",
       "1880-03-01          3100.090000     15.677000           82.069576  \n",
       "1880-04-01          3099.774500     15.833500           82.069576  \n",
       "1880-05-01          3099.459000     15.990000           82.069576  \n",
       "...                         ...           ...                 ...  \n",
       "2020-09-01          3978.615333  11622.042333          296.198000  \n",
       "2020-10-01          3968.722500  11675.821500          296.185000  \n",
       "2020-11-01          3958.829667  11729.600667          296.172000  \n",
       "2020-12-01          3948.936833  11783.379833          296.159000  \n",
       "2021-01-01          3939.044000  11837.159000          296.146000  \n",
       "\n",
       "[1693 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DataSets /WORLD-OWID-Features-Monthly')\n",
    "df.set_index('year', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see tha value of 2020 is an outlier compared to the other years because of corona... We will change this value with the mean of 2019 and 2021 combined. This way our future predictions/foreastings will be more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_co2 = df.loc[[2019, 2021], 'co2'].mean()\n",
    "#df.loc[2020, 'co2'] = mean_co2\n",
    "\n",
    "#df.tail(5)\n",
    "\n",
    "##   Already done when turning yearly into monthly  !!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JRtDG_sdx7D"
   },
   "source": [
    "# Defining our train, test & val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPO_kbNYH-uI",
    "outputId": "c72521fc-8f9e-445f-ee66-8870d2c5fa53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "1880-01-01 to 2000-01-01\n",
      "Test set:\n",
      "2000-02-01 to 2021-01-01\n"
     ]
    }
   ],
   "source": [
    "train_set = df[(df.index <= '2000-01-01')]\n",
    "test_set = df[(df.index > '2000-01-01')]\n",
    "\n",
    "print(\"Train set:\")\n",
    "print(train_set.index.min(), \"to\", train_set.index.max())\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(test_set.index.min(), \"to\", test_set.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jL-XpRpCLbFr"
   },
   "outputs": [],
   "source": [
    "train_data = [train_set[column].tolist() for column in train_set.columns]\n",
    "test_data = [test_set[column].tolist() for column in test_set.columns]\n",
    "# val_data = [validate_set[column].tolist() for column in validate_set.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRwUSHvpdp0O",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# defining our Window/LookBack length\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmJgDgpqIsT6",
    "outputId": "96f9f1d4-fd0f-47bc-e4b9-c9fad2517cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (1436, 8, 5)\n",
      "Shape of test set: (247, 8, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sequences(seq_size, obs_list):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs_list[0]) - seq_size):\n",
    "        window = [obs[i:i + seq_size] for obs in obs_list]\n",
    "        after_window = [obs[i + seq_size] for obs in obs_list]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "SEQUENCE_SIZE = 5\n",
    "\n",
    "train_data = [train_set[column].tolist() for column in train_set.columns]\n",
    "test_data = [test_set[column].tolist() for column in test_set.columns]\n",
    "#val_data = [validate_set[column].tolist() for column in validate_set.columns]\n",
    "\n",
    "x_train, y_train = to_sequences(SEQUENCE_SIZE, train_data)\n",
    "x_test, y_test = to_sequences(SEQUENCE_SIZE, test_data)\n",
    "#x_val, y_val = to_sequences(SEQUENCE_SIZE, val_data)\n",
    "\n",
    "print(\"Shape of training set: {}\".format(x_train.shape))\n",
    "print(\"Shape of test set: {}\".format(x_test.shape))\n",
    "#print(\"Shape of val set: {}\".format(x_val.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgq1ukssXFX9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "umZ3MBVsL-IN",
    "outputId": "5fcc635d-58c8-42f9-9a09-b43c87dc5b8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_data = np.log(train_data)\\ntest_data = np.log(test_data)\\nval_data = np.log(val_data)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_data = np.log(train_data)\n",
    "test_data = np.log(test_data)\n",
    "val_data = np.log(val_data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U4MAYRs0XEIs",
    "outputId": "b70d20f2-301c-49f6-ca11-e28e4c353021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\n\\n# Maak een scaler voor de kenmerken (X)\\nfeature_scaler = StandardScaler()\\nX_train_scaled = feature_scaler.fit_transform(x_train)\\nX_test_scaled = feature_scaler.transform(x_test)\\nX_val_scaled = feature_scaler.transform(x_val)\\n\\n# Maak een scaler voor de doelvariabelen (y)\\ntarget_scaler = StandardScaler()\\ny_train_scaled = target_scaler.fit_transform(y_train)\\ny_test_scaled = target_scaler.transform(y_test)\\ny_val_scaled = target_scaler.transform(y_val)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Maak een scaler voor de kenmerken (X)\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(x_train)\n",
    "X_test_scaled = feature_scaler.transform(x_test)\n",
    "X_val_scaled = feature_scaler.transform(x_val)\n",
    "\n",
    "# Maak een scaler voor de doelvariabelen (y)\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "y_test_scaled = target_scaler.transform(y_test)\n",
    "y_val_scaled = target_scaler.transform(y_val)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "texlTtkJXEF9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huSns1vJdlkO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f8AyGY9YU6CW"
   },
   "outputs": [],
   "source": [
    "def print_metrics(pred, y_test, model_name):\n",
    "    mae_ = mean_absolute_error(pred, y_test)\n",
    "    rmse_ = np.sqrt(mean_squared_error(pred, y_test))\n",
    "    mape_ = mean_absolute_percentage_error(pred, y_test)\n",
    "    r2_score_ = r2_score(pred, y_test)\n",
    "\n",
    "    dict_ = {\n",
    "        'MAE': mae_,\n",
    "        'RMSE': rmse_,\n",
    "        'MAPE': mape_,\n",
    "        'R2': r2_score_\n",
    "    }\n",
    "\n",
    "    metrics = pd.DataFrame(dict_, index=[model_name])\n",
    "\n",
    "    return metrics.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1NvMXr9U6Hc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keZMdH3Xfqcl"
   },
   "source": [
    "# Building the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Rj02PdYJjgM"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)    #!! \n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qlXHxLR-JjiK"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"linear\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    x = layers.Dense(16)(x)\n",
    "    outputs = layers.Dense(8)(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAc-bsd87GTy"
   },
   "source": [
    "# Using Optuna for Optimal Paramater search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "S5Qbb705Jjjx"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "  input_shape = x_train.shape[1:]\n",
    "\n",
    "  head_size = trial.suggest_int(\"head_size\", 64, 256)\n",
    "  num_heads = trial.suggest_int(\"num_heads\", 4, 8)\n",
    "  ff_dim = trial.suggest_int(\"ff_dim\", 8, 24)\n",
    "  num_transformer_blocks = trial.suggest_int(\"num_transformer_blocks\", 4, 8)\n",
    "  mlp_units = [trial.suggest_int(\"mlp_units\", 64, 128)]\n",
    "  mlp_dropout = trial.suggest_float(\"mlp_dropout\", 0.1, 0.35)\n",
    "  dropout = trial.suggest_float(\"dropout\", 0.1, 0.35)\n",
    "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
    "  n_epochs = trial.suggest_int(\"n_epochs\", 64, 200)\n",
    "  #batch_size = trial.suggest_int(\"batch_size\", 16, 128, step=16)\n",
    "\n",
    "  model = build_model(\n",
    "      input_shape,\n",
    "      head_size=head_size,\n",
    "      num_heads=num_heads,\n",
    "      ff_dim=ff_dim,\n",
    "      num_transformer_blocks=num_transformer_blocks,\n",
    "      mlp_units=mlp_units,\n",
    "      mlp_dropout=mlp_dropout,\n",
    "      dropout=dropout,\n",
    "  )\n",
    "\n",
    "  model.compile(\n",
    "      loss=\"mean_absolute_error\",\n",
    "      optimizer=keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "  )\n",
    "  model.summary()\n",
    "\n",
    "  callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "  model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_split=0.15,\n",
    "      epochs=n_epochs,\n",
    "      batch_size=64,\n",
    "      callbacks=callbacks,\n",
    "  )\n",
    "\n",
    "  loss = model.evaluate(x_test, y_test, verbose=1)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ijvwuqO7Lmp",
    "outputId": "ac6ea718-0b1f-45df-e5a1-3c5f154df35b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:44:31,137] A new study created in memory with name: no-name-334c5d5d-6b2a-4730-bedb-6d3d2c4254a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 8, 5)                 10        ['input_1[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 8, 5)                 22453     ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 8, 5)                 0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 8, 5)                 0         ['dropout[0][0]',             \n",
      " Lambda)                                                             'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add[0][0]']\n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 8, 23)                138       ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 8, 23)                0         ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 8, 5)                 120       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 8, 5)                 0         ['conv1d_1[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 8, 5)                 22453     ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 8, 5)                 0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 8, 5)                 0         ['dropout_2[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 8, 23)                138       ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 8, 23)                0         ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 8, 5)                 120       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 8, 5)                 0         ['conv1d_3[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 8, 5)                 22453     ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 8, 5)                 0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 8, 5)                 0         ['dropout_4[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 8, 23)                138       ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 8, 23)                0         ['conv1d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 8, 5)                 120       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 8, 5)                 0         ['conv1d_5[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 8, 5)                 22453     ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 8, 5)                 0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 8, 5)                 0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 8, 23)                138       ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 8, 23)                0         ['conv1d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 8, 5)                 120       ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 8, 5)                 0         ['conv1d_7[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 8)                    0         ['tf.__operators__.add_7[0][0]\n",
      " GlobalAveragePooling1D)                                            ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 125)                  1125      ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 125)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 16)                   2016      ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 8)                    136       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 94201 (367.97 KB)\n",
      "Trainable params: 94201 (367.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/163\n",
      "20/20 [==============================] - 2s 33ms/step - loss: 2664.1667 - val_loss: 8079.8271\n",
      "Epoch 2/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2503.7424 - val_loss: 7584.2383\n",
      "Epoch 3/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2365.9775 - val_loss: 7128.0781\n",
      "Epoch 4/163\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2239.5288 - val_loss: 6654.4331\n",
      "Epoch 5/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2116.4062 - val_loss: 6265.7085\n",
      "Epoch 6/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2013.3944 - val_loss: 5953.4883\n",
      "Epoch 7/163\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1901.0775 - val_loss: 5684.5112\n",
      "Epoch 8/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1793.2363 - val_loss: 5467.1133\n",
      "Epoch 9/163\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1696.2063 - val_loss: 5268.6401\n",
      "Epoch 10/163\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1585.4047 - val_loss: 5040.9746\n",
      "Epoch 11/163\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1487.7819 - val_loss: 4811.8115\n",
      "Epoch 12/163\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1397.2688 - val_loss: 4574.9590\n",
      "Epoch 13/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1316.7120 - val_loss: 4309.6558\n",
      "Epoch 14/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1235.9438 - val_loss: 4081.0295\n",
      "Epoch 15/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1162.0867 - val_loss: 3906.6379\n",
      "Epoch 16/163\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1100.6310 - val_loss: 3693.5356\n",
      "Epoch 17/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1031.1152 - val_loss: 3520.0029\n",
      "Epoch 18/163\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 979.7921 - val_loss: 3344.6187\n",
      "Epoch 19/163\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 922.8845 - val_loss: 3185.9705\n",
      "Epoch 20/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 881.4909 - val_loss: 3043.7942\n",
      "Epoch 21/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 838.9240 - val_loss: 2899.3772\n",
      "Epoch 22/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 807.9543 - val_loss: 2772.3098\n",
      "Epoch 23/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 775.7452 - val_loss: 2640.5469\n",
      "Epoch 24/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 759.2537 - val_loss: 2494.2217\n",
      "Epoch 25/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 734.3238 - val_loss: 2390.3657\n",
      "Epoch 26/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 712.0651 - val_loss: 2259.6086\n",
      "Epoch 27/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 686.9801 - val_loss: 2122.1646\n",
      "Epoch 28/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 678.1041 - val_loss: 2015.3717\n",
      "Epoch 29/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 656.2509 - val_loss: 1891.2445\n",
      "Epoch 30/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 657.3965 - val_loss: 1794.7510\n",
      "Epoch 31/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 630.5544 - val_loss: 1719.5936\n",
      "Epoch 32/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 624.2174 - val_loss: 1614.4816\n",
      "Epoch 33/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 601.3011 - val_loss: 1521.4052\n",
      "Epoch 34/163\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 592.9073 - val_loss: 1443.8616\n",
      "Epoch 35/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 574.1014 - val_loss: 1383.5552\n",
      "Epoch 36/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 580.2789 - val_loss: 1328.4259\n",
      "Epoch 37/163\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 574.1001 - val_loss: 1285.3827\n",
      "Epoch 38/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 545.9058 - val_loss: 1183.6899\n",
      "Epoch 39/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 546.7161 - val_loss: 1151.4891\n",
      "Epoch 40/163\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 535.8860 - val_loss: 1083.6306\n",
      "Epoch 41/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 519.0776 - val_loss: 1038.0975\n",
      "Epoch 42/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 506.3295 - val_loss: 954.9921\n",
      "Epoch 43/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 501.4610 - val_loss: 919.8137\n",
      "Epoch 44/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 494.8127 - val_loss: 876.9809\n",
      "Epoch 45/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 493.0734 - val_loss: 846.4366\n",
      "Epoch 46/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 474.8445 - val_loss: 828.9535\n",
      "Epoch 47/163\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 476.4503 - val_loss: 759.7633\n",
      "Epoch 48/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 467.4993 - val_loss: 735.2423\n",
      "Epoch 49/163\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 466.5127 - val_loss: 776.8685\n",
      "Epoch 50/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 458.9925 - val_loss: 702.8675\n",
      "Epoch 51/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 455.3040 - val_loss: 660.5681\n",
      "Epoch 52/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 450.1377 - val_loss: 633.0257\n",
      "Epoch 53/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 446.7237 - val_loss: 628.4622\n",
      "Epoch 54/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 437.8831 - val_loss: 625.8328\n",
      "Epoch 55/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 429.4100 - val_loss: 591.7861\n",
      "Epoch 56/163\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 426.9514 - val_loss: 577.3693\n",
      "Epoch 57/163\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 435.2259 - val_loss: 563.0851\n",
      "Epoch 58/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 423.7690 - val_loss: 530.4300\n",
      "Epoch 59/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 427.8684 - val_loss: 526.5712\n",
      "Epoch 60/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 422.1417 - val_loss: 524.3935\n",
      "Epoch 61/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 425.8088 - val_loss: 457.7719\n",
      "Epoch 62/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 414.1116 - val_loss: 545.0840\n",
      "Epoch 63/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 408.7630 - val_loss: 495.8109\n",
      "Epoch 64/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 411.1373 - val_loss: 475.8107\n",
      "Epoch 65/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 408.2214 - val_loss: 492.0714\n",
      "Epoch 66/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 404.5600 - val_loss: 463.0771\n",
      "Epoch 67/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 395.8100 - val_loss: 500.4419\n",
      "Epoch 68/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 394.5439 - val_loss: 538.6802\n",
      "Epoch 69/163\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 400.0966 - val_loss: 512.6107\n",
      "Epoch 70/163\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 389.8083 - val_loss: 455.3554\n",
      "Epoch 71/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 392.3882 - val_loss: 483.4632\n",
      "Epoch 72/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 393.8249 - val_loss: 425.1131\n",
      "Epoch 73/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 390.7442 - val_loss: 503.9325\n",
      "Epoch 74/163\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 386.2085 - val_loss: 445.3360\n",
      "Epoch 75/163\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 384.5447 - val_loss: 466.8414\n",
      "Epoch 76/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 384.6310 - val_loss: 466.6297\n",
      "Epoch 77/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 384.4849 - val_loss: 470.3596\n",
      "Epoch 78/163\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 376.8533 - val_loss: 481.7702\n",
      "Epoch 79/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 381.1604 - val_loss: 446.9576\n",
      "Epoch 80/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 372.1712 - val_loss: 437.3809\n",
      "Epoch 81/163\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 374.2217 - val_loss: 459.5149\n",
      "Epoch 82/163\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 361.4449 - val_loss: 525.6941\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1294.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:45:22,301] Trial 0 finished with value: 1294.032470703125 and parameters: {'head_size': 122, 'num_heads': 8, 'ff_dim': 23, 'num_transformer_blocks': 4, 'mlp_units': 125, 'mlp_dropout': 0.10044333051974799, 'dropout': 0.2588344397399829, 'learning_rate': 5.168877234235836e-05, 'n_epochs': 163}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 8, 5)                 10        ['input_2[0][0]']             \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 8, 5)                 13368     ['layer_normalization_8[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 8, 5)                 0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 8, 5)                 0         ['dropout_9[0][0]',           \n",
      " OpLambda)                                                           'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 8, 5)                 10        ['tf.__operators__.add_8[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 8, 16)                96        ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 8, 16)                0         ['conv1d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 8, 5)                 85        ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TF  (None, 8, 5)                 0         ['conv1d_9[0][0]',            \n",
      " OpLambda)                                                           'tf.__operators__.add_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 8, 5)                 10        ['tf.__operators__.add_9[0][0]\n",
      " yerNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 8, 5)                 13368     ['layer_normalization_10[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (T  (None, 8, 5)                 0         ['dropout_11[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 8, 5)                 10        ['tf.__operators__.add_10[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 8, 16)                0         ['conv1d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 8, 5)                 85        ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (T  (None, 8, 5)                 0         ['conv1d_11[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_10[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 8, 5)                 10        ['tf.__operators__.add_11[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 8, 5)                 13368     ['layer_normalization_12[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (T  (None, 8, 5)                 0         ['dropout_13[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_11[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 8, 5)                 10        ['tf.__operators__.add_12[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 8, 16)                0         ['conv1d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 8, 5)                 85        ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (T  (None, 8, 5)                 0         ['conv1d_13[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_12[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 8, 5)                 10        ['tf.__operators__.add_13[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 8, 5)                 13368     ['layer_normalization_14[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (T  (None, 8, 5)                 0         ['dropout_15[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_13[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 8, 5)                 10        ['tf.__operators__.add_14[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 8, 16)                0         ['conv1d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 8, 5)                 85        ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (T  (None, 8, 5)                 0         ['conv1d_15[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_14[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 8, 5)                 10        ['tf.__operators__.add_15[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (Mu  (None, 8, 5)                 13368     ['layer_normalization_16[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (T  (None, 8, 5)                 0         ['dropout_17[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_15[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 8, 5)                 10        ['tf.__operators__.add_16[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)        (None, 8, 16)                0         ['conv1d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 8, 5)                 85        ['dropout_18[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (T  (None, 8, 5)                 0         ['conv1d_17[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_16[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_18 (La  (None, 8, 5)                 10        ['tf.__operators__.add_17[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (Mu  (None, 8, 5)                 13368     ['layer_normalization_18[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (T  (None, 8, 5)                 0         ['dropout_19[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_17[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_19 (La  (None, 8, 5)                 10        ['tf.__operators__.add_18[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)        (None, 8, 16)                0         ['conv1d_18[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 8, 5)                 85        ['dropout_20[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (T  (None, 8, 5)                 0         ['conv1d_19[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_18[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_19[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 71)                   639       ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)        (None, 71)                   0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 16)                   1152      ['dropout_21[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 8)                    136       ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 83341 (325.55 KB)\n",
      "Trainable params: 83341 (325.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/72\n",
      "20/20 [==============================] - 2s 35ms/step - loss: 3720.4138 - val_loss: 10448.1826\n",
      "Epoch 2/72\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 3682.4016 - val_loss: 10423.7178\n",
      "Epoch 3/72\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 3689.6372 - val_loss: 10398.4141\n",
      "Epoch 4/72\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 3665.1116 - val_loss: 10374.2773\n",
      "Epoch 5/72\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 3671.3755 - val_loss: 10350.1875\n",
      "Epoch 6/72\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 3676.3381 - val_loss: 10326.1992\n",
      "Epoch 7/72\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 3651.7156 - val_loss: 10300.7900\n",
      "Epoch 8/72\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 3640.8833 - val_loss: 10275.6064\n",
      "Epoch 9/72\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 3645.2827 - val_loss: 10250.6885\n",
      "Epoch 10/72\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 3612.7429 - val_loss: 10225.2568\n",
      "Epoch 11/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3613.0012 - val_loss: 10200.9189\n",
      "Epoch 12/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3611.4861 - val_loss: 10176.4541\n",
      "Epoch 13/72\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 3615.1172 - val_loss: 10151.5762\n",
      "Epoch 14/72\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 3621.8948 - val_loss: 10128.1006\n",
      "Epoch 15/72\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 3601.7227 - val_loss: 10104.3730\n",
      "Epoch 16/72\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 3604.5110 - val_loss: 10080.5449\n",
      "Epoch 17/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3601.1160 - val_loss: 10056.5000\n",
      "Epoch 18/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3594.0706 - val_loss: 10030.7021\n",
      "Epoch 19/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3586.1128 - val_loss: 10007.2412\n",
      "Epoch 20/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3565.3845 - val_loss: 9983.7480\n",
      "Epoch 21/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3556.6443 - val_loss: 9961.1738\n",
      "Epoch 22/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3557.8247 - val_loss: 9938.3477\n",
      "Epoch 23/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3557.3250 - val_loss: 9915.3623\n",
      "Epoch 24/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3553.0720 - val_loss: 9893.2422\n",
      "Epoch 25/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3554.7983 - val_loss: 9871.1299\n",
      "Epoch 26/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3522.2373 - val_loss: 9847.3320\n",
      "Epoch 27/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3503.2200 - val_loss: 9824.1172\n",
      "Epoch 28/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3515.7417 - val_loss: 9800.7275\n",
      "Epoch 29/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3520.3032 - val_loss: 9777.7822\n",
      "Epoch 30/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3507.5767 - val_loss: 9755.9961\n",
      "Epoch 31/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3493.1602 - val_loss: 9733.5186\n",
      "Epoch 32/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3483.2429 - val_loss: 9711.0693\n",
      "Epoch 33/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3480.5557 - val_loss: 9688.4307\n",
      "Epoch 34/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3478.4446 - val_loss: 9665.5801\n",
      "Epoch 35/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3475.4241 - val_loss: 9643.0674\n",
      "Epoch 36/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3449.6394 - val_loss: 9619.8047\n",
      "Epoch 37/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3441.1294 - val_loss: 9597.8984\n",
      "Epoch 38/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3451.5569 - val_loss: 9574.5352\n",
      "Epoch 39/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3432.1196 - val_loss: 9551.4619\n",
      "Epoch 40/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3421.8872 - val_loss: 9530.1445\n",
      "Epoch 41/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3427.4802 - val_loss: 9507.1377\n",
      "Epoch 42/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3419.0159 - val_loss: 9484.9883\n",
      "Epoch 43/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3416.4575 - val_loss: 9462.7764\n",
      "Epoch 44/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3403.4507 - val_loss: 9440.8750\n",
      "Epoch 45/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3402.5376 - val_loss: 9418.2891\n",
      "Epoch 46/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3408.3762 - val_loss: 9396.4121\n",
      "Epoch 47/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3390.4207 - val_loss: 9374.1260\n",
      "Epoch 48/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3384.0325 - val_loss: 9353.0068\n",
      "Epoch 49/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3368.8638 - val_loss: 9331.4580\n",
      "Epoch 50/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3374.7439 - val_loss: 9309.1104\n",
      "Epoch 51/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3362.8381 - val_loss: 9287.0039\n",
      "Epoch 52/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3351.3557 - val_loss: 9264.1230\n",
      "Epoch 53/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3368.4509 - val_loss: 9240.7480\n",
      "Epoch 54/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3318.5522 - val_loss: 9218.8721\n",
      "Epoch 55/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3324.0447 - val_loss: 9195.4336\n",
      "Epoch 56/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3350.8408 - val_loss: 9173.4590\n",
      "Epoch 57/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3327.1460 - val_loss: 9150.5791\n",
      "Epoch 58/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3321.3430 - val_loss: 9128.9365\n",
      "Epoch 59/72\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3312.9905 - val_loss: 9106.8105\n",
      "Epoch 60/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3301.3308 - val_loss: 9084.2773\n",
      "Epoch 61/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3296.3840 - val_loss: 9061.8271\n",
      "Epoch 62/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3305.0503 - val_loss: 9039.2783\n",
      "Epoch 63/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3264.3577 - val_loss: 9017.9609\n",
      "Epoch 64/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3283.0066 - val_loss: 8997.1709\n",
      "Epoch 65/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3258.6587 - val_loss: 8976.4355\n",
      "Epoch 66/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3280.6450 - val_loss: 8955.0225\n",
      "Epoch 67/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3270.3591 - val_loss: 8933.8896\n",
      "Epoch 68/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3260.4448 - val_loss: 8911.9062\n",
      "Epoch 69/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3241.4255 - val_loss: 8890.6113\n",
      "Epoch 70/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3244.0815 - val_loss: 8870.1309\n",
      "Epoch 71/72\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3242.1638 - val_loss: 8849.1084\n",
      "Epoch 72/72\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3237.2625 - val_loss: 8828.2617\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12303.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:46:11,985] Trial 1 finished with value: 12303.41015625 and parameters: {'head_size': 83, 'num_heads': 7, 'ff_dim': 16, 'num_transformer_blocks': 6, 'mlp_units': 71, 'mlp_dropout': 0.1918807652624585, 'dropout': 0.26468056009909036, 'learning_rate': 2.7045490632128686e-06, 'n_epochs': 72}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_20 (La  (None, 8, 5)                 10        ['input_3[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (M  (None, 8, 5)                 22545     ['layer_normalization_20[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_10[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (T  (None, 8, 5)                 0         ['dropout_22[0][0]',          \n",
      " FOpLambda)                                                          'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_21 (La  (None, 8, 5)                 10        ['tf.__operators__.add_20[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)        (None, 8, 20)                0         ['conv1d_20[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)          (None, 8, 5)                 105       ['dropout_23[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (T  (None, 8, 5)                 0         ['conv1d_21[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_20[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_22 (La  (None, 8, 5)                 10        ['tf.__operators__.add_21[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (M  (None, 8, 5)                 22545     ['layer_normalization_22[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_11[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (T  (None, 8, 5)                 0         ['dropout_24[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_21[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_23 (La  (None, 8, 5)                 10        ['tf.__operators__.add_22[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)        (None, 8, 20)                0         ['conv1d_22[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)          (None, 8, 5)                 105       ['dropout_25[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (T  (None, 8, 5)                 0         ['conv1d_23[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_24 (La  (None, 8, 5)                 10        ['tf.__operators__.add_23[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (M  (None, 8, 5)                 22545     ['layer_normalization_24[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_12[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (T  (None, 8, 5)                 0         ['dropout_26[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_23[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_25 (La  (None, 8, 5)                 10        ['tf.__operators__.add_24[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)        (None, 8, 20)                0         ['conv1d_24[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)          (None, 8, 5)                 105       ['dropout_27[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (T  (None, 8, 5)                 0         ['conv1d_25[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_24[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_26 (La  (None, 8, 5)                 10        ['tf.__operators__.add_25[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (M  (None, 8, 5)                 22545     ['layer_normalization_26[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_13[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (T  (None, 8, 5)                 0         ['dropout_28[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_25[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_27 (La  (None, 8, 5)                 10        ['tf.__operators__.add_26[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)        (None, 8, 20)                0         ['conv1d_26[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)          (None, 8, 5)                 105       ['dropout_29[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (T  (None, 8, 5)                 0         ['conv1d_27[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_26[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_28 (La  (None, 8, 5)                 10        ['tf.__operators__.add_27[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (M  (None, 8, 5)                 22545     ['layer_normalization_28[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_14[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (T  (None, 8, 5)                 0         ['dropout_30[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_27[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_29 (La  (None, 8, 5)                 10        ['tf.__operators__.add_28[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)        (None, 8, 20)                0         ['conv1d_28[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)          (None, 8, 5)                 105       ['dropout_31[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (T  (None, 8, 5)                 0         ['conv1d_29[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_28[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_30 (La  (None, 8, 5)                 10        ['tf.__operators__.add_29[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (M  (None, 8, 5)                 22545     ['layer_normalization_30[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_15[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (T  (None, 8, 5)                 0         ['dropout_32[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_29[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_31 (La  (None, 8, 5)                 10        ['tf.__operators__.add_30[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_31[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)        (None, 8, 20)                0         ['conv1d_30[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)          (None, 8, 5)                 105       ['dropout_33[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (T  (None, 8, 5)                 0         ['conv1d_31[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_30[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_32 (La  (None, 8, 5)                 10        ['tf.__operators__.add_31[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (M  (None, 8, 5)                 22545     ['layer_normalization_32[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_16[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (T  (None, 8, 5)                 0         ['dropout_34[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_31[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_33 (La  (None, 8, 5)                 10        ['tf.__operators__.add_32[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)        (None, 8, 20)                0         ['conv1d_32[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)          (None, 8, 5)                 105       ['dropout_35[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (T  (None, 8, 5)                 0         ['conv1d_33[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_32[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_34 (La  (None, 8, 5)                 10        ['tf.__operators__.add_33[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (M  (None, 8, 5)                 22545     ['layer_normalization_34[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_17[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (T  (None, 8, 5)                 0         ['dropout_36[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_33[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_35 (La  (None, 8, 5)                 10        ['tf.__operators__.add_34[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)          (None, 8, 20)                120       ['layer_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 8, 20)                0         ['conv1d_34[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)          (None, 8, 5)                 105       ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (T  (None, 8, 5)                 0         ['conv1d_35[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_34[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_35[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 101)                  909       ['global_average_pooling1d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)        (None, 101)                  0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 16)                   1632      ['dropout_38[0][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 8)                    136       ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 184997 (722.64 KB)\n",
      "Trainable params: 184997 (722.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/65\n",
      "20/20 [==============================] - 3s 57ms/step - loss: 2969.6628 - val_loss: 7861.1587\n",
      "Epoch 2/65\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 2723.9851 - val_loss: 7240.7632\n",
      "Epoch 3/65\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2521.6011 - val_loss: 6745.1055\n",
      "Epoch 4/65\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2374.7095 - val_loss: 6311.5854\n",
      "Epoch 5/65\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2175.6436 - val_loss: 5884.9448\n",
      "Epoch 6/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2039.0392 - val_loss: 5501.5332\n",
      "Epoch 7/65\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 1908.5682 - val_loss: 5189.3032\n",
      "Epoch 8/65\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 1756.5891 - val_loss: 4853.4170\n",
      "Epoch 9/65\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 1652.5770 - val_loss: 4541.7544\n",
      "Epoch 10/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1545.4707 - val_loss: 4184.5376\n",
      "Epoch 11/65\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 1453.4587 - val_loss: 3856.3271\n",
      "Epoch 12/65\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 1398.2957 - val_loss: 3531.5928\n",
      "Epoch 13/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 1337.0048 - val_loss: 3293.3831\n",
      "Epoch 14/65\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 1283.0903 - val_loss: 3027.4065\n",
      "Epoch 15/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1222.0992 - val_loss: 2822.3943\n",
      "Epoch 16/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1174.5491 - val_loss: 2614.2080\n",
      "Epoch 17/65\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 1160.6910 - val_loss: 2408.4998\n",
      "Epoch 18/65\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 1125.6578 - val_loss: 2282.6824\n",
      "Epoch 19/65\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 1087.7533 - val_loss: 2117.1147\n",
      "Epoch 20/65\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 1065.3035 - val_loss: 1999.0117\n",
      "Epoch 21/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 1045.1473 - val_loss: 1845.8276\n",
      "Epoch 22/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 1012.3107 - val_loss: 1742.5341\n",
      "Epoch 23/65\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 990.1946 - val_loss: 1547.3292\n",
      "Epoch 24/65\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 978.5080 - val_loss: 1391.1486\n",
      "Epoch 25/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 945.6293 - val_loss: 1342.8267\n",
      "Epoch 26/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 928.0915 - val_loss: 1304.6586\n",
      "Epoch 27/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 913.3075 - val_loss: 1255.2339\n",
      "Epoch 28/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 911.4099 - val_loss: 1237.3735\n",
      "Epoch 29/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 883.4804 - val_loss: 1152.7156\n",
      "Epoch 30/65\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 854.7485 - val_loss: 1143.0977\n",
      "Epoch 31/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 859.7115 - val_loss: 1026.5945\n",
      "Epoch 32/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 852.6322 - val_loss: 931.4400\n",
      "Epoch 33/65\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 827.5824 - val_loss: 921.4144\n",
      "Epoch 34/65\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 799.9619 - val_loss: 787.7465\n",
      "Epoch 35/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 805.1716 - val_loss: 753.4778\n",
      "Epoch 36/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 789.5352 - val_loss: 786.8807\n",
      "Epoch 37/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 769.1188 - val_loss: 699.2427\n",
      "Epoch 38/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 766.7540 - val_loss: 676.8721\n",
      "Epoch 39/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 745.6309 - val_loss: 682.3878\n",
      "Epoch 40/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 739.2381 - val_loss: 602.3807\n",
      "Epoch 41/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 720.4087 - val_loss: 598.8040\n",
      "Epoch 42/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 725.6663 - val_loss: 578.6585\n",
      "Epoch 43/65\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 719.7154 - val_loss: 604.2877\n",
      "Epoch 44/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 706.1978 - val_loss: 543.7146\n",
      "Epoch 45/65\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 703.0132 - val_loss: 568.4153\n",
      "Epoch 46/65\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 684.0943 - val_loss: 624.3914\n",
      "Epoch 47/65\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 677.3314 - val_loss: 515.4581\n",
      "Epoch 48/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 660.2202 - val_loss: 448.4414\n",
      "Epoch 49/65\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 666.6323 - val_loss: 407.8588\n",
      "Epoch 50/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 653.0745 - val_loss: 494.4188\n",
      "Epoch 51/65\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 642.5425 - val_loss: 453.2570\n",
      "Epoch 52/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 630.0305 - val_loss: 516.4114\n",
      "Epoch 53/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 631.2811 - val_loss: 463.6199\n",
      "Epoch 54/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 622.8094 - val_loss: 499.9995\n",
      "Epoch 55/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 611.9960 - val_loss: 528.0076\n",
      "Epoch 56/65\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 605.2449 - val_loss: 531.3995\n",
      "Epoch 57/65\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 591.6254 - val_loss: 444.6882\n",
      "Epoch 58/65\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 595.5051 - val_loss: 427.3737\n",
      "Epoch 59/65\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 589.5895 - val_loss: 442.2762\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1333.9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:47:19,728] Trial 2 finished with value: 1333.9058837890625 and parameters: {'head_size': 140, 'num_heads': 7, 'ff_dim': 20, 'num_transformer_blocks': 8, 'mlp_units': 101, 'mlp_dropout': 0.3035796771926379, 'dropout': 0.1385924664607595, 'learning_rate': 8.748067918463609e-05, 'n_epochs': 65}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_36 (La  (None, 8, 5)                 10        ['input_4[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (M  (None, 8, 5)                 26892     ['layer_normalization_36[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_18[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (T  (None, 8, 5)                 0         ['dropout_39[0][0]',          \n",
      " FOpLambda)                                                          'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_37 (La  (None, 8, 5)                 10        ['tf.__operators__.add_36[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)        (None, 8, 16)                0         ['conv1d_36[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)          (None, 8, 5)                 85        ['dropout_40[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (T  (None, 8, 5)                 0         ['conv1d_37[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_36[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_38 (La  (None, 8, 5)                 10        ['tf.__operators__.add_37[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (M  (None, 8, 5)                 26892     ['layer_normalization_38[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_19[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (T  (None, 8, 5)                 0         ['dropout_41[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_37[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_39 (La  (None, 8, 5)                 10        ['tf.__operators__.add_38[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)        (None, 8, 16)                0         ['conv1d_38[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)          (None, 8, 5)                 85        ['dropout_42[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (T  (None, 8, 5)                 0         ['conv1d_39[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_38[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_40 (La  (None, 8, 5)                 10        ['tf.__operators__.add_39[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (M  (None, 8, 5)                 26892     ['layer_normalization_40[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_20[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_40 (T  (None, 8, 5)                 0         ['dropout_43[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_39[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_41 (La  (None, 8, 5)                 10        ['tf.__operators__.add_40[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)        (None, 8, 16)                0         ['conv1d_40[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)          (None, 8, 5)                 85        ['dropout_44[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_41 (T  (None, 8, 5)                 0         ['conv1d_41[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_40[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_42 (La  (None, 8, 5)                 10        ['tf.__operators__.add_41[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_21 (M  (None, 8, 5)                 26892     ['layer_normalization_42[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_21[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_42 (T  (None, 8, 5)                 0         ['dropout_45[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_41[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_43 (La  (None, 8, 5)                 10        ['tf.__operators__.add_42[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_42 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)        (None, 8, 16)                0         ['conv1d_42[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)          (None, 8, 5)                 85        ['dropout_46[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_43 (T  (None, 8, 5)                 0         ['conv1d_43[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_42[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_44 (La  (None, 8, 5)                 10        ['tf.__operators__.add_43[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (M  (None, 8, 5)                 26892     ['layer_normalization_44[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_44 (T  (None, 8, 5)                 0         ['dropout_47[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_43[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_45 (La  (None, 8, 5)                 10        ['tf.__operators__.add_44[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)          (None, 8, 16)                96        ['layer_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)        (None, 8, 16)                0         ['conv1d_44[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_45 (Conv1D)          (None, 8, 5)                 85        ['dropout_48[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_45 (T  (None, 8, 5)                 0         ['conv1d_45[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_44[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_45[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 118)                  1062      ['global_average_pooling1d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)        (None, 118)                  0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 16)                   1904      ['dropout_49[0][0]']          \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 8)                    136       ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 138567 (541.28 KB)\n",
      "Trainable params: 138567 (541.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/152\n",
      "20/20 [==============================] - 2s 43ms/step - loss: 3332.2432 - val_loss: 9516.3008\n",
      "Epoch 2/152\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 3283.4395 - val_loss: 9348.1289\n",
      "Epoch 3/152\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3214.1851 - val_loss: 9187.7197\n",
      "Epoch 4/152\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3157.7893 - val_loss: 9030.8633\n",
      "Epoch 5/152\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3103.7366 - val_loss: 8872.3389\n",
      "Epoch 6/152\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 3058.8884 - val_loss: 8715.1406\n",
      "Epoch 7/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2987.4106 - val_loss: 8563.5205\n",
      "Epoch 8/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2944.7866 - val_loss: 8413.3486\n",
      "Epoch 9/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2899.3237 - val_loss: 8265.0635\n",
      "Epoch 10/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2859.0361 - val_loss: 8119.3657\n",
      "Epoch 11/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2800.9653 - val_loss: 7983.3926\n",
      "Epoch 12/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2763.4644 - val_loss: 7869.9839\n",
      "Epoch 13/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2707.7034 - val_loss: 7758.6006\n",
      "Epoch 14/152\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2674.6318 - val_loss: 7650.2402\n",
      "Epoch 15/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2615.1230 - val_loss: 7546.2993\n",
      "Epoch 16/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2589.3647 - val_loss: 7442.2627\n",
      "Epoch 17/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2535.6306 - val_loss: 7338.0562\n",
      "Epoch 18/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2499.2583 - val_loss: 7234.1133\n",
      "Epoch 19/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2458.3743 - val_loss: 7126.6782\n",
      "Epoch 20/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2426.2825 - val_loss: 7021.2354\n",
      "Epoch 21/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2379.7961 - val_loss: 6915.7383\n",
      "Epoch 22/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2338.8323 - val_loss: 6803.4082\n",
      "Epoch 23/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2298.9944 - val_loss: 6693.3975\n",
      "Epoch 24/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2262.3374 - val_loss: 6580.4863\n",
      "Epoch 25/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2238.3884 - val_loss: 6470.8530\n",
      "Epoch 26/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2186.1052 - val_loss: 6355.7788\n",
      "Epoch 27/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2150.8416 - val_loss: 6239.5024\n",
      "Epoch 28/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2109.3872 - val_loss: 6131.1689\n",
      "Epoch 29/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2068.7930 - val_loss: 6020.8511\n",
      "Epoch 30/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2041.7433 - val_loss: 5904.2725\n",
      "Epoch 31/152\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2009.4109 - val_loss: 5794.9600\n",
      "Epoch 32/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1976.7902 - val_loss: 5687.0728\n",
      "Epoch 33/152\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1950.7240 - val_loss: 5575.7871\n",
      "Epoch 34/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1914.9084 - val_loss: 5469.1074\n",
      "Epoch 35/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1884.1465 - val_loss: 5365.2026\n",
      "Epoch 36/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1861.7325 - val_loss: 5259.8779\n",
      "Epoch 37/152\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1820.5988 - val_loss: 5162.9121\n",
      "Epoch 38/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1798.3451 - val_loss: 5061.5083\n",
      "Epoch 39/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1769.4611 - val_loss: 4964.0254\n",
      "Epoch 40/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1743.2627 - val_loss: 4862.6943\n",
      "Epoch 41/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1716.9932 - val_loss: 4770.1138\n",
      "Epoch 42/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1682.8171 - val_loss: 4674.0044\n",
      "Epoch 43/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1648.6381 - val_loss: 4575.0176\n",
      "Epoch 44/152\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1636.5386 - val_loss: 4484.2251\n",
      "Epoch 45/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1606.3280 - val_loss: 4391.6738\n",
      "Epoch 46/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1584.2417 - val_loss: 4304.0962\n",
      "Epoch 47/152\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1558.9694 - val_loss: 4218.1777\n",
      "Epoch 48/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1540.2886 - val_loss: 4135.0225\n",
      "Epoch 49/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1518.9324 - val_loss: 4053.7297\n",
      "Epoch 50/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1489.4092 - val_loss: 3973.4583\n",
      "Epoch 51/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1465.9264 - val_loss: 3903.8428\n",
      "Epoch 52/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1451.1562 - val_loss: 3846.8208\n",
      "Epoch 53/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1426.3315 - val_loss: 3790.2239\n",
      "Epoch 54/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1400.2028 - val_loss: 3737.9004\n",
      "Epoch 55/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1378.1060 - val_loss: 3690.2812\n",
      "Epoch 56/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1361.1887 - val_loss: 3647.6736\n",
      "Epoch 57/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1346.9305 - val_loss: 3601.0647\n",
      "Epoch 58/152\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1330.4984 - val_loss: 3559.5046\n",
      "Epoch 59/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1306.3665 - val_loss: 3518.7043\n",
      "Epoch 60/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1287.0483 - val_loss: 3474.0112\n",
      "Epoch 61/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1273.5093 - val_loss: 3434.9119\n",
      "Epoch 62/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1251.2166 - val_loss: 3403.9746\n",
      "Epoch 63/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1239.6058 - val_loss: 3369.6670\n",
      "Epoch 64/152\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1226.4268 - val_loss: 3331.2690\n",
      "Epoch 65/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1202.4484 - val_loss: 3287.3201\n",
      "Epoch 66/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1184.7280 - val_loss: 3247.0859\n",
      "Epoch 67/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1186.6161 - val_loss: 3214.0171\n",
      "Epoch 68/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1168.6512 - val_loss: 3187.1453\n",
      "Epoch 69/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1154.8591 - val_loss: 3146.2036\n",
      "Epoch 70/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1130.8081 - val_loss: 3109.6707\n",
      "Epoch 71/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1124.4432 - val_loss: 3070.6160\n",
      "Epoch 72/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1105.8610 - val_loss: 3035.6863\n",
      "Epoch 73/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1080.9517 - val_loss: 3000.4817\n",
      "Epoch 74/152\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1071.5573 - val_loss: 2965.9800\n",
      "Epoch 75/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1066.6075 - val_loss: 2930.4504\n",
      "Epoch 76/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1057.1453 - val_loss: 2886.3079\n",
      "Epoch 77/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1050.9064 - val_loss: 2856.3564\n",
      "Epoch 78/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1034.4883 - val_loss: 2826.9941\n",
      "Epoch 79/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1021.8181 - val_loss: 2799.5198\n",
      "Epoch 80/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1017.3667 - val_loss: 2775.8845\n",
      "Epoch 81/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1008.4359 - val_loss: 2749.2021\n",
      "Epoch 82/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 992.8372 - val_loss: 2719.1467\n",
      "Epoch 83/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 980.8028 - val_loss: 2690.5420\n",
      "Epoch 84/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 968.1450 - val_loss: 2662.2778\n",
      "Epoch 85/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 952.9630 - val_loss: 2626.5295\n",
      "Epoch 86/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 957.3706 - val_loss: 2602.5535\n",
      "Epoch 87/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 942.6326 - val_loss: 2580.7185\n",
      "Epoch 88/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 931.0817 - val_loss: 2554.2615\n",
      "Epoch 89/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 915.6092 - val_loss: 2525.9302\n",
      "Epoch 90/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 910.6336 - val_loss: 2499.8184\n",
      "Epoch 91/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 904.9635 - val_loss: 2467.5449\n",
      "Epoch 92/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 903.4655 - val_loss: 2439.0239\n",
      "Epoch 93/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 897.7393 - val_loss: 2411.2527\n",
      "Epoch 94/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 867.1585 - val_loss: 2393.7542\n",
      "Epoch 95/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 867.5815 - val_loss: 2373.5576\n",
      "Epoch 96/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 859.8748 - val_loss: 2348.5183\n",
      "Epoch 97/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 859.3172 - val_loss: 2326.3396\n",
      "Epoch 98/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 854.9576 - val_loss: 2300.3818\n",
      "Epoch 99/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 833.7435 - val_loss: 2275.3889\n",
      "Epoch 100/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 823.6341 - val_loss: 2252.5227\n",
      "Epoch 101/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 825.0107 - val_loss: 2225.9385\n",
      "Epoch 102/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 816.7798 - val_loss: 2199.9497\n",
      "Epoch 103/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 814.3785 - val_loss: 2176.1162\n",
      "Epoch 104/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 802.7899 - val_loss: 2149.5508\n",
      "Epoch 105/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 798.6484 - val_loss: 2129.9565\n",
      "Epoch 106/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 788.1616 - val_loss: 2117.9402\n",
      "Epoch 107/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 776.3981 - val_loss: 2099.3411\n",
      "Epoch 108/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 774.2664 - val_loss: 2075.6985\n",
      "Epoch 109/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 765.1474 - val_loss: 2054.1060\n",
      "Epoch 110/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 775.7832 - val_loss: 2028.7665\n",
      "Epoch 111/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 758.6210 - val_loss: 2011.6904\n",
      "Epoch 112/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 755.0804 - val_loss: 1991.8213\n",
      "Epoch 113/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 748.5671 - val_loss: 1979.4849\n",
      "Epoch 114/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 741.9446 - val_loss: 1963.5369\n",
      "Epoch 115/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 732.0961 - val_loss: 1940.3202\n",
      "Epoch 116/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 727.8874 - val_loss: 1910.8850\n",
      "Epoch 117/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 734.6370 - val_loss: 1895.3934\n",
      "Epoch 118/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 713.8071 - val_loss: 1878.3510\n",
      "Epoch 119/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 705.6449 - val_loss: 1852.8695\n",
      "Epoch 120/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 704.6692 - val_loss: 1828.7814\n",
      "Epoch 121/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 692.4512 - val_loss: 1810.6115\n",
      "Epoch 122/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 699.3579 - val_loss: 1792.7709\n",
      "Epoch 123/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 681.7662 - val_loss: 1769.6018\n",
      "Epoch 124/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 681.7894 - val_loss: 1748.3667\n",
      "Epoch 125/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 682.0566 - val_loss: 1730.7908\n",
      "Epoch 126/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 670.1582 - val_loss: 1710.3995\n",
      "Epoch 127/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 670.7196 - val_loss: 1684.5956\n",
      "Epoch 128/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 665.5138 - val_loss: 1660.7408\n",
      "Epoch 129/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 667.8194 - val_loss: 1638.1167\n",
      "Epoch 130/152\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 659.8099 - val_loss: 1609.7697\n",
      "Epoch 131/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 659.2196 - val_loss: 1580.8229\n",
      "Epoch 132/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 645.8700 - val_loss: 1560.6736\n",
      "Epoch 133/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 644.5649 - val_loss: 1541.7354\n",
      "Epoch 134/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 646.5493 - val_loss: 1523.1886\n",
      "Epoch 135/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 641.4030 - val_loss: 1502.7135\n",
      "Epoch 136/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 619.5057 - val_loss: 1476.1489\n",
      "Epoch 137/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 626.0540 - val_loss: 1445.4396\n",
      "Epoch 138/152\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 619.7910 - val_loss: 1413.4208\n",
      "Epoch 139/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 608.6382 - val_loss: 1390.6360\n",
      "Epoch 140/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 610.5052 - val_loss: 1369.1501\n",
      "Epoch 141/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 605.5998 - val_loss: 1343.4170\n",
      "Epoch 142/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 613.0876 - val_loss: 1326.2837\n",
      "Epoch 143/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 597.8506 - val_loss: 1292.4109\n",
      "Epoch 144/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 600.1263 - val_loss: 1266.2782\n",
      "Epoch 145/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 597.1901 - val_loss: 1249.4966\n",
      "Epoch 146/152\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 598.8279 - val_loss: 1228.0701\n",
      "Epoch 147/152\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 589.3319 - val_loss: 1198.9584\n",
      "Epoch 148/152\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 585.2425 - val_loss: 1174.2665\n",
      "Epoch 149/152\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 587.2065 - val_loss: 1145.4188\n",
      "Epoch 150/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 572.9874 - val_loss: 1125.1564\n",
      "Epoch 151/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 591.2034 - val_loss: 1106.0065\n",
      "Epoch 152/152\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 576.5555 - val_loss: 1087.1487\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2364.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:49:15,738] Trial 3 finished with value: 2364.010009765625 and parameters: {'head_size': 167, 'num_heads': 7, 'ff_dim': 16, 'num_transformer_blocks': 5, 'mlp_units': 118, 'mlp_dropout': 0.1137610312106131, 'dropout': 0.1727810377371068, 'learning_rate': 1.3569089341209295e-05, 'n_epochs': 152}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_46 (La  (None, 8, 5)                 10        ['input_5[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_23 (M  (None, 8, 5)                 22200     ['layer_normalization_46[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_23[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_46 (T  (None, 8, 5)                 0         ['dropout_50[0][0]',          \n",
      " FOpLambda)                                                          'input_5[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_47 (La  (None, 8, 5)                 10        ['tf.__operators__.add_46[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_46 (Conv1D)          (None, 8, 23)                138       ['layer_normalization_47[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)        (None, 8, 23)                0         ['conv1d_46[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)          (None, 8, 5)                 120       ['dropout_51[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_47 (T  (None, 8, 5)                 0         ['conv1d_47[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_46[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_48 (La  (None, 8, 5)                 10        ['tf.__operators__.add_47[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_24 (M  (None, 8, 5)                 22200     ['layer_normalization_48[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_24[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_48 (T  (None, 8, 5)                 0         ['dropout_52[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_47[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_49 (La  (None, 8, 5)                 10        ['tf.__operators__.add_48[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_48 (Conv1D)          (None, 8, 23)                138       ['layer_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)        (None, 8, 23)                0         ['conv1d_48[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_49 (Conv1D)          (None, 8, 5)                 120       ['dropout_53[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_49 (T  (None, 8, 5)                 0         ['conv1d_49[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_48[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_50 (La  (None, 8, 5)                 10        ['tf.__operators__.add_49[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_25 (M  (None, 8, 5)                 22200     ['layer_normalization_50[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_25[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_50 (T  (None, 8, 5)                 0         ['dropout_54[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_49[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_51 (La  (None, 8, 5)                 10        ['tf.__operators__.add_50[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_50 (Conv1D)          (None, 8, 23)                138       ['layer_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_55 (Dropout)        (None, 8, 23)                0         ['conv1d_50[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_51 (Conv1D)          (None, 8, 5)                 120       ['dropout_55[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_51 (T  (None, 8, 5)                 0         ['conv1d_51[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_50[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_52 (La  (None, 8, 5)                 10        ['tf.__operators__.add_51[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_26 (M  (None, 8, 5)                 22200     ['layer_normalization_52[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_56 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_26[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_52 (T  (None, 8, 5)                 0         ['dropout_56[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_51[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_53 (La  (None, 8, 5)                 10        ['tf.__operators__.add_52[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_52 (Conv1D)          (None, 8, 23)                138       ['layer_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_57 (Dropout)        (None, 8, 23)                0         ['conv1d_52[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_53 (Conv1D)          (None, 8, 5)                 120       ['dropout_57[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_53 (T  (None, 8, 5)                 0         ['conv1d_53[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_52[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_54 (La  (None, 8, 5)                 10        ['tf.__operators__.add_53[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_27 (M  (None, 8, 5)                 22200     ['layer_normalization_54[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_58 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_27[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_54 (T  (None, 8, 5)                 0         ['dropout_58[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_53[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_55 (La  (None, 8, 5)                 10        ['tf.__operators__.add_54[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_54 (Conv1D)          (None, 8, 23)                138       ['layer_normalization_55[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_59 (Dropout)        (None, 8, 23)                0         ['conv1d_54[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_55 (Conv1D)          (None, 8, 5)                 120       ['dropout_59[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_55 (T  (None, 8, 5)                 0         ['conv1d_55[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_54[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_55[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 99)                   891       ['global_average_pooling1d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_60 (Dropout)        (None, 99)                   0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 16)                   1600      ['dropout_60[0][0]']          \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 8)                    136       ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 115017 (449.29 KB)\n",
      "Trainable params: 115017 (449.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/188\n",
      "20/20 [==============================] - 2s 41ms/step - loss: 2442.5129 - val_loss: 7902.6738\n",
      "Epoch 2/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2434.0000 - val_loss: 7888.0605\n",
      "Epoch 3/188\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2426.5122 - val_loss: 7873.6582\n",
      "Epoch 4/188\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2426.2585 - val_loss: 7859.2529\n",
      "Epoch 5/188\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2417.2183 - val_loss: 7844.3506\n",
      "Epoch 6/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2421.1794 - val_loss: 7830.7744\n",
      "Epoch 7/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2414.2200 - val_loss: 7816.9814\n",
      "Epoch 8/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2423.3538 - val_loss: 7803.2305\n",
      "Epoch 9/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2415.7622 - val_loss: 7788.1362\n",
      "Epoch 10/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2400.3137 - val_loss: 7773.9634\n",
      "Epoch 11/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2409.2959 - val_loss: 7759.4697\n",
      "Epoch 12/188\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2409.3660 - val_loss: 7745.5059\n",
      "Epoch 13/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2391.4136 - val_loss: 7731.6758\n",
      "Epoch 14/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2400.4043 - val_loss: 7718.5835\n",
      "Epoch 15/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2400.4307 - val_loss: 7705.0073\n",
      "Epoch 16/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2386.1873 - val_loss: 7689.5967\n",
      "Epoch 17/188\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 2380.1387 - val_loss: 7676.2261\n",
      "Epoch 18/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2369.1970 - val_loss: 7662.7095\n",
      "Epoch 19/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2383.9497 - val_loss: 7649.0283\n",
      "Epoch 20/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2377.3271 - val_loss: 7635.2178\n",
      "Epoch 21/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2370.2407 - val_loss: 7621.9102\n",
      "Epoch 22/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2372.8667 - val_loss: 7608.5854\n",
      "Epoch 23/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2354.7910 - val_loss: 7594.4551\n",
      "Epoch 24/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2355.0850 - val_loss: 7580.2617\n",
      "Epoch 25/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2358.6980 - val_loss: 7566.0874\n",
      "Epoch 26/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2352.6602 - val_loss: 7551.3911\n",
      "Epoch 27/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2351.3276 - val_loss: 7538.2593\n",
      "Epoch 28/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2334.8452 - val_loss: 7523.6182\n",
      "Epoch 29/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2350.3137 - val_loss: 7510.6509\n",
      "Epoch 30/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2321.4102 - val_loss: 7497.2627\n",
      "Epoch 31/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2332.6501 - val_loss: 7484.2466\n",
      "Epoch 32/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2347.5369 - val_loss: 7470.2437\n",
      "Epoch 33/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2319.4548 - val_loss: 7457.8301\n",
      "Epoch 34/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2318.5479 - val_loss: 7443.8276\n",
      "Epoch 35/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2332.6572 - val_loss: 7429.2222\n",
      "Epoch 36/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2314.5598 - val_loss: 7415.2676\n",
      "Epoch 37/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2327.1987 - val_loss: 7402.0947\n",
      "Epoch 38/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2316.4182 - val_loss: 7387.9868\n",
      "Epoch 39/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2320.2856 - val_loss: 7373.4570\n",
      "Epoch 40/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2317.3391 - val_loss: 7361.1562\n",
      "Epoch 41/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2312.0632 - val_loss: 7348.6895\n",
      "Epoch 42/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2307.4285 - val_loss: 7335.1074\n",
      "Epoch 43/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2297.2466 - val_loss: 7321.8179\n",
      "Epoch 44/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2291.9280 - val_loss: 7308.7368\n",
      "Epoch 45/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2294.5767 - val_loss: 7295.1665\n",
      "Epoch 46/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2285.3040 - val_loss: 7282.3438\n",
      "Epoch 47/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2295.7578 - val_loss: 7268.8501\n",
      "Epoch 48/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2271.5776 - val_loss: 7256.1792\n",
      "Epoch 49/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2285.4189 - val_loss: 7243.3716\n",
      "Epoch 50/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2291.4045 - val_loss: 7229.6089\n",
      "Epoch 51/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2276.5986 - val_loss: 7216.4927\n",
      "Epoch 52/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2266.5249 - val_loss: 7204.0996\n",
      "Epoch 53/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2278.4382 - val_loss: 7191.6895\n",
      "Epoch 54/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2258.5186 - val_loss: 7177.8125\n",
      "Epoch 55/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2259.6150 - val_loss: 7164.8130\n",
      "Epoch 56/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2260.5928 - val_loss: 7151.8057\n",
      "Epoch 57/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2263.9980 - val_loss: 7139.2388\n",
      "Epoch 58/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2265.9814 - val_loss: 7127.5962\n",
      "Epoch 59/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2252.3000 - val_loss: 7115.2915\n",
      "Epoch 60/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2246.7166 - val_loss: 7101.9443\n",
      "Epoch 61/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2239.6460 - val_loss: 7089.6904\n",
      "Epoch 62/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2228.2339 - val_loss: 7075.2822\n",
      "Epoch 63/188\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 2244.3091 - val_loss: 7062.5542\n",
      "Epoch 64/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2233.1753 - val_loss: 7050.7500\n",
      "Epoch 65/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2229.8901 - val_loss: 7037.6289\n",
      "Epoch 66/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2231.2351 - val_loss: 7024.5708\n",
      "Epoch 67/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2218.4297 - val_loss: 7012.0723\n",
      "Epoch 68/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2221.2354 - val_loss: 6998.6553\n",
      "Epoch 69/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2220.0276 - val_loss: 6986.6655\n",
      "Epoch 70/188\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2213.7512 - val_loss: 6973.6680\n",
      "Epoch 71/188\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 2220.5347 - val_loss: 6961.3794\n",
      "Epoch 72/188\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 2208.9182 - val_loss: 6948.4028\n",
      "Epoch 73/188\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2208.9497 - val_loss: 6934.9863\n",
      "Epoch 74/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2203.6479 - val_loss: 6923.7788\n",
      "Epoch 75/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2200.5601 - val_loss: 6911.7095\n",
      "Epoch 76/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2193.1260 - val_loss: 6898.1777\n",
      "Epoch 77/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2207.7876 - val_loss: 6885.3213\n",
      "Epoch 78/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2180.4839 - val_loss: 6871.9087\n",
      "Epoch 79/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2200.6196 - val_loss: 6858.9805\n",
      "Epoch 80/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2195.2263 - val_loss: 6846.6724\n",
      "Epoch 81/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2180.5793 - val_loss: 6835.1094\n",
      "Epoch 82/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2178.1331 - val_loss: 6823.1592\n",
      "Epoch 83/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2189.2234 - val_loss: 6811.0571\n",
      "Epoch 84/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2169.7493 - val_loss: 6797.4946\n",
      "Epoch 85/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2183.6345 - val_loss: 6784.2686\n",
      "Epoch 86/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2173.2466 - val_loss: 6772.0850\n",
      "Epoch 87/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2162.8477 - val_loss: 6760.3320\n",
      "Epoch 88/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2173.4141 - val_loss: 6747.9756\n",
      "Epoch 89/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2154.4980 - val_loss: 6737.2441\n",
      "Epoch 90/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2153.4158 - val_loss: 6723.7964\n",
      "Epoch 91/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2149.7712 - val_loss: 6711.5557\n",
      "Epoch 92/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2145.1279 - val_loss: 6698.9146\n",
      "Epoch 93/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2141.1121 - val_loss: 6687.4868\n",
      "Epoch 94/188\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2161.1077 - val_loss: 6674.2925\n",
      "Epoch 95/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2163.6780 - val_loss: 6662.5229\n",
      "Epoch 96/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2150.0957 - val_loss: 6650.6113\n",
      "Epoch 97/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2124.6914 - val_loss: 6637.1396\n",
      "Epoch 98/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2138.4023 - val_loss: 6626.0850\n",
      "Epoch 99/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2135.0461 - val_loss: 6614.8662\n",
      "Epoch 100/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2129.0239 - val_loss: 6603.7749\n",
      "Epoch 101/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2134.2102 - val_loss: 6592.8091\n",
      "Epoch 102/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2134.5173 - val_loss: 6581.2339\n",
      "Epoch 103/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2134.6946 - val_loss: 6568.4224\n",
      "Epoch 104/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2125.4065 - val_loss: 6556.3057\n",
      "Epoch 105/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2117.7566 - val_loss: 6545.2588\n",
      "Epoch 106/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2114.8547 - val_loss: 6533.1240\n",
      "Epoch 107/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2122.0471 - val_loss: 6521.6533\n",
      "Epoch 108/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2117.4949 - val_loss: 6509.4214\n",
      "Epoch 109/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2091.8667 - val_loss: 6497.1553\n",
      "Epoch 110/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2088.1191 - val_loss: 6484.6772\n",
      "Epoch 111/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2097.4753 - val_loss: 6472.7197\n",
      "Epoch 112/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2109.9509 - val_loss: 6459.3276\n",
      "Epoch 113/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2095.7739 - val_loss: 6447.7183\n",
      "Epoch 114/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2106.7070 - val_loss: 6435.5786\n",
      "Epoch 115/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2091.5500 - val_loss: 6424.4668\n",
      "Epoch 116/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2095.0195 - val_loss: 6412.8311\n",
      "Epoch 117/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2077.2146 - val_loss: 6402.0151\n",
      "Epoch 118/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2079.8213 - val_loss: 6390.9282\n",
      "Epoch 119/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2079.1624 - val_loss: 6379.3472\n",
      "Epoch 120/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2083.0300 - val_loss: 6368.0981\n",
      "Epoch 121/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2064.6555 - val_loss: 6357.2837\n",
      "Epoch 122/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2052.0857 - val_loss: 6346.0562\n",
      "Epoch 123/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2077.8259 - val_loss: 6334.7505\n",
      "Epoch 124/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2064.9158 - val_loss: 6323.3804\n",
      "Epoch 125/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2071.9976 - val_loss: 6310.6230\n",
      "Epoch 126/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2067.8203 - val_loss: 6299.8320\n",
      "Epoch 127/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2050.5818 - val_loss: 6289.3145\n",
      "Epoch 128/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2059.3843 - val_loss: 6278.1978\n",
      "Epoch 129/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2053.4548 - val_loss: 6266.9302\n",
      "Epoch 130/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2050.5745 - val_loss: 6255.1152\n",
      "Epoch 131/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2040.1373 - val_loss: 6244.2754\n",
      "Epoch 132/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2032.4725 - val_loss: 6233.4839\n",
      "Epoch 133/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2041.1636 - val_loss: 6222.8027\n",
      "Epoch 134/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2030.3702 - val_loss: 6212.2905\n",
      "Epoch 135/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2031.8746 - val_loss: 6201.9463\n",
      "Epoch 136/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2042.5945 - val_loss: 6190.8833\n",
      "Epoch 137/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2034.9780 - val_loss: 6179.1758\n",
      "Epoch 138/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2027.6271 - val_loss: 6167.5596\n",
      "Epoch 139/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2031.5824 - val_loss: 6155.2290\n",
      "Epoch 140/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2027.3242 - val_loss: 6143.7725\n",
      "Epoch 141/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2015.0157 - val_loss: 6132.6987\n",
      "Epoch 142/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2017.8740 - val_loss: 6121.8569\n",
      "Epoch 143/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2011.9958 - val_loss: 6110.3379\n",
      "Epoch 144/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2020.2865 - val_loss: 6099.3936\n",
      "Epoch 145/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2018.4832 - val_loss: 6087.9038\n",
      "Epoch 146/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 2016.5634 - val_loss: 6075.7598\n",
      "Epoch 147/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2003.3156 - val_loss: 6063.7988\n",
      "Epoch 148/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2009.3448 - val_loss: 6051.8022\n",
      "Epoch 149/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1997.4027 - val_loss: 6040.2832\n",
      "Epoch 150/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2003.1262 - val_loss: 6027.2881\n",
      "Epoch 151/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1979.1533 - val_loss: 6014.1118\n",
      "Epoch 152/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1989.3533 - val_loss: 6003.8315\n",
      "Epoch 153/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1988.9531 - val_loss: 5993.8687\n",
      "Epoch 154/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1989.0129 - val_loss: 5981.5381\n",
      "Epoch 155/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1993.8728 - val_loss: 5969.3052\n",
      "Epoch 156/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1978.3617 - val_loss: 5959.1724\n",
      "Epoch 157/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1974.1000 - val_loss: 5948.5010\n",
      "Epoch 158/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1971.3578 - val_loss: 5938.3354\n",
      "Epoch 159/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1988.4106 - val_loss: 5927.9087\n",
      "Epoch 160/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1976.4321 - val_loss: 5918.2090\n",
      "Epoch 161/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1960.6219 - val_loss: 5907.8657\n",
      "Epoch 162/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1960.9299 - val_loss: 5897.6802\n",
      "Epoch 163/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1956.4557 - val_loss: 5886.4121\n",
      "Epoch 164/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1953.3684 - val_loss: 5877.1660\n",
      "Epoch 165/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1948.6132 - val_loss: 5865.3086\n",
      "Epoch 166/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1940.5021 - val_loss: 5854.9526\n",
      "Epoch 167/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1939.0513 - val_loss: 5845.3384\n",
      "Epoch 168/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1951.6268 - val_loss: 5834.7930\n",
      "Epoch 169/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1939.4885 - val_loss: 5824.3413\n",
      "Epoch 170/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1938.2195 - val_loss: 5815.1724\n",
      "Epoch 171/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1934.3832 - val_loss: 5804.0337\n",
      "Epoch 172/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1932.4556 - val_loss: 5792.2681\n",
      "Epoch 173/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1929.3906 - val_loss: 5782.0308\n",
      "Epoch 174/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1940.6981 - val_loss: 5771.9619\n",
      "Epoch 175/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1931.9984 - val_loss: 5761.8403\n",
      "Epoch 176/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1930.2994 - val_loss: 5752.2886\n",
      "Epoch 177/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1939.6075 - val_loss: 5741.2822\n",
      "Epoch 178/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1923.5022 - val_loss: 5731.7939\n",
      "Epoch 179/188\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1923.2053 - val_loss: 5719.8823\n",
      "Epoch 180/188\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1916.6842 - val_loss: 5708.7178\n",
      "Epoch 181/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1916.1553 - val_loss: 5697.0815\n",
      "Epoch 182/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1919.4377 - val_loss: 5685.4053\n",
      "Epoch 183/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1911.0190 - val_loss: 5675.1748\n",
      "Epoch 184/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1897.7847 - val_loss: 5665.3457\n",
      "Epoch 185/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1899.1489 - val_loss: 5655.7124\n",
      "Epoch 186/188\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1914.9916 - val_loss: 5646.1128\n",
      "Epoch 187/188\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1907.6062 - val_loss: 5636.6992\n",
      "Epoch 188/188\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1897.6986 - val_loss: 5625.7686\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 8521.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:51:27,497] Trial 4 finished with value: 8521.4755859375 and parameters: {'head_size': 193, 'num_heads': 5, 'ff_dim': 23, 'num_transformer_blocks': 5, 'mlp_units': 99, 'mlp_dropout': 0.13774297001567828, 'dropout': 0.22243986524532755, 'learning_rate': 1.3213070957287278e-06, 'n_epochs': 188}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_56 (La  (None, 8, 5)                 10        ['input_6[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_28 (M  (None, 8, 5)                 7273      ['layer_normalization_56[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_61 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_28[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_56 (T  (None, 8, 5)                 0         ['dropout_61[0][0]',          \n",
      " FOpLambda)                                                          'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_57 (La  (None, 8, 5)                 10        ['tf.__operators__.add_56[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_56 (Conv1D)          (None, 8, 22)                132       ['layer_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_62 (Dropout)        (None, 8, 22)                0         ['conv1d_56[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_57 (Conv1D)          (None, 8, 5)                 115       ['dropout_62[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_57 (T  (None, 8, 5)                 0         ['conv1d_57[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_56[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_58 (La  (None, 8, 5)                 10        ['tf.__operators__.add_57[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_29 (M  (None, 8, 5)                 7273      ['layer_normalization_58[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_58[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_63 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_29[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_58 (T  (None, 8, 5)                 0         ['dropout_63[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_57[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_59 (La  (None, 8, 5)                 10        ['tf.__operators__.add_58[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_58 (Conv1D)          (None, 8, 22)                132       ['layer_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_64 (Dropout)        (None, 8, 22)                0         ['conv1d_58[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_59 (Conv1D)          (None, 8, 5)                 115       ['dropout_64[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_59 (T  (None, 8, 5)                 0         ['conv1d_59[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_58[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_60 (La  (None, 8, 5)                 10        ['tf.__operators__.add_59[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_30 (M  (None, 8, 5)                 7273      ['layer_normalization_60[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_60[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_30[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_60 (T  (None, 8, 5)                 0         ['dropout_65[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_59[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_61 (La  (None, 8, 5)                 10        ['tf.__operators__.add_60[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_60 (Conv1D)          (None, 8, 22)                132       ['layer_normalization_61[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)        (None, 8, 22)                0         ['conv1d_60[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_61 (Conv1D)          (None, 8, 5)                 115       ['dropout_66[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_61 (T  (None, 8, 5)                 0         ['conv1d_61[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_60[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_62 (La  (None, 8, 5)                 10        ['tf.__operators__.add_61[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_31 (M  (None, 8, 5)                 7273      ['layer_normalization_62[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_62[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_31[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_62 (T  (None, 8, 5)                 0         ['dropout_67[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_61[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_63 (La  (None, 8, 5)                 10        ['tf.__operators__.add_62[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_62 (Conv1D)          (None, 8, 22)                132       ['layer_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)        (None, 8, 22)                0         ['conv1d_62[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_63 (Conv1D)          (None, 8, 5)                 115       ['dropout_68[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_63 (T  (None, 8, 5)                 0         ['conv1d_63[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_62[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_64 (La  (None, 8, 5)                 10        ['tf.__operators__.add_63[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_32 (M  (None, 8, 5)                 7273      ['layer_normalization_64[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_64[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_32[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_64 (T  (None, 8, 5)                 0         ['dropout_69[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_63[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_65 (La  (None, 8, 5)                 10        ['tf.__operators__.add_64[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_64 (Conv1D)          (None, 8, 22)                132       ['layer_normalization_65[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)        (None, 8, 22)                0         ['conv1d_64[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_65 (Conv1D)          (None, 8, 5)                 115       ['dropout_70[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_65 (T  (None, 8, 5)                 0         ['conv1d_65[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_64[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_66 (La  (None, 8, 5)                 10        ['tf.__operators__.add_65[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_33 (M  (None, 8, 5)                 7273      ['layer_normalization_66[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_33[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_66 (T  (None, 8, 5)                 0         ['dropout_71[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_65[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_67 (La  (None, 8, 5)                 10        ['tf.__operators__.add_66[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_66 (Conv1D)          (None, 8, 22)                132       ['layer_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)        (None, 8, 22)                0         ['conv1d_66[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_67 (Conv1D)          (None, 8, 5)                 115       ['dropout_72[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_67 (T  (None, 8, 5)                 0         ['conv1d_67[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_66[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5  (None, 8)                    0         ['tf.__operators__.add_67[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 77)                   693       ['global_average_pooling1d_5[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)        (None, 77)                   0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 16)                   1248      ['dropout_73[0][0]']          \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 8)                    136       ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47317 (184.83 KB)\n",
      "Trainable params: 47317 (184.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/75\n",
      "20/20 [==============================] - 2s 30ms/step - loss: 2281.8506 - val_loss: 4890.6509\n",
      "Epoch 2/75\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 2281.6262 - val_loss: 4877.9312\n",
      "Epoch 3/75\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 2265.2368 - val_loss: 4865.5176\n",
      "Epoch 4/75\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 2255.8508 - val_loss: 4853.4189\n",
      "Epoch 5/75\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2269.4177 - val_loss: 4841.0405\n",
      "Epoch 6/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2231.6982 - val_loss: 4829.1855\n",
      "Epoch 7/75\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2274.8220 - val_loss: 4816.9502\n",
      "Epoch 8/75\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2229.8450 - val_loss: 4804.3867\n",
      "Epoch 9/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2250.9912 - val_loss: 4792.0859\n",
      "Epoch 10/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2255.8335 - val_loss: 4780.7637\n",
      "Epoch 11/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2232.7231 - val_loss: 4767.9756\n",
      "Epoch 12/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2231.9414 - val_loss: 4756.0303\n",
      "Epoch 13/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2226.5845 - val_loss: 4744.9321\n",
      "Epoch 14/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2200.5339 - val_loss: 4733.1147\n",
      "Epoch 15/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2223.5620 - val_loss: 4723.0728\n",
      "Epoch 16/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2204.4424 - val_loss: 4711.6997\n",
      "Epoch 17/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2203.0176 - val_loss: 4699.7290\n",
      "Epoch 18/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2215.0969 - val_loss: 4688.8320\n",
      "Epoch 19/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2206.3196 - val_loss: 4678.3164\n",
      "Epoch 20/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2193.7185 - val_loss: 4666.9731\n",
      "Epoch 21/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2201.1218 - val_loss: 4655.8564\n",
      "Epoch 22/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2224.2981 - val_loss: 4645.5073\n",
      "Epoch 23/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2209.5269 - val_loss: 4635.4580\n",
      "Epoch 24/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2200.8931 - val_loss: 4625.1909\n",
      "Epoch 25/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2168.1111 - val_loss: 4615.8862\n",
      "Epoch 26/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2166.9675 - val_loss: 4606.5659\n",
      "Epoch 27/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2174.9985 - val_loss: 4597.8604\n",
      "Epoch 28/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2162.4397 - val_loss: 4588.6133\n",
      "Epoch 29/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2173.4590 - val_loss: 4579.4946\n",
      "Epoch 30/75\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2168.0398 - val_loss: 4569.6895\n",
      "Epoch 31/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2162.0093 - val_loss: 4559.5088\n",
      "Epoch 32/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2144.5823 - val_loss: 4550.0322\n",
      "Epoch 33/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2152.0496 - val_loss: 4540.4785\n",
      "Epoch 34/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2137.6741 - val_loss: 4531.7773\n",
      "Epoch 35/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2149.6907 - val_loss: 4523.3550\n",
      "Epoch 36/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2126.7273 - val_loss: 4514.2119\n",
      "Epoch 37/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2132.1316 - val_loss: 4505.4316\n",
      "Epoch 38/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2138.2585 - val_loss: 4496.7788\n",
      "Epoch 39/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2123.3494 - val_loss: 4487.3789\n",
      "Epoch 40/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2108.2612 - val_loss: 4478.4058\n",
      "Epoch 41/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2136.9814 - val_loss: 4469.7715\n",
      "Epoch 42/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2114.4253 - val_loss: 4460.7915\n",
      "Epoch 43/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2121.6282 - val_loss: 4452.2085\n",
      "Epoch 44/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2117.0107 - val_loss: 4443.9365\n",
      "Epoch 45/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2108.2197 - val_loss: 4435.8467\n",
      "Epoch 46/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2114.9514 - val_loss: 4427.4336\n",
      "Epoch 47/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2101.4497 - val_loss: 4418.4712\n",
      "Epoch 48/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2106.0432 - val_loss: 4410.0708\n",
      "Epoch 49/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2108.3936 - val_loss: 4401.4028\n",
      "Epoch 50/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2090.2744 - val_loss: 4392.1880\n",
      "Epoch 51/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2082.6863 - val_loss: 4383.9805\n",
      "Epoch 52/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2094.1614 - val_loss: 4375.7471\n",
      "Epoch 53/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2081.2290 - val_loss: 4367.2427\n",
      "Epoch 54/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2058.5330 - val_loss: 4358.7231\n",
      "Epoch 55/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2073.9446 - val_loss: 4350.2383\n",
      "Epoch 56/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2062.9963 - val_loss: 4341.6689\n",
      "Epoch 57/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2079.1897 - val_loss: 4333.7734\n",
      "Epoch 58/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2066.7830 - val_loss: 4325.1709\n",
      "Epoch 59/75\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2063.8630 - val_loss: 4316.8506\n",
      "Epoch 60/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2042.6470 - val_loss: 4309.1611\n",
      "Epoch 61/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2051.8923 - val_loss: 4301.0625\n",
      "Epoch 62/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2038.1860 - val_loss: 4292.3252\n",
      "Epoch 63/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2056.9312 - val_loss: 4284.2490\n",
      "Epoch 64/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2047.5284 - val_loss: 4276.4697\n",
      "Epoch 65/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2040.2886 - val_loss: 4269.0396\n",
      "Epoch 66/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2033.5641 - val_loss: 4261.7695\n",
      "Epoch 67/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2030.7404 - val_loss: 4254.6621\n",
      "Epoch 68/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2024.1051 - val_loss: 4246.6353\n",
      "Epoch 69/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2028.1865 - val_loss: 4238.9229\n",
      "Epoch 70/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2024.5383 - val_loss: 4230.8789\n",
      "Epoch 71/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2032.8168 - val_loss: 4223.2061\n",
      "Epoch 72/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2008.1666 - val_loss: 4215.7832\n",
      "Epoch 73/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2027.9707 - val_loss: 4207.8613\n",
      "Epoch 74/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2020.4629 - val_loss: 4199.9609\n",
      "Epoch 75/75\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2014.9614 - val_loss: 4192.3477\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6085.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:52:10,710] Trial 5 finished with value: 6085.98095703125 and parameters: {'head_size': 79, 'num_heads': 4, 'ff_dim': 22, 'num_transformer_blocks': 6, 'mlp_units': 77, 'mlp_dropout': 0.28459438758316813, 'dropout': 0.3356670292592307, 'learning_rate': 1.4606844924500637e-06, 'n_epochs': 75}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_68 (La  (None, 8, 5)                 10        ['input_7[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_34 (M  (None, 8, 5)                 19739     ['layer_normalization_68[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_68[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_34[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_68 (T  (None, 8, 5)                 0         ['dropout_74[0][0]',          \n",
      " FOpLambda)                                                          'input_7[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_69 (La  (None, 8, 5)                 10        ['tf.__operators__.add_68[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_68 (Conv1D)          (None, 8, 13)                78        ['layer_normalization_69[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)        (None, 8, 13)                0         ['conv1d_68[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_69 (Conv1D)          (None, 8, 5)                 70        ['dropout_75[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_69 (T  (None, 8, 5)                 0         ['conv1d_69[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_68[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_70 (La  (None, 8, 5)                 10        ['tf.__operators__.add_69[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_35 (M  (None, 8, 5)                 19739     ['layer_normalization_70[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_35[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_70 (T  (None, 8, 5)                 0         ['dropout_76[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_69[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_71 (La  (None, 8, 5)                 10        ['tf.__operators__.add_70[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_70 (Conv1D)          (None, 8, 13)                78        ['layer_normalization_71[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)        (None, 8, 13)                0         ['conv1d_70[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_71 (Conv1D)          (None, 8, 5)                 70        ['dropout_77[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_71 (T  (None, 8, 5)                 0         ['conv1d_71[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_70[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_72 (La  (None, 8, 5)                 10        ['tf.__operators__.add_71[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_36 (M  (None, 8, 5)                 19739     ['layer_normalization_72[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_72[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_36[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_72 (T  (None, 8, 5)                 0         ['dropout_78[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_71[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_73 (La  (None, 8, 5)                 10        ['tf.__operators__.add_72[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_72 (Conv1D)          (None, 8, 13)                78        ['layer_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)        (None, 8, 13)                0         ['conv1d_72[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_73 (Conv1D)          (None, 8, 5)                 70        ['dropout_79[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_73 (T  (None, 8, 5)                 0         ['conv1d_73[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_72[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_74 (La  (None, 8, 5)                 10        ['tf.__operators__.add_73[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_37 (M  (None, 8, 5)                 19739     ['layer_normalization_74[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_74[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_37[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_74 (T  (None, 8, 5)                 0         ['dropout_80[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_73[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_75 (La  (None, 8, 5)                 10        ['tf.__operators__.add_74[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_74 (Conv1D)          (None, 8, 13)                78        ['layer_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)        (None, 8, 13)                0         ['conv1d_74[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_75 (Conv1D)          (None, 8, 5)                 70        ['dropout_81[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_75 (T  (None, 8, 5)                 0         ['conv1d_75[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_74[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_76 (La  (None, 8, 5)                 10        ['tf.__operators__.add_75[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_38 (M  (None, 8, 5)                 19739     ['layer_normalization_76[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_76[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_82 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_38[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_76 (T  (None, 8, 5)                 0         ['dropout_82[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_75[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_77 (La  (None, 8, 5)                 10        ['tf.__operators__.add_76[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_76 (Conv1D)          (None, 8, 13)                78        ['layer_normalization_77[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_83 (Dropout)        (None, 8, 13)                0         ['conv1d_76[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_77 (Conv1D)          (None, 8, 5)                 70        ['dropout_83[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_77 (T  (None, 8, 5)                 0         ['conv1d_77[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_76[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_78 (La  (None, 8, 5)                 10        ['tf.__operators__.add_77[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_39 (M  (None, 8, 5)                 19739     ['layer_normalization_78[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_78[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_84 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_39[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_78 (T  (None, 8, 5)                 0         ['dropout_84[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_77[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_79 (La  (None, 8, 5)                 10        ['tf.__operators__.add_78[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_78 (Conv1D)          (None, 8, 13)                78        ['layer_normalization_79[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_85 (Dropout)        (None, 8, 13)                0         ['conv1d_78[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_79 (Conv1D)          (None, 8, 5)                 70        ['dropout_85[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_79 (T  (None, 8, 5)                 0         ['conv1d_79[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_78[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6  (None, 8)                    0         ['tf.__operators__.add_79[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 121)                  1089      ['global_average_pooling1d_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)        (None, 121)                  0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 16)                   1952      ['dropout_86[0][0]']          \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 8)                    136       ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 122619 (478.98 KB)\n",
      "Trainable params: 122619 (478.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/134\n",
      "20/20 [==============================] - 2s 41ms/step - loss: 2590.6780 - val_loss: 6668.4102\n",
      "Epoch 2/134\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2538.3228 - val_loss: 6532.0093\n",
      "Epoch 3/134\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2475.5222 - val_loss: 6400.8784\n",
      "Epoch 4/134\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2441.7026 - val_loss: 6261.5059\n",
      "Epoch 5/134\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2380.5178 - val_loss: 6122.0317\n",
      "Epoch 6/134\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2337.9971 - val_loss: 5985.7656\n",
      "Epoch 7/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2295.8220 - val_loss: 5852.0791\n",
      "Epoch 8/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2240.6433 - val_loss: 5717.5962\n",
      "Epoch 9/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2198.7004 - val_loss: 5576.3662\n",
      "Epoch 10/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2150.4023 - val_loss: 5452.3706\n",
      "Epoch 11/134\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2103.9727 - val_loss: 5331.1865\n",
      "Epoch 12/134\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2077.9431 - val_loss: 5197.2085\n",
      "Epoch 13/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2039.9672 - val_loss: 5073.4810\n",
      "Epoch 14/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2004.3557 - val_loss: 4945.3184\n",
      "Epoch 15/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1967.1866 - val_loss: 4819.0840\n",
      "Epoch 16/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1917.6395 - val_loss: 4700.8413\n",
      "Epoch 17/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1884.7572 - val_loss: 4583.8672\n",
      "Epoch 18/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1858.2817 - val_loss: 4463.6367\n",
      "Epoch 19/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1822.4480 - val_loss: 4332.5059\n",
      "Epoch 20/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1777.0559 - val_loss: 4207.2759\n",
      "Epoch 21/134\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1743.5582 - val_loss: 4091.2520\n",
      "Epoch 22/134\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1719.8436 - val_loss: 3995.5740\n",
      "Epoch 23/134\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1687.1151 - val_loss: 3906.5449\n",
      "Epoch 24/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1670.3276 - val_loss: 3814.5452\n",
      "Epoch 25/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1626.3503 - val_loss: 3733.3838\n",
      "Epoch 26/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1581.0490 - val_loss: 3641.1917\n",
      "Epoch 27/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1580.6501 - val_loss: 3562.6948\n",
      "Epoch 28/134\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1512.3221 - val_loss: 3484.9531\n",
      "Epoch 29/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1508.7682 - val_loss: 3407.4902\n",
      "Epoch 30/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1471.7430 - val_loss: 3344.8784\n",
      "Epoch 31/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1456.0731 - val_loss: 3263.5574\n",
      "Epoch 32/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1416.9313 - val_loss: 3196.2324\n",
      "Epoch 33/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1381.3688 - val_loss: 3125.9949\n",
      "Epoch 34/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1355.2405 - val_loss: 3057.6155\n",
      "Epoch 35/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1344.4462 - val_loss: 2990.8496\n",
      "Epoch 36/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1320.4274 - val_loss: 2922.3606\n",
      "Epoch 37/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1293.8840 - val_loss: 2860.0281\n",
      "Epoch 38/134\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1275.1329 - val_loss: 2805.2744\n",
      "Epoch 39/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1243.3074 - val_loss: 2742.3281\n",
      "Epoch 40/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1229.8691 - val_loss: 2664.8357\n",
      "Epoch 41/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1222.7567 - val_loss: 2608.6448\n",
      "Epoch 42/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1179.5756 - val_loss: 2543.8328\n",
      "Epoch 43/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1149.7446 - val_loss: 2480.1396\n",
      "Epoch 44/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1149.0812 - val_loss: 2420.5693\n",
      "Epoch 45/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1129.1165 - val_loss: 2353.9839\n",
      "Epoch 46/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1120.6898 - val_loss: 2301.4558\n",
      "Epoch 47/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1085.2599 - val_loss: 2254.9094\n",
      "Epoch 48/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1068.5553 - val_loss: 2200.6128\n",
      "Epoch 49/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1062.3832 - val_loss: 2140.6133\n",
      "Epoch 50/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1030.9379 - val_loss: 2086.8967\n",
      "Epoch 51/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1047.4624 - val_loss: 2044.7518\n",
      "Epoch 52/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1028.8402 - val_loss: 2017.3778\n",
      "Epoch 53/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 1012.0804 - val_loss: 1969.9133\n",
      "Epoch 54/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 997.2343 - val_loss: 1930.7994\n",
      "Epoch 55/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 996.3113 - val_loss: 1892.9806\n",
      "Epoch 56/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 991.4379 - val_loss: 1863.1158\n",
      "Epoch 57/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 967.8812 - val_loss: 1833.0641\n",
      "Epoch 58/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 961.5936 - val_loss: 1788.5797\n",
      "Epoch 59/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 948.7136 - val_loss: 1756.8387\n",
      "Epoch 60/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 937.3392 - val_loss: 1721.7380\n",
      "Epoch 61/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 953.9416 - val_loss: 1709.0358\n",
      "Epoch 62/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 934.5599 - val_loss: 1692.5529\n",
      "Epoch 63/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 921.5741 - val_loss: 1666.4694\n",
      "Epoch 64/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 913.2924 - val_loss: 1645.8976\n",
      "Epoch 65/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 909.6288 - val_loss: 1625.7601\n",
      "Epoch 66/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 909.2383 - val_loss: 1602.7677\n",
      "Epoch 67/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 889.6696 - val_loss: 1578.2523\n",
      "Epoch 68/134\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 894.7515 - val_loss: 1562.3068\n",
      "Epoch 69/134\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 884.6241 - val_loss: 1542.0789\n",
      "Epoch 70/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 894.6707 - val_loss: 1518.4484\n",
      "Epoch 71/134\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 886.5053 - val_loss: 1496.1178\n",
      "Epoch 72/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 856.5045 - val_loss: 1473.6174\n",
      "Epoch 73/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 866.1581 - val_loss: 1448.4053\n",
      "Epoch 74/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 860.3152 - val_loss: 1433.6417\n",
      "Epoch 75/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 860.2789 - val_loss: 1425.9524\n",
      "Epoch 76/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 860.8573 - val_loss: 1398.9683\n",
      "Epoch 77/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 856.8080 - val_loss: 1377.4851\n",
      "Epoch 78/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 839.2667 - val_loss: 1354.8256\n",
      "Epoch 79/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 851.5104 - val_loss: 1332.0415\n",
      "Epoch 80/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 829.2218 - val_loss: 1323.0128\n",
      "Epoch 81/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 840.3458 - val_loss: 1311.9327\n",
      "Epoch 82/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 829.5307 - val_loss: 1281.1461\n",
      "Epoch 83/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 833.5032 - val_loss: 1252.9803\n",
      "Epoch 84/134\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 829.9840 - val_loss: 1224.5172\n",
      "Epoch 85/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 807.7435 - val_loss: 1215.8414\n",
      "Epoch 86/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 809.8058 - val_loss: 1209.6716\n",
      "Epoch 87/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 806.6074 - val_loss: 1185.9816\n",
      "Epoch 88/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 802.9325 - val_loss: 1182.9351\n",
      "Epoch 89/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 807.9827 - val_loss: 1169.2592\n",
      "Epoch 90/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 801.4371 - val_loss: 1150.7325\n",
      "Epoch 91/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 792.5650 - val_loss: 1129.6735\n",
      "Epoch 92/134\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 795.8912 - val_loss: 1129.9088\n",
      "Epoch 93/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 788.7955 - val_loss: 1125.7432\n",
      "Epoch 94/134\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 783.1892 - val_loss: 1107.0940\n",
      "Epoch 95/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 791.0395 - val_loss: 1079.0743\n",
      "Epoch 96/134\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 767.1546 - val_loss: 1063.4491\n",
      "Epoch 97/134\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 781.4743 - val_loss: 1047.6005\n",
      "Epoch 98/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 759.7866 - val_loss: 1041.5679\n",
      "Epoch 99/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 768.8785 - val_loss: 1034.6322\n",
      "Epoch 100/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 759.5167 - val_loss: 1023.9781\n",
      "Epoch 101/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 766.1382 - val_loss: 1007.4104\n",
      "Epoch 102/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 750.1149 - val_loss: 1005.7556\n",
      "Epoch 103/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 749.7859 - val_loss: 987.9136\n",
      "Epoch 104/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 751.9817 - val_loss: 970.0663\n",
      "Epoch 105/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 745.9179 - val_loss: 944.5549\n",
      "Epoch 106/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 749.8821 - val_loss: 951.4966\n",
      "Epoch 107/134\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 744.5980 - val_loss: 945.4323\n",
      "Epoch 108/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 736.0193 - val_loss: 938.6224\n",
      "Epoch 109/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 735.0173 - val_loss: 922.3097\n",
      "Epoch 110/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 739.8882 - val_loss: 905.9358\n",
      "Epoch 111/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 722.5378 - val_loss: 888.6658\n",
      "Epoch 112/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 723.4286 - val_loss: 867.9698\n",
      "Epoch 113/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 733.2836 - val_loss: 856.3668\n",
      "Epoch 114/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 717.6362 - val_loss: 855.4155\n",
      "Epoch 115/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 726.2554 - val_loss: 855.7163\n",
      "Epoch 116/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 701.2443 - val_loss: 837.4007\n",
      "Epoch 117/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 708.6600 - val_loss: 834.1981\n",
      "Epoch 118/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 702.2589 - val_loss: 834.7996\n",
      "Epoch 119/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 704.2696 - val_loss: 817.9203\n",
      "Epoch 120/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 697.8199 - val_loss: 817.2208\n",
      "Epoch 121/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 700.9847 - val_loss: 799.2898\n",
      "Epoch 122/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 686.9318 - val_loss: 786.9530\n",
      "Epoch 123/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 703.1310 - val_loss: 781.7918\n",
      "Epoch 124/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 693.4794 - val_loss: 766.0539\n",
      "Epoch 125/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 692.1915 - val_loss: 756.4644\n",
      "Epoch 126/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 698.3961 - val_loss: 753.6809\n",
      "Epoch 127/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 677.5682 - val_loss: 760.1817\n",
      "Epoch 128/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 677.7720 - val_loss: 770.9380\n",
      "Epoch 129/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 670.5756 - val_loss: 762.1777\n",
      "Epoch 130/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 680.8547 - val_loss: 745.9163\n",
      "Epoch 131/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 674.8076 - val_loss: 729.8551\n",
      "Epoch 132/134\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 659.8555 - val_loss: 712.0458\n",
      "Epoch 133/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 659.7136 - val_loss: 695.0230\n",
      "Epoch 134/134\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 673.5137 - val_loss: 704.3058\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1876.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:53:49,902] Trial 6 finished with value: 1876.0277099609375 and parameters: {'head_size': 143, 'num_heads': 6, 'ff_dim': 13, 'num_transformer_blocks': 6, 'mlp_units': 121, 'mlp_dropout': 0.3294229808542499, 'dropout': 0.24716197248647304, 'learning_rate': 1.9553274486835937e-05, 'n_epochs': 134}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_80 (La  (None, 8, 5)                 10        ['input_8[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_40 (M  (None, 8, 5)                 8469      ['layer_normalization_80[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_80[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_87 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_40[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_80 (T  (None, 8, 5)                 0         ['dropout_87[0][0]',          \n",
      " FOpLambda)                                                          'input_8[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_81 (La  (None, 8, 5)                 10        ['tf.__operators__.add_80[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_80 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_81[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_88 (Dropout)        (None, 8, 21)                0         ['conv1d_80[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_81 (Conv1D)          (None, 8, 5)                 110       ['dropout_88[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_81 (T  (None, 8, 5)                 0         ['conv1d_81[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_80[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_82 (La  (None, 8, 5)                 10        ['tf.__operators__.add_81[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_41 (M  (None, 8, 5)                 8469      ['layer_normalization_82[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_82[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_89 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_41[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_82 (T  (None, 8, 5)                 0         ['dropout_89[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_81[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_83 (La  (None, 8, 5)                 10        ['tf.__operators__.add_82[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_82 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_83[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_90 (Dropout)        (None, 8, 21)                0         ['conv1d_82[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_83 (Conv1D)          (None, 8, 5)                 110       ['dropout_90[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_83 (T  (None, 8, 5)                 0         ['conv1d_83[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_82[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_84 (La  (None, 8, 5)                 10        ['tf.__operators__.add_83[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_42 (M  (None, 8, 5)                 8469      ['layer_normalization_84[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_84[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_91 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_42[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_84 (T  (None, 8, 5)                 0         ['dropout_91[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_83[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_85 (La  (None, 8, 5)                 10        ['tf.__operators__.add_84[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_84 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_85[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_92 (Dropout)        (None, 8, 21)                0         ['conv1d_84[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_85 (Conv1D)          (None, 8, 5)                 110       ['dropout_92[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_85 (T  (None, 8, 5)                 0         ['conv1d_85[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_84[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_86 (La  (None, 8, 5)                 10        ['tf.__operators__.add_85[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_43 (M  (None, 8, 5)                 8469      ['layer_normalization_86[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_86[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_93 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_43[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_86 (T  (None, 8, 5)                 0         ['dropout_93[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_85[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_87 (La  (None, 8, 5)                 10        ['tf.__operators__.add_86[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_86 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_87[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_94 (Dropout)        (None, 8, 21)                0         ['conv1d_86[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_87 (Conv1D)          (None, 8, 5)                 110       ['dropout_94[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_87 (T  (None, 8, 5)                 0         ['conv1d_87[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_86[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_7  (None, 8)                    0         ['tf.__operators__.add_87[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 87)                   783       ['global_average_pooling1d_7[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_95 (Dropout)        (None, 87)                   0         ['dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 16)                   1408      ['dropout_95[0][0]']          \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 8)                    136       ['dense_22[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37227 (145.42 KB)\n",
      "Trainable params: 37227 (145.42 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/114\n",
      "20/20 [==============================] - 2s 23ms/step - loss: 3231.0798 - val_loss: 9689.8984\n",
      "Epoch 2/114\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 3192.3181 - val_loss: 9580.7168\n",
      "Epoch 3/114\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 3167.5203 - val_loss: 9474.0352\n",
      "Epoch 4/114\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 3122.4297 - val_loss: 9376.1855\n",
      "Epoch 5/114\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 3113.3704 - val_loss: 9272.6348\n",
      "Epoch 6/114\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 3115.5291 - val_loss: 9167.9775\n",
      "Epoch 7/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 3096.3401 - val_loss: 9062.4307\n",
      "Epoch 8/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 3048.0815 - val_loss: 8962.7725\n",
      "Epoch 9/114\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 3023.4524 - val_loss: 8862.5791\n",
      "Epoch 10/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 3010.7188 - val_loss: 8763.3877\n",
      "Epoch 11/114\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2979.5950 - val_loss: 8659.5410\n",
      "Epoch 12/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2954.8030 - val_loss: 8558.9258\n",
      "Epoch 13/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2934.8337 - val_loss: 8462.1025\n",
      "Epoch 14/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2902.5723 - val_loss: 8369.2598\n",
      "Epoch 15/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2875.4419 - val_loss: 8276.5674\n",
      "Epoch 16/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2839.0410 - val_loss: 8190.4805\n",
      "Epoch 17/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2837.0083 - val_loss: 8108.0957\n",
      "Epoch 18/114\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 2819.7683 - val_loss: 8025.3838\n",
      "Epoch 19/114\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2801.2771 - val_loss: 7949.0254\n",
      "Epoch 20/114\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2774.1472 - val_loss: 7867.8447\n",
      "Epoch 21/114\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2753.3743 - val_loss: 7794.2124\n",
      "Epoch 22/114\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 2730.2971 - val_loss: 7724.5161\n",
      "Epoch 23/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2699.0925 - val_loss: 7651.4404\n",
      "Epoch 24/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2708.9241 - val_loss: 7576.9863\n",
      "Epoch 25/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2684.5542 - val_loss: 7508.9277\n",
      "Epoch 26/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2661.4990 - val_loss: 7441.3813\n",
      "Epoch 27/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2641.5203 - val_loss: 7369.4399\n",
      "Epoch 28/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2613.1973 - val_loss: 7305.8149\n",
      "Epoch 29/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2620.5942 - val_loss: 7238.2280\n",
      "Epoch 30/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2602.3105 - val_loss: 7171.1333\n",
      "Epoch 31/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2573.7292 - val_loss: 7102.7949\n",
      "Epoch 32/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2565.5212 - val_loss: 7037.4600\n",
      "Epoch 33/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2567.8408 - val_loss: 6973.3774\n",
      "Epoch 34/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2528.8799 - val_loss: 6908.1333\n",
      "Epoch 35/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2512.2686 - val_loss: 6844.6694\n",
      "Epoch 36/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2509.6323 - val_loss: 6778.1606\n",
      "Epoch 37/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2482.2131 - val_loss: 6712.1138\n",
      "Epoch 38/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2483.5190 - val_loss: 6652.5591\n",
      "Epoch 39/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2458.5544 - val_loss: 6594.1919\n",
      "Epoch 40/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2454.9390 - val_loss: 6539.3540\n",
      "Epoch 41/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2458.4414 - val_loss: 6485.4585\n",
      "Epoch 42/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2422.8071 - val_loss: 6434.7075\n",
      "Epoch 43/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2410.3972 - val_loss: 6383.9316\n",
      "Epoch 44/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2394.7744 - val_loss: 6334.9810\n",
      "Epoch 45/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2377.6230 - val_loss: 6285.3804\n",
      "Epoch 46/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2372.7690 - val_loss: 6235.5693\n",
      "Epoch 47/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2362.5605 - val_loss: 6187.5000\n",
      "Epoch 48/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2345.1729 - val_loss: 6136.6060\n",
      "Epoch 49/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2339.8403 - val_loss: 6089.3188\n",
      "Epoch 50/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2317.0637 - val_loss: 6039.2617\n",
      "Epoch 51/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2300.4331 - val_loss: 5987.5283\n",
      "Epoch 52/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2286.6511 - val_loss: 5936.8755\n",
      "Epoch 53/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2263.3157 - val_loss: 5888.1631\n",
      "Epoch 54/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2280.2827 - val_loss: 5839.2065\n",
      "Epoch 55/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2244.3735 - val_loss: 5792.4380\n",
      "Epoch 56/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2245.4004 - val_loss: 5741.9648\n",
      "Epoch 57/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2234.0740 - val_loss: 5695.5742\n",
      "Epoch 58/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2243.3037 - val_loss: 5648.3633\n",
      "Epoch 59/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2222.7063 - val_loss: 5599.1689\n",
      "Epoch 60/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2187.0168 - val_loss: 5550.2915\n",
      "Epoch 61/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2179.5527 - val_loss: 5503.4199\n",
      "Epoch 62/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2166.2888 - val_loss: 5455.1040\n",
      "Epoch 63/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2171.5457 - val_loss: 5410.3853\n",
      "Epoch 64/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2145.1230 - val_loss: 5365.6377\n",
      "Epoch 65/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2137.5200 - val_loss: 5318.0493\n",
      "Epoch 66/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2134.2273 - val_loss: 5268.0220\n",
      "Epoch 67/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2104.5793 - val_loss: 5219.4805\n",
      "Epoch 68/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2135.6697 - val_loss: 5172.7847\n",
      "Epoch 69/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2096.4365 - val_loss: 5125.8970\n",
      "Epoch 70/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2095.4160 - val_loss: 5077.4814\n",
      "Epoch 71/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2073.8040 - val_loss: 5028.9619\n",
      "Epoch 72/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 2065.9033 - val_loss: 4984.0132\n",
      "Epoch 73/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2054.2578 - val_loss: 4934.0332\n",
      "Epoch 74/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2047.5293 - val_loss: 4885.7090\n",
      "Epoch 75/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2042.2035 - val_loss: 4838.3481\n",
      "Epoch 76/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2031.5438 - val_loss: 4792.5474\n",
      "Epoch 77/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 2003.2932 - val_loss: 4746.1421\n",
      "Epoch 78/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2007.6344 - val_loss: 4698.3975\n",
      "Epoch 79/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1989.4161 - val_loss: 4652.3911\n",
      "Epoch 80/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1979.9061 - val_loss: 4604.1528\n",
      "Epoch 81/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1957.0380 - val_loss: 4564.0562\n",
      "Epoch 82/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1961.3759 - val_loss: 4520.3325\n",
      "Epoch 83/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1946.9719 - val_loss: 4476.8613\n",
      "Epoch 84/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1947.8397 - val_loss: 4428.6499\n",
      "Epoch 85/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1932.5854 - val_loss: 4384.4102\n",
      "Epoch 86/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1924.5791 - val_loss: 4338.3389\n",
      "Epoch 87/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1932.9164 - val_loss: 4295.8066\n",
      "Epoch 88/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1895.0703 - val_loss: 4253.6899\n",
      "Epoch 89/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1890.7017 - val_loss: 4213.2446\n",
      "Epoch 90/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1886.6426 - val_loss: 4171.9292\n",
      "Epoch 91/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1874.3689 - val_loss: 4131.0493\n",
      "Epoch 92/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1861.6184 - val_loss: 4087.9053\n",
      "Epoch 93/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1864.9572 - val_loss: 4047.2405\n",
      "Epoch 94/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1852.6871 - val_loss: 4004.6543\n",
      "Epoch 95/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1835.5145 - val_loss: 3958.1838\n",
      "Epoch 96/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1828.9215 - val_loss: 3909.0940\n",
      "Epoch 97/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1827.3103 - val_loss: 3856.4692\n",
      "Epoch 98/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1803.0289 - val_loss: 3818.3777\n",
      "Epoch 99/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1790.7676 - val_loss: 3779.3171\n",
      "Epoch 100/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1787.2699 - val_loss: 3735.7546\n",
      "Epoch 101/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1781.7709 - val_loss: 3695.1875\n",
      "Epoch 102/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1772.8632 - val_loss: 3655.8716\n",
      "Epoch 103/114\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1756.8074 - val_loss: 3617.2390\n",
      "Epoch 104/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1758.1871 - val_loss: 3577.1084\n",
      "Epoch 105/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1723.7338 - val_loss: 3540.5952\n",
      "Epoch 106/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1733.2437 - val_loss: 3507.2686\n",
      "Epoch 107/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1717.5798 - val_loss: 3472.1584\n",
      "Epoch 108/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1706.0627 - val_loss: 3435.5981\n",
      "Epoch 109/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1698.7517 - val_loss: 3398.5151\n",
      "Epoch 110/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1701.6196 - val_loss: 3356.4089\n",
      "Epoch 111/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1687.4437 - val_loss: 3321.3936\n",
      "Epoch 112/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1675.1896 - val_loss: 3284.2708\n",
      "Epoch 113/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1681.8594 - val_loss: 3247.8728\n",
      "Epoch 114/114\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 1660.9454 - val_loss: 3211.8801\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4985.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:54:36,818] Trial 7 finished with value: 4985.630859375 and parameters: {'head_size': 92, 'num_heads': 4, 'ff_dim': 21, 'num_transformer_blocks': 4, 'mlp_units': 87, 'mlp_dropout': 0.20709031895709212, 'dropout': 0.1983283487954779, 'learning_rate': 8.024461055875242e-06, 'n_epochs': 114}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_88 (La  (None, 8, 5)                 10        ['input_9[0][0]']             \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_44 (M  (None, 8, 5)                 22890     ['layer_normalization_88[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_88[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_96 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_44[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_88 (T  (None, 8, 5)                 0         ['dropout_96[0][0]',          \n",
      " FOpLambda)                                                          'input_9[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_89 (La  (None, 8, 5)                 10        ['tf.__operators__.add_88[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_88 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_97 (Dropout)        (None, 8, 21)                0         ['conv1d_88[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_89 (Conv1D)          (None, 8, 5)                 110       ['dropout_97[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_89 (T  (None, 8, 5)                 0         ['conv1d_89[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_88[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_90 (La  (None, 8, 5)                 10        ['tf.__operators__.add_89[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_45 (M  (None, 8, 5)                 22890     ['layer_normalization_90[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_90[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_98 (Dropout)        (None, 8, 5)                 0         ['multi_head_attention_45[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_90 (T  (None, 8, 5)                 0         ['dropout_98[0][0]',          \n",
      " FOpLambda)                                                          'tf.__operators__.add_89[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_91 (La  (None, 8, 5)                 10        ['tf.__operators__.add_90[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_90 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_91[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_99 (Dropout)        (None, 8, 21)                0         ['conv1d_90[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_91 (Conv1D)          (None, 8, 5)                 110       ['dropout_99[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_91 (T  (None, 8, 5)                 0         ['conv1d_91[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_90[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_92 (La  (None, 8, 5)                 10        ['tf.__operators__.add_91[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_46 (M  (None, 8, 5)                 22890     ['layer_normalization_92[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_92[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_100 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_46[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_92 (T  (None, 8, 5)                 0         ['dropout_100[0][0]',         \n",
      " FOpLambda)                                                          'tf.__operators__.add_91[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_93 (La  (None, 8, 5)                 10        ['tf.__operators__.add_92[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_92 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_93[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_101 (Dropout)       (None, 8, 21)                0         ['conv1d_92[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_93 (Conv1D)          (None, 8, 5)                 110       ['dropout_101[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_93 (T  (None, 8, 5)                 0         ['conv1d_93[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_92[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_94 (La  (None, 8, 5)                 10        ['tf.__operators__.add_93[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_47 (M  (None, 8, 5)                 22890     ['layer_normalization_94[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_94[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_102 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_47[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_94 (T  (None, 8, 5)                 0         ['dropout_102[0][0]',         \n",
      " FOpLambda)                                                          'tf.__operators__.add_93[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_95 (La  (None, 8, 5)                 10        ['tf.__operators__.add_94[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_94 (Conv1D)          (None, 8, 21)                126       ['layer_normalization_95[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_103 (Dropout)       (None, 8, 21)                0         ['conv1d_94[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_95 (Conv1D)          (None, 8, 5)                 110       ['dropout_103[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_95 (T  (None, 8, 5)                 0         ['conv1d_95[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_94[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_8  (None, 8)                    0         ['tf.__operators__.add_95[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 65)                   585       ['global_average_pooling1d_8[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_104 (Dropout)       (None, 65)                   0         ['dense_24[0][0]']            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 16)                   1056      ['dropout_104[0][0]']         \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 8)                    136       ['dense_25[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 94361 (368.60 KB)\n",
      "Trainable params: 94361 (368.60 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/96\n",
      "20/20 [==============================] - 2s 30ms/step - loss: 3380.4648 - val_loss: 9300.6895\n",
      "Epoch 2/96\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 3180.2244 - val_loss: 8739.5225\n",
      "Epoch 3/96\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 3002.7749 - val_loss: 8160.0820\n",
      "Epoch 4/96\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 2822.0581 - val_loss: 7588.6270\n",
      "Epoch 5/96\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 2658.0142 - val_loss: 7027.8511\n",
      "Epoch 6/96\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 2509.4565 - val_loss: 6492.0239\n",
      "Epoch 7/96\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 2355.0156 - val_loss: 5982.0444\n",
      "Epoch 8/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2235.1538 - val_loss: 5557.9502\n",
      "Epoch 9/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2116.3416 - val_loss: 5227.4561\n",
      "Epoch 10/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2018.5022 - val_loss: 4922.3867\n",
      "Epoch 11/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1926.5344 - val_loss: 4649.2446\n",
      "Epoch 12/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1839.4213 - val_loss: 4476.5054\n",
      "Epoch 13/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1754.2177 - val_loss: 4229.3647\n",
      "Epoch 14/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1677.4254 - val_loss: 4015.4138\n",
      "Epoch 15/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1603.7831 - val_loss: 3754.6865\n",
      "Epoch 16/96\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 1508.4954 - val_loss: 3542.0906\n",
      "Epoch 17/96\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 1439.9440 - val_loss: 3378.4988\n",
      "Epoch 18/96\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 1344.6115 - val_loss: 3188.2278\n",
      "Epoch 19/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1274.4280 - val_loss: 2910.8425\n",
      "Epoch 20/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1212.3619 - val_loss: 2731.6152\n",
      "Epoch 21/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1129.2411 - val_loss: 2519.7417\n",
      "Epoch 22/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 1061.9932 - val_loss: 2371.1411\n",
      "Epoch 23/96\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 998.7645 - val_loss: 2181.4182\n",
      "Epoch 24/96\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 932.6219 - val_loss: 1912.0536\n",
      "Epoch 25/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 866.5211 - val_loss: 1786.0751\n",
      "Epoch 26/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 842.5320 - val_loss: 1564.3693\n",
      "Epoch 27/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 794.3362 - val_loss: 1374.0109\n",
      "Epoch 28/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 761.1083 - val_loss: 1284.1422\n",
      "Epoch 29/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 729.8675 - val_loss: 1217.8313\n",
      "Epoch 30/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 716.8906 - val_loss: 1062.0510\n",
      "Epoch 31/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 706.5972 - val_loss: 981.5700\n",
      "Epoch 32/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 709.1687 - val_loss: 949.2059\n",
      "Epoch 33/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 706.9002 - val_loss: 869.8636\n",
      "Epoch 34/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 687.7382 - val_loss: 900.2022\n",
      "Epoch 35/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 680.3353 - val_loss: 845.5246\n",
      "Epoch 36/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 668.9482 - val_loss: 813.2183\n",
      "Epoch 37/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 671.0183 - val_loss: 812.0126\n",
      "Epoch 38/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 653.1152 - val_loss: 774.6758\n",
      "Epoch 39/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 659.7224 - val_loss: 723.0317\n",
      "Epoch 40/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 640.6066 - val_loss: 771.3748\n",
      "Epoch 41/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 647.8299 - val_loss: 715.5280\n",
      "Epoch 42/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 624.4833 - val_loss: 682.8214\n",
      "Epoch 43/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 626.4451 - val_loss: 706.7409\n",
      "Epoch 44/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 619.2849 - val_loss: 724.2883\n",
      "Epoch 45/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 608.7715 - val_loss: 619.8958\n",
      "Epoch 46/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 602.0017 - val_loss: 572.9688\n",
      "Epoch 47/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 601.6218 - val_loss: 633.2889\n",
      "Epoch 48/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 583.6424 - val_loss: 673.2481\n",
      "Epoch 49/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 576.5670 - val_loss: 691.9460\n",
      "Epoch 50/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 572.1503 - val_loss: 688.0667\n",
      "Epoch 51/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 576.6976 - val_loss: 677.4379\n",
      "Epoch 52/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 573.4453 - val_loss: 664.4536\n",
      "Epoch 53/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 552.6354 - val_loss: 655.4750\n",
      "Epoch 54/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 559.0742 - val_loss: 608.3716\n",
      "Epoch 55/96\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 552.7512 - val_loss: 598.2749\n",
      "Epoch 56/96\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 547.4436 - val_loss: 606.8361\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2131.4919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:55:08,289] Trial 8 finished with value: 2131.491943359375 and parameters: {'head_size': 199, 'num_heads': 5, 'ff_dim': 21, 'num_transformer_blocks': 4, 'mlp_units': 65, 'mlp_dropout': 0.15110437742344196, 'dropout': 0.19301094374435301, 'learning_rate': 9.517577525016992e-05, 'n_epochs': 96}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_96 (La  (None, 8, 5)                 10        ['input_10[0][0]']            \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_48 (M  (None, 8, 5)                 18221     ['layer_normalization_96[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_96[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_105 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_48[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_96 (T  (None, 8, 5)                 0         ['dropout_105[0][0]',         \n",
      " FOpLambda)                                                          'input_10[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_97 (La  (None, 8, 5)                 10        ['tf.__operators__.add_96[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_96 (Conv1D)          (None, 8, 24)                144       ['layer_normalization_97[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_106 (Dropout)       (None, 8, 24)                0         ['conv1d_96[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_97 (Conv1D)          (None, 8, 5)                 125       ['dropout_106[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_97 (T  (None, 8, 5)                 0         ['conv1d_97[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_96[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_98 (La  (None, 8, 5)                 10        ['tf.__operators__.add_97[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_49 (M  (None, 8, 5)                 18221     ['layer_normalization_98[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_107 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_49[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_98 (T  (None, 8, 5)                 0         ['dropout_107[0][0]',         \n",
      " FOpLambda)                                                          'tf.__operators__.add_97[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_99 (La  (None, 8, 5)                 10        ['tf.__operators__.add_98[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " conv1d_98 (Conv1D)          (None, 8, 24)                144       ['layer_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_108 (Dropout)       (None, 8, 24)                0         ['conv1d_98[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_99 (Conv1D)          (None, 8, 5)                 125       ['dropout_108[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_99 (T  (None, 8, 5)                 0         ['conv1d_99[0][0]',           \n",
      " FOpLambda)                                                          'tf.__operators__.add_98[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_100 (L  (None, 8, 5)                 10        ['tf.__operators__.add_99[0][0\n",
      " ayerNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_50 (M  (None, 8, 5)                 18221     ['layer_normalization_100[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_100[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_109 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_50[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_100 (  (None, 8, 5)                 0         ['dropout_109[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_99[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_101 (L  (None, 8, 5)                 10        ['tf.__operators__.add_100[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_100 (Conv1D)         (None, 8, 24)                144       ['layer_normalization_101[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_110 (Dropout)       (None, 8, 24)                0         ['conv1d_100[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_101 (Conv1D)         (None, 8, 5)                 125       ['dropout_110[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_101 (  (None, 8, 5)                 0         ['conv1d_101[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_100[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_102 (L  (None, 8, 5)                 10        ['tf.__operators__.add_101[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_51 (M  (None, 8, 5)                 18221     ['layer_normalization_102[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_102[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_111 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_51[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_102 (  (None, 8, 5)                 0         ['dropout_111[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_101[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_103 (L  (None, 8, 5)                 10        ['tf.__operators__.add_102[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_102 (Conv1D)         (None, 8, 24)                144       ['layer_normalization_103[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_112 (Dropout)       (None, 8, 24)                0         ['conv1d_102[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_103 (Conv1D)         (None, 8, 5)                 125       ['dropout_112[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_103 (  (None, 8, 5)                 0         ['conv1d_103[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_102[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_104 (L  (None, 8, 5)                 10        ['tf.__operators__.add_103[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_52 (M  (None, 8, 5)                 18221     ['layer_normalization_104[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_104[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_113 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_52[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_104 (  (None, 8, 5)                 0         ['dropout_113[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_103[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_105 (L  (None, 8, 5)                 10        ['tf.__operators__.add_104[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_104 (Conv1D)         (None, 8, 24)                144       ['layer_normalization_105[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_114 (Dropout)       (None, 8, 24)                0         ['conv1d_104[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_105 (Conv1D)         (None, 8, 5)                 125       ['dropout_114[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_105 (  (None, 8, 5)                 0         ['conv1d_105[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_104[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_106 (L  (None, 8, 5)                 10        ['tf.__operators__.add_105[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_53 (M  (None, 8, 5)                 18221     ['layer_normalization_106[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_106[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_115 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_53[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_106 (  (None, 8, 5)                 0         ['dropout_115[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_105[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_107 (L  (None, 8, 5)                 10        ['tf.__operators__.add_106[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_106 (Conv1D)         (None, 8, 24)                144       ['layer_normalization_107[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_116 (Dropout)       (None, 8, 24)                0         ['conv1d_106[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_107 (Conv1D)         (None, 8, 5)                 125       ['dropout_116[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_107 (  (None, 8, 5)                 0         ['conv1d_107[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_106[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_108 (L  (None, 8, 5)                 10        ['tf.__operators__.add_107[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_54 (M  (None, 8, 5)                 18221     ['layer_normalization_108[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_108[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_117 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_54[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_108 (  (None, 8, 5)                 0         ['dropout_117[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_107[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_109 (L  (None, 8, 5)                 10        ['tf.__operators__.add_108[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_108 (Conv1D)         (None, 8, 24)                144       ['layer_normalization_109[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_118 (Dropout)       (None, 8, 24)                0         ['conv1d_108[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_109 (Conv1D)         (None, 8, 5)                 125       ['dropout_118[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_109 (  (None, 8, 5)                 0         ['conv1d_109[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_108[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_9  (None, 8)                    0         ['tf.__operators__.add_109[0][\n",
      "  (GlobalAveragePooling1D)                                          0]']                          \n",
      "                                                                                                  \n",
      " dense_27 (Dense)            (None, 124)                  1116      ['global_average_pooling1d_9[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_119 (Dropout)       (None, 124)                  0         ['dense_27[0][0]']            \n",
      "                                                                                                  \n",
      " dense_28 (Dense)            (None, 16)                   2000      ['dropout_119[0][0]']         \n",
      "                                                                                                  \n",
      " dense_29 (Dense)            (None, 8)                    136       ['dense_28[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132822 (518.84 KB)\n",
      "Trainable params: 132822 (518.84 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/109\n",
      "20/20 [==============================] - 3s 46ms/step - loss: 3205.9199 - val_loss: 8679.4678\n",
      "Epoch 2/109\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 3178.1616 - val_loss: 8497.9678\n",
      "Epoch 3/109\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 3110.7559 - val_loss: 8326.4717\n",
      "Epoch 4/109\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 3039.1702 - val_loss: 8157.9277\n",
      "Epoch 5/109\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 2978.1575 - val_loss: 7987.2388\n",
      "Epoch 6/109\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 2963.4324 - val_loss: 7810.3057\n",
      "Epoch 7/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 2896.9324 - val_loss: 7647.5684\n",
      "Epoch 8/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 2867.7378 - val_loss: 7482.9033\n",
      "Epoch 9/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2800.1704 - val_loss: 7315.3135\n",
      "Epoch 10/109\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 2745.0098 - val_loss: 7151.8125\n",
      "Epoch 11/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2718.8542 - val_loss: 6987.6699\n",
      "Epoch 12/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2645.1489 - val_loss: 6822.9253\n",
      "Epoch 13/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 2596.3057 - val_loss: 6665.3774\n",
      "Epoch 14/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2550.4314 - val_loss: 6516.3887\n",
      "Epoch 15/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2510.5493 - val_loss: 6365.7056\n",
      "Epoch 16/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 2478.1953 - val_loss: 6215.7510\n",
      "Epoch 17/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 2434.5403 - val_loss: 6075.0278\n",
      "Epoch 18/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 2375.3708 - val_loss: 5945.2837\n",
      "Epoch 19/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2340.5154 - val_loss: 5825.2305\n",
      "Epoch 20/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2280.0601 - val_loss: 5706.7710\n",
      "Epoch 21/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2262.5200 - val_loss: 5591.8721\n",
      "Epoch 22/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2202.7908 - val_loss: 5480.9619\n",
      "Epoch 23/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 2178.8958 - val_loss: 5368.9883\n",
      "Epoch 24/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2149.0403 - val_loss: 5248.2427\n",
      "Epoch 25/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 2127.4580 - val_loss: 5138.8804\n",
      "Epoch 26/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 2113.4739 - val_loss: 5033.5928\n",
      "Epoch 27/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 2074.6667 - val_loss: 4920.7896\n",
      "Epoch 28/109\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 2025.0244 - val_loss: 4801.1694\n",
      "Epoch 29/109\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1974.3867 - val_loss: 4687.5615\n",
      "Epoch 30/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1950.5867 - val_loss: 4585.9316\n",
      "Epoch 31/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1920.1359 - val_loss: 4491.5068\n",
      "Epoch 32/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1895.9318 - val_loss: 4395.6182\n",
      "Epoch 33/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1882.7506 - val_loss: 4300.2744\n",
      "Epoch 34/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1852.1750 - val_loss: 4209.3022\n",
      "Epoch 35/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1819.4240 - val_loss: 4128.9106\n",
      "Epoch 36/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1814.0769 - val_loss: 4053.8425\n",
      "Epoch 37/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1775.5664 - val_loss: 3980.9700\n",
      "Epoch 38/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1760.9569 - val_loss: 3904.3391\n",
      "Epoch 39/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1747.8738 - val_loss: 3835.4121\n",
      "Epoch 40/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1698.6761 - val_loss: 3761.6362\n",
      "Epoch 41/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1683.3463 - val_loss: 3693.4331\n",
      "Epoch 42/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1695.8177 - val_loss: 3623.7532\n",
      "Epoch 43/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1648.0551 - val_loss: 3558.0469\n",
      "Epoch 44/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1633.0442 - val_loss: 3489.8152\n",
      "Epoch 45/109\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1626.1818 - val_loss: 3428.7129\n",
      "Epoch 46/109\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1614.2145 - val_loss: 3358.3513\n",
      "Epoch 47/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1555.3058 - val_loss: 3290.2422\n",
      "Epoch 48/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1586.7568 - val_loss: 3221.6926\n",
      "Epoch 49/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1556.2870 - val_loss: 3162.9048\n",
      "Epoch 50/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1530.1089 - val_loss: 3102.1519\n",
      "Epoch 51/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1530.2169 - val_loss: 3053.6794\n",
      "Epoch 52/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1500.2433 - val_loss: 2999.8630\n",
      "Epoch 53/109\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1470.8271 - val_loss: 2953.8574\n",
      "Epoch 54/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1467.2111 - val_loss: 2913.7080\n",
      "Epoch 55/109\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1471.8110 - val_loss: 2870.7395\n",
      "Epoch 56/109\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1455.5203 - val_loss: 2810.1011\n",
      "Epoch 57/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1446.2694 - val_loss: 2755.5574\n",
      "Epoch 58/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1415.1583 - val_loss: 2709.0720\n",
      "Epoch 59/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1425.7163 - val_loss: 2666.3704\n",
      "Epoch 60/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1417.5264 - val_loss: 2621.0718\n",
      "Epoch 61/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1366.2155 - val_loss: 2583.4058\n",
      "Epoch 62/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1380.7782 - val_loss: 2548.9973\n",
      "Epoch 63/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1368.0833 - val_loss: 2513.7322\n",
      "Epoch 64/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1344.1260 - val_loss: 2481.1677\n",
      "Epoch 65/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1323.4465 - val_loss: 2437.4348\n",
      "Epoch 66/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1327.9739 - val_loss: 2403.9570\n",
      "Epoch 67/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1307.0288 - val_loss: 2366.8630\n",
      "Epoch 68/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1293.8026 - val_loss: 2332.1082\n",
      "Epoch 69/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1292.7389 - val_loss: 2300.5583\n",
      "Epoch 70/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1266.1550 - val_loss: 2263.6965\n",
      "Epoch 71/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1268.0393 - val_loss: 2229.6533\n",
      "Epoch 72/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1274.4579 - val_loss: 2197.3857\n",
      "Epoch 73/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1249.6155 - val_loss: 2171.9392\n",
      "Epoch 74/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1242.0045 - val_loss: 2140.1377\n",
      "Epoch 75/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1216.8589 - val_loss: 2106.9858\n",
      "Epoch 76/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1221.9686 - val_loss: 2073.1697\n",
      "Epoch 77/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1225.5085 - val_loss: 2042.8982\n",
      "Epoch 78/109\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1192.4615 - val_loss: 2017.0944\n",
      "Epoch 79/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1182.1931 - val_loss: 1989.6454\n",
      "Epoch 80/109\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1183.0122 - val_loss: 1967.5865\n",
      "Epoch 81/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1160.8213 - val_loss: 1944.0981\n",
      "Epoch 82/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1161.7462 - val_loss: 1912.9916\n",
      "Epoch 83/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1146.4021 - val_loss: 1889.6259\n",
      "Epoch 84/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1158.4402 - val_loss: 1849.8474\n",
      "Epoch 85/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1134.7083 - val_loss: 1796.4993\n",
      "Epoch 86/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1131.0848 - val_loss: 1780.6627\n",
      "Epoch 87/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1137.1703 - val_loss: 1764.2521\n",
      "Epoch 88/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1130.3383 - val_loss: 1729.3265\n",
      "Epoch 89/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1119.8351 - val_loss: 1702.2556\n",
      "Epoch 90/109\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1099.7274 - val_loss: 1678.3492\n",
      "Epoch 91/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1094.1108 - val_loss: 1639.4408\n",
      "Epoch 92/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1079.7854 - val_loss: 1610.4208\n",
      "Epoch 93/109\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1073.3087 - val_loss: 1586.6675\n",
      "Epoch 94/109\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1096.1750 - val_loss: 1549.0243\n",
      "Epoch 95/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1091.1444 - val_loss: 1510.4214\n",
      "Epoch 96/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1068.2904 - val_loss: 1497.1224\n",
      "Epoch 97/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1038.9661 - val_loss: 1484.5759\n",
      "Epoch 98/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1076.0343 - val_loss: 1459.1454\n",
      "Epoch 99/109\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1045.4552 - val_loss: 1437.9788\n",
      "Epoch 100/109\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1053.7953 - val_loss: 1424.0021\n",
      "Epoch 101/109\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1049.4110 - val_loss: 1428.7673\n",
      "Epoch 102/109\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1013.0868 - val_loss: 1413.1781\n",
      "Epoch 103/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1036.3859 - val_loss: 1397.0448\n",
      "Epoch 104/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1016.9153 - val_loss: 1377.7891\n",
      "Epoch 105/109\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1015.4576 - val_loss: 1344.7861\n",
      "Epoch 106/109\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 993.9302 - val_loss: 1331.8195\n",
      "Epoch 107/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 998.3309 - val_loss: 1310.0151\n",
      "Epoch 108/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1022.5648 - val_loss: 1293.2877\n",
      "Epoch 109/109\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1015.5960 - val_loss: 1277.9286\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2643.2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:56:43,443] Trial 9 finished with value: 2643.272216796875 and parameters: {'head_size': 132, 'num_heads': 6, 'ff_dim': 24, 'num_transformer_blocks': 7, 'mlp_units': 124, 'mlp_dropout': 0.34685830428035924, 'dropout': 0.2011384730383074, 'learning_rate': 1.3855772559918511e-05, 'n_epochs': 109}. Best is trial 0 with value: 1294.032470703125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_110 (L  (None, 8, 5)                 10        ['input_11[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_55 (M  (None, 8, 5)                 46005     ['layer_normalization_110[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_110[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_120 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_55[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_110 (  (None, 8, 5)                 0         ['dropout_120[0][0]',         \n",
      " TFOpLambda)                                                         'input_11[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_111 (L  (None, 8, 5)                 10        ['tf.__operators__.add_110[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_110 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_111[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_121 (Dropout)       (None, 8, 9)                 0         ['conv1d_110[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_111 (Conv1D)         (None, 8, 5)                 50        ['dropout_121[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_111 (  (None, 8, 5)                 0         ['conv1d_111[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_110[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_112 (L  (None, 8, 5)                 10        ['tf.__operators__.add_111[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_56 (M  (None, 8, 5)                 46005     ['layer_normalization_112[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_112[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_122 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_56[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_112 (  (None, 8, 5)                 0         ['dropout_122[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_111[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_113 (L  (None, 8, 5)                 10        ['tf.__operators__.add_112[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_112 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_113[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_123 (Dropout)       (None, 8, 9)                 0         ['conv1d_112[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_113 (Conv1D)         (None, 8, 5)                 50        ['dropout_123[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_113 (  (None, 8, 5)                 0         ['conv1d_113[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_112[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_114 (L  (None, 8, 5)                 10        ['tf.__operators__.add_113[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_57 (M  (None, 8, 5)                 46005     ['layer_normalization_114[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_114[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_124 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_57[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_114 (  (None, 8, 5)                 0         ['dropout_124[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_113[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_115 (L  (None, 8, 5)                 10        ['tf.__operators__.add_114[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_114 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_115[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_125 (Dropout)       (None, 8, 9)                 0         ['conv1d_114[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_115 (Conv1D)         (None, 8, 5)                 50        ['dropout_125[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_115 (  (None, 8, 5)                 0         ['conv1d_115[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_114[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_116 (L  (None, 8, 5)                 10        ['tf.__operators__.add_115[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_58 (M  (None, 8, 5)                 46005     ['layer_normalization_116[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_116[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_126 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_58[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_116 (  (None, 8, 5)                 0         ['dropout_126[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_115[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_117 (L  (None, 8, 5)                 10        ['tf.__operators__.add_116[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_116 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_117[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_127 (Dropout)       (None, 8, 9)                 0         ['conv1d_116[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_117 (Conv1D)         (None, 8, 5)                 50        ['dropout_127[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_117 (  (None, 8, 5)                 0         ['conv1d_117[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_116[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_117[0][\n",
      " 0 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_30 (Dense)            (None, 111)                  999       ['global_average_pooling1d_10[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_128 (Dropout)       (None, 111)                  0         ['dense_30[0][0]']            \n",
      "                                                                                                  \n",
      " dense_31 (Dense)            (None, 16)                   1792      ['dropout_128[0][0]']         \n",
      "                                                                                                  \n",
      " dense_32 (Dense)            (None, 8)                    136       ['dense_31[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 187443 (732.20 KB)\n",
      "Trainable params: 187443 (732.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/190\n",
      "20/20 [==============================] - 2s 43ms/step - loss: 2150.5312 - val_loss: 4519.3120\n",
      "Epoch 2/190\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1129.4614 - val_loss: 2378.4434\n",
      "Epoch 3/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 608.3183 - val_loss: 681.6376\n",
      "Epoch 4/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 489.3114 - val_loss: 631.8903\n",
      "Epoch 5/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 443.6649 - val_loss: 505.7441\n",
      "Epoch 6/190\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 431.0968 - val_loss: 558.6517\n",
      "Epoch 7/190\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 401.3099 - val_loss: 540.7695\n",
      "Epoch 8/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 382.8380 - val_loss: 584.3571\n",
      "Epoch 9/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 356.2810 - val_loss: 517.7413\n",
      "Epoch 10/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 329.6762 - val_loss: 593.1144\n",
      "Epoch 11/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 316.2939 - val_loss: 552.1765\n",
      "Epoch 12/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 305.7220 - val_loss: 511.8082\n",
      "Epoch 13/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 299.4151 - val_loss: 558.3393\n",
      "Epoch 14/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 287.8599 - val_loss: 471.5887\n",
      "Epoch 15/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 276.1029 - val_loss: 547.9360\n",
      "Epoch 16/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 259.1924 - val_loss: 434.8871\n",
      "Epoch 17/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 248.8973 - val_loss: 432.1465\n",
      "Epoch 18/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 240.0472 - val_loss: 432.7260\n",
      "Epoch 19/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 228.8265 - val_loss: 396.4591\n",
      "Epoch 20/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 225.7166 - val_loss: 391.1936\n",
      "Epoch 21/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 214.3682 - val_loss: 368.2614\n",
      "Epoch 22/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 210.4138 - val_loss: 357.4730\n",
      "Epoch 23/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 203.7465 - val_loss: 347.6839\n",
      "Epoch 24/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 200.3471 - val_loss: 375.1778\n",
      "Epoch 25/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 193.1570 - val_loss: 411.0410\n",
      "Epoch 26/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 183.5513 - val_loss: 422.6639\n",
      "Epoch 27/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 181.4301 - val_loss: 383.6035\n",
      "Epoch 28/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 176.4867 - val_loss: 334.9473\n",
      "Epoch 29/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 174.5262 - val_loss: 353.9453\n",
      "Epoch 30/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 167.3440 - val_loss: 417.9366\n",
      "Epoch 31/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 165.1492 - val_loss: 329.3477\n",
      "Epoch 32/190\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 161.6462 - val_loss: 321.5380\n",
      "Epoch 33/190\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 156.8769 - val_loss: 335.3726\n",
      "Epoch 34/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 160.9590 - val_loss: 324.3107\n",
      "Epoch 35/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 147.8151 - val_loss: 343.0059\n",
      "Epoch 36/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 147.9798 - val_loss: 436.7177\n",
      "Epoch 37/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 147.6651 - val_loss: 385.1320\n",
      "Epoch 38/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 142.3576 - val_loss: 350.6500\n",
      "Epoch 39/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 145.9104 - val_loss: 340.5191\n",
      "Epoch 40/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 141.8046 - val_loss: 316.7025\n",
      "Epoch 41/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 141.3621 - val_loss: 327.6727\n",
      "Epoch 42/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 136.8119 - val_loss: 315.3638\n",
      "Epoch 43/190\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 136.0223 - val_loss: 334.5638\n",
      "Epoch 44/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 133.6463 - val_loss: 330.0375\n",
      "Epoch 45/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 129.5892 - val_loss: 380.3239\n",
      "Epoch 46/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 131.3417 - val_loss: 324.3730\n",
      "Epoch 47/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 127.7381 - val_loss: 347.8796\n",
      "Epoch 48/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 127.3544 - val_loss: 313.3349\n",
      "Epoch 49/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 127.3400 - val_loss: 393.8336\n",
      "Epoch 50/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 130.9993 - val_loss: 310.6823\n",
      "Epoch 51/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 124.6024 - val_loss: 322.6168\n",
      "Epoch 52/190\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 123.8745 - val_loss: 283.7898\n",
      "Epoch 53/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 118.7192 - val_loss: 317.8243\n",
      "Epoch 54/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 119.6789 - val_loss: 301.7811\n",
      "Epoch 55/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 123.2853 - val_loss: 339.3469\n",
      "Epoch 56/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 118.9218 - val_loss: 334.7004\n",
      "Epoch 57/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 118.5599 - val_loss: 297.6445\n",
      "Epoch 58/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 116.8214 - val_loss: 277.6065\n",
      "Epoch 59/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 110.9330 - val_loss: 324.0515\n",
      "Epoch 60/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 113.3689 - val_loss: 283.4724\n",
      "Epoch 61/190\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 113.6393 - val_loss: 289.5541\n",
      "Epoch 62/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 110.2625 - val_loss: 290.2949\n",
      "Epoch 63/190\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 113.6218 - val_loss: 285.4366\n",
      "Epoch 64/190\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 109.2992 - val_loss: 329.1815\n",
      "Epoch 65/190\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 113.9197 - val_loss: 288.3996\n",
      "Epoch 66/190\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 112.1244 - val_loss: 308.2129\n",
      "Epoch 67/190\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 108.0109 - val_loss: 304.1256\n",
      "Epoch 68/190\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 110.9456 - val_loss: 300.0769\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 895.7382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:57:38,065] Trial 10 finished with value: 895.7382202148438 and parameters: {'head_size': 250, 'num_heads': 8, 'ff_dim': 9, 'num_transformer_blocks': 4, 'mlp_units': 111, 'mlp_dropout': 0.10027779492932605, 'dropout': 0.2928629154004858, 'learning_rate': 0.0006996251508671432, 'n_epochs': 190}. Best is trial 10 with value: 895.7382202148438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_118 (L  (None, 8, 5)                 10        ['input_12[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_59 (M  (None, 8, 5)                 46373     ['layer_normalization_118[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_118[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_129 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_59[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_118 (  (None, 8, 5)                 0         ['dropout_129[0][0]',         \n",
      " TFOpLambda)                                                         'input_12[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_119 (L  (None, 8, 5)                 10        ['tf.__operators__.add_118[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_118 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_119[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_130 (Dropout)       (None, 8, 8)                 0         ['conv1d_118[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_119 (Conv1D)         (None, 8, 5)                 45        ['dropout_130[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_119 (  (None, 8, 5)                 0         ['conv1d_119[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_118[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_120 (L  (None, 8, 5)                 10        ['tf.__operators__.add_119[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_60 (M  (None, 8, 5)                 46373     ['layer_normalization_120[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_120[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_131 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_60[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_120 (  (None, 8, 5)                 0         ['dropout_131[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_119[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_121 (L  (None, 8, 5)                 10        ['tf.__operators__.add_120[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_120 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_121[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_132 (Dropout)       (None, 8, 8)                 0         ['conv1d_120[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_121 (Conv1D)         (None, 8, 5)                 45        ['dropout_132[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_121 (  (None, 8, 5)                 0         ['conv1d_121[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_120[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_122 (L  (None, 8, 5)                 10        ['tf.__operators__.add_121[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_61 (M  (None, 8, 5)                 46373     ['layer_normalization_122[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_122[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_133 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_61[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_122 (  (None, 8, 5)                 0         ['dropout_133[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_121[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_123 (L  (None, 8, 5)                 10        ['tf.__operators__.add_122[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_122 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_123[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_134 (Dropout)       (None, 8, 8)                 0         ['conv1d_122[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_123 (Conv1D)         (None, 8, 5)                 45        ['dropout_134[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_123 (  (None, 8, 5)                 0         ['conv1d_123[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_122[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_124 (L  (None, 8, 5)                 10        ['tf.__operators__.add_123[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_62 (M  (None, 8, 5)                 46373     ['layer_normalization_124[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_124[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_135 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_62[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_124 (  (None, 8, 5)                 0         ['dropout_135[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_123[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_125 (L  (None, 8, 5)                 10        ['tf.__operators__.add_124[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_124 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_125[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_136 (Dropout)       (None, 8, 8)                 0         ['conv1d_124[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_125 (Conv1D)         (None, 8, 5)                 45        ['dropout_136[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_125 (  (None, 8, 5)                 0         ['conv1d_125[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_124[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_125[0][\n",
      " 1 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_33 (Dense)            (None, 108)                  972       ['global_average_pooling1d_11[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_137 (Dropout)       (None, 108)                  0         ['dense_33[0][0]']            \n",
      "                                                                                                  \n",
      " dense_34 (Dense)            (None, 16)                   1744      ['dropout_137[0][0]']         \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 8)                    136       ['dense_34[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 188796 (737.48 KB)\n",
      "Trainable params: 188796 (737.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/197\n",
      "20/20 [==============================] - 2s 48ms/step - loss: 1997.2180 - val_loss: 3424.4988\n",
      "Epoch 2/197\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 974.5874 - val_loss: 857.2829\n",
      "Epoch 3/197\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 610.6926 - val_loss: 1102.7809\n",
      "Epoch 4/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 500.3318 - val_loss: 458.1878\n",
      "Epoch 5/197\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 423.8902 - val_loss: 528.5163\n",
      "Epoch 6/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 392.7318 - val_loss: 369.5254\n",
      "Epoch 7/197\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 373.7223 - val_loss: 365.0221\n",
      "Epoch 8/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 345.9550 - val_loss: 509.9461\n",
      "Epoch 9/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 332.6956 - val_loss: 582.3876\n",
      "Epoch 10/197\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 311.1076 - val_loss: 407.9390\n",
      "Epoch 11/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 291.1896 - val_loss: 504.0881\n",
      "Epoch 12/197\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 277.1019 - val_loss: 347.4553\n",
      "Epoch 13/197\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 265.6197 - val_loss: 345.5416\n",
      "Epoch 14/197\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 251.6853 - val_loss: 536.2273\n",
      "Epoch 15/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 251.0560 - val_loss: 428.8571\n",
      "Epoch 16/197\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 228.7641 - val_loss: 413.7877\n",
      "Epoch 17/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 218.8725 - val_loss: 400.3451\n",
      "Epoch 18/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 205.2804 - val_loss: 397.7103\n",
      "Epoch 19/197\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 199.4984 - val_loss: 337.5401\n",
      "Epoch 20/197\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 197.6965 - val_loss: 350.5796\n",
      "Epoch 21/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 191.9573 - val_loss: 427.8573\n",
      "Epoch 22/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 190.9865 - val_loss: 367.8031\n",
      "Epoch 23/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 181.5808 - val_loss: 359.6161\n",
      "Epoch 24/197\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 173.7074 - val_loss: 383.2599\n",
      "Epoch 25/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 173.8727 - val_loss: 323.4229\n",
      "Epoch 26/197\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 164.3488 - val_loss: 327.1802\n",
      "Epoch 27/197\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 165.7667 - val_loss: 351.4211\n",
      "Epoch 28/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 160.4927 - val_loss: 279.7823\n",
      "Epoch 29/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 161.1993 - val_loss: 329.4737\n",
      "Epoch 30/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 151.9949 - val_loss: 363.0533\n",
      "Epoch 31/197\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 150.8589 - val_loss: 323.4568\n",
      "Epoch 32/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 146.1418 - val_loss: 366.7301\n",
      "Epoch 33/197\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 140.8076 - val_loss: 356.2577\n",
      "Epoch 34/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 143.8470 - val_loss: 329.0235\n",
      "Epoch 35/197\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 142.4760 - val_loss: 333.9418\n",
      "Epoch 36/197\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 143.8910 - val_loss: 326.9065\n",
      "Epoch 37/197\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 139.3061 - val_loss: 304.6736\n",
      "Epoch 38/197\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 134.6394 - val_loss: 304.6080\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 938.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:58:07,697] Trial 11 finished with value: 938.586669921875 and parameters: {'head_size': 252, 'num_heads': 8, 'ff_dim': 8, 'num_transformer_blocks': 4, 'mlp_units': 108, 'mlp_dropout': 0.10331353153207999, 'dropout': 0.30061770341830923, 'learning_rate': 0.0008979855084808416, 'n_epochs': 197}. Best is trial 10 with value: 895.7382202148438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_126 (L  (None, 8, 5)                 10        ['input_13[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_63 (M  (None, 8, 5)                 46741     ['layer_normalization_126[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_126[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_138 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_63[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_126 (  (None, 8, 5)                 0         ['dropout_138[0][0]',         \n",
      " TFOpLambda)                                                         'input_13[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_127 (L  (None, 8, 5)                 10        ['tf.__operators__.add_126[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_126 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_127[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_139 (Dropout)       (None, 8, 8)                 0         ['conv1d_126[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_127 (Conv1D)         (None, 8, 5)                 45        ['dropout_139[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_127 (  (None, 8, 5)                 0         ['conv1d_127[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_126[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_128 (L  (None, 8, 5)                 10        ['tf.__operators__.add_127[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_64 (M  (None, 8, 5)                 46741     ['layer_normalization_128[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_128[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_140 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_64[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_128 (  (None, 8, 5)                 0         ['dropout_140[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_127[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_129 (L  (None, 8, 5)                 10        ['tf.__operators__.add_128[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_128 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_129[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_141 (Dropout)       (None, 8, 8)                 0         ['conv1d_128[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_129 (Conv1D)         (None, 8, 5)                 45        ['dropout_141[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_129 (  (None, 8, 5)                 0         ['conv1d_129[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_128[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_130 (L  (None, 8, 5)                 10        ['tf.__operators__.add_129[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_65 (M  (None, 8, 5)                 46741     ['layer_normalization_130[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_130[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_142 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_65[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_130 (  (None, 8, 5)                 0         ['dropout_142[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_129[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_131 (L  (None, 8, 5)                 10        ['tf.__operators__.add_130[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_130 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_131[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_143 (Dropout)       (None, 8, 8)                 0         ['conv1d_130[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_131 (Conv1D)         (None, 8, 5)                 45        ['dropout_143[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_131 (  (None, 8, 5)                 0         ['conv1d_131[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_130[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_132 (L  (None, 8, 5)                 10        ['tf.__operators__.add_131[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_66 (M  (None, 8, 5)                 46741     ['layer_normalization_132[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_132[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_144 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_66[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_132 (  (None, 8, 5)                 0         ['dropout_144[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_131[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_133 (L  (None, 8, 5)                 10        ['tf.__operators__.add_132[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_132 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_133[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_145 (Dropout)       (None, 8, 8)                 0         ['conv1d_132[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_133 (Conv1D)         (None, 8, 5)                 45        ['dropout_145[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_133 (  (None, 8, 5)                 0         ['conv1d_133[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_132[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_134 (L  (None, 8, 5)                 10        ['tf.__operators__.add_133[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_67 (M  (None, 8, 5)                 46741     ['layer_normalization_134[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_134[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_146 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_67[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_134 (  (None, 8, 5)                 0         ['dropout_146[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_133[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_135 (L  (None, 8, 5)                 10        ['tf.__operators__.add_134[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_134 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_135[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_147 (Dropout)       (None, 8, 8)                 0         ['conv1d_134[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_135 (Conv1D)         (None, 8, 5)                 45        ['dropout_147[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_135 (  (None, 8, 5)                 0         ['conv1d_135[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_134[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_135[0][\n",
      " 2 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_36 (Dense)            (None, 113)                  1017      ['global_average_pooling1d_12[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_148 (Dropout)       (None, 113)                  0         ['dense_36[0][0]']            \n",
      "                                                                                                  \n",
      " dense_37 (Dense)            (None, 16)                   1824      ['dropout_148[0][0]']         \n",
      "                                                                                                  \n",
      " dense_38 (Dense)            (None, 8)                    136       ['dense_37[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 237247 (926.75 KB)\n",
      "Trainable params: 237247 (926.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/198\n",
      "20/20 [==============================] - 3s 52ms/step - loss: 1670.7385 - val_loss: 2517.9849\n",
      "Epoch 2/198\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 901.6873 - val_loss: 1994.4154\n",
      "Epoch 3/198\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 652.4486 - val_loss: 926.2552\n",
      "Epoch 4/198\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 503.1071 - val_loss: 382.6064\n",
      "Epoch 5/198\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 441.8628 - val_loss: 570.5068\n",
      "Epoch 6/198\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 402.3794 - val_loss: 351.4518\n",
      "Epoch 7/198\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 368.3662 - val_loss: 390.6311\n",
      "Epoch 8/198\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 337.8865 - val_loss: 353.4789\n",
      "Epoch 9/198\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 308.5784 - val_loss: 444.7899\n",
      "Epoch 10/198\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 290.7029 - val_loss: 273.8320\n",
      "Epoch 11/198\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 278.8937 - val_loss: 363.2137\n",
      "Epoch 12/198\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 262.8127 - val_loss: 368.3134\n",
      "Epoch 13/198\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 248.5071 - val_loss: 326.7957\n",
      "Epoch 14/198\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 237.6619 - val_loss: 381.6117\n",
      "Epoch 15/198\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 225.6566 - val_loss: 409.9372\n",
      "Epoch 16/198\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 224.5330 - val_loss: 372.4814\n",
      "Epoch 17/198\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 213.2852 - val_loss: 339.2277\n",
      "Epoch 18/198\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 203.4762 - val_loss: 337.4277\n",
      "Epoch 19/198\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 197.4603 - val_loss: 329.9066\n",
      "Epoch 20/198\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 190.5338 - val_loss: 371.2560\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 845.5035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:58:28,321] Trial 12 finished with value: 845.5035400390625 and parameters: {'head_size': 254, 'num_heads': 8, 'ff_dim': 8, 'num_transformer_blocks': 5, 'mlp_units': 113, 'mlp_dropout': 0.16544108961239506, 'dropout': 0.313133413557173, 'learning_rate': 0.0009594903526381891, 'n_epochs': 198}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_136 (L  (None, 8, 5)                 10        ['input_14[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_68 (M  (None, 8, 5)                 44165     ['layer_normalization_136[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_136[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_149 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_68[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_136 (  (None, 8, 5)                 0         ['dropout_149[0][0]',         \n",
      " TFOpLambda)                                                         'input_14[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_137 (L  (None, 8, 5)                 10        ['tf.__operators__.add_136[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_136 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_137[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_150 (Dropout)       (None, 8, 8)                 0         ['conv1d_136[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_137 (Conv1D)         (None, 8, 5)                 45        ['dropout_150[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_137 (  (None, 8, 5)                 0         ['conv1d_137[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_136[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_138 (L  (None, 8, 5)                 10        ['tf.__operators__.add_137[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_69 (M  (None, 8, 5)                 44165     ['layer_normalization_138[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_138[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_151 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_69[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_138 (  (None, 8, 5)                 0         ['dropout_151[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_137[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_139 (L  (None, 8, 5)                 10        ['tf.__operators__.add_138[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_138 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_139[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_152 (Dropout)       (None, 8, 8)                 0         ['conv1d_138[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_139 (Conv1D)         (None, 8, 5)                 45        ['dropout_152[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_139 (  (None, 8, 5)                 0         ['conv1d_139[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_138[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_140 (L  (None, 8, 5)                 10        ['tf.__operators__.add_139[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_70 (M  (None, 8, 5)                 44165     ['layer_normalization_140[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_140[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_153 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_70[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_140 (  (None, 8, 5)                 0         ['dropout_153[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_139[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_141 (L  (None, 8, 5)                 10        ['tf.__operators__.add_140[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_140 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_141[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_154 (Dropout)       (None, 8, 8)                 0         ['conv1d_140[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_141 (Conv1D)         (None, 8, 5)                 45        ['dropout_154[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_141 (  (None, 8, 5)                 0         ['conv1d_141[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_140[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_142 (L  (None, 8, 5)                 10        ['tf.__operators__.add_141[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_71 (M  (None, 8, 5)                 44165     ['layer_normalization_142[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_142[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_155 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_71[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_142 (  (None, 8, 5)                 0         ['dropout_155[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_141[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_143 (L  (None, 8, 5)                 10        ['tf.__operators__.add_142[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_142 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_143[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_156 (Dropout)       (None, 8, 8)                 0         ['conv1d_142[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_143 (Conv1D)         (None, 8, 5)                 45        ['dropout_156[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_143 (  (None, 8, 5)                 0         ['conv1d_143[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_142[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_144 (L  (None, 8, 5)                 10        ['tf.__operators__.add_143[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_72 (M  (None, 8, 5)                 44165     ['layer_normalization_144[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_144[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_157 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_72[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_144 (  (None, 8, 5)                 0         ['dropout_157[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_143[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_145 (L  (None, 8, 5)                 10        ['tf.__operators__.add_144[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_144 (Conv1D)         (None, 8, 8)                 48        ['layer_normalization_145[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_158 (Dropout)       (None, 8, 8)                 0         ['conv1d_144[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_145 (Conv1D)         (None, 8, 5)                 45        ['dropout_158[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_145 (  (None, 8, 5)                 0         ['conv1d_145[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_144[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_145[0][\n",
      " 3 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_39 (Dense)            (None, 112)                  1008      ['global_average_pooling1d_13[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_159 (Dropout)       (None, 112)                  0         ['dense_39[0][0]']            \n",
      "                                                                                                  \n",
      " dense_40 (Dense)            (None, 16)                   1808      ['dropout_159[0][0]']         \n",
      "                                                                                                  \n",
      " dense_41 (Dense)            (None, 8)                    136       ['dense_40[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 224342 (876.34 KB)\n",
      "Trainable params: 224342 (876.34 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/175\n",
      "20/20 [==============================] - 2s 54ms/step - loss: 1687.7017 - val_loss: 1550.2985\n",
      "Epoch 2/175\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 765.8348 - val_loss: 1411.8711\n",
      "Epoch 3/175\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 592.8468 - val_loss: 395.2549\n",
      "Epoch 4/175\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 521.2616 - val_loss: 510.2271\n",
      "Epoch 5/175\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 475.1729 - val_loss: 336.5887\n",
      "Epoch 6/175\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 450.6230 - val_loss: 257.5816\n",
      "Epoch 7/175\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 405.0048 - val_loss: 438.5283\n",
      "Epoch 8/175\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 388.1658 - val_loss: 475.2005\n",
      "Epoch 9/175\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 363.1364 - val_loss: 463.1629\n",
      "Epoch 10/175\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 335.0896 - val_loss: 337.8806\n",
      "Epoch 11/175\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 311.3936 - val_loss: 323.5205\n",
      "Epoch 12/175\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 294.7786 - val_loss: 309.0421\n",
      "Epoch 13/175\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 281.2798 - val_loss: 405.6929\n",
      "Epoch 14/175\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 271.6100 - val_loss: 420.1133\n",
      "Epoch 15/175\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 254.9495 - val_loss: 278.2325\n",
      "Epoch 16/175\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 240.6960 - val_loss: 387.7280\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 986.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:58:44,618] Trial 13 finished with value: 986.80615234375 and parameters: {'head_size': 240, 'num_heads': 8, 'ff_dim': 8, 'num_transformer_blocks': 5, 'mlp_units': 112, 'mlp_dropout': 0.16772055415894435, 'dropout': 0.33709598324331574, 'learning_rate': 0.0008687000377077636, 'n_epochs': 175}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_146 (L  (None, 8, 5)                 10        ['input_15[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_73 (M  (None, 8, 5)                 41037     ['layer_normalization_146[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_146[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_160 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_73[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_146 (  (None, 8, 5)                 0         ['dropout_160[0][0]',         \n",
      " TFOpLambda)                                                         'input_15[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_147 (L  (None, 8, 5)                 10        ['tf.__operators__.add_146[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_146 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_147[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_161 (Dropout)       (None, 8, 11)                0         ['conv1d_146[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_147 (Conv1D)         (None, 8, 5)                 60        ['dropout_161[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_147 (  (None, 8, 5)                 0         ['conv1d_147[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_146[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_148 (L  (None, 8, 5)                 10        ['tf.__operators__.add_147[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_74 (M  (None, 8, 5)                 41037     ['layer_normalization_148[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_148[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_162 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_74[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_148 (  (None, 8, 5)                 0         ['dropout_162[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_147[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_149 (L  (None, 8, 5)                 10        ['tf.__operators__.add_148[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_148 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_149[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_163 (Dropout)       (None, 8, 11)                0         ['conv1d_148[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_149 (Conv1D)         (None, 8, 5)                 60        ['dropout_163[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_149 (  (None, 8, 5)                 0         ['conv1d_149[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_148[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_150 (L  (None, 8, 5)                 10        ['tf.__operators__.add_149[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_75 (M  (None, 8, 5)                 41037     ['layer_normalization_150[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_150[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_164 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_75[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_150 (  (None, 8, 5)                 0         ['dropout_164[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_149[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_151 (L  (None, 8, 5)                 10        ['tf.__operators__.add_150[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_150 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_151[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_165 (Dropout)       (None, 8, 11)                0         ['conv1d_150[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_151 (Conv1D)         (None, 8, 5)                 60        ['dropout_165[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_151 (  (None, 8, 5)                 0         ['conv1d_151[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_150[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_152 (L  (None, 8, 5)                 10        ['tf.__operators__.add_151[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_76 (M  (None, 8, 5)                 41037     ['layer_normalization_152[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_152[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_166 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_76[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_152 (  (None, 8, 5)                 0         ['dropout_166[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_151[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_153 (L  (None, 8, 5)                 10        ['tf.__operators__.add_152[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_152 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_153[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_167 (Dropout)       (None, 8, 11)                0         ['conv1d_152[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_153 (Conv1D)         (None, 8, 5)                 60        ['dropout_167[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_153 (  (None, 8, 5)                 0         ['conv1d_153[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_152[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_154 (L  (None, 8, 5)                 10        ['tf.__operators__.add_153[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_77 (M  (None, 8, 5)                 41037     ['layer_normalization_154[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_154[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_168 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_77[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_154 (  (None, 8, 5)                 0         ['dropout_168[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_153[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_155 (L  (None, 8, 5)                 10        ['tf.__operators__.add_154[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_154 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_155[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_169 (Dropout)       (None, 8, 11)                0         ['conv1d_154[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_155 (Conv1D)         (None, 8, 5)                 60        ['dropout_169[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_155 (  (None, 8, 5)                 0         ['conv1d_155[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_154[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_155[0][\n",
      " 4 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_42 (Dense)            (None, 89)                   801       ['global_average_pooling1d_14[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_170 (Dropout)       (None, 89)                   0         ['dense_42[0][0]']            \n",
      "                                                                                                  \n",
      " dense_43 (Dense)            (None, 16)                   1440      ['dropout_170[0][0]']         \n",
      "                                                                                                  \n",
      " dense_44 (Dense)            (None, 8)                    136       ['dense_43[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 208292 (813.64 KB)\n",
      "Trainable params: 208292 (813.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/179\n",
      "20/20 [==============================] - 2s 54ms/step - loss: 3001.6555 - val_loss: 5756.7197\n",
      "Epoch 2/179\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 2142.6445 - val_loss: 3452.7815\n",
      "Epoch 3/179\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1564.5249 - val_loss: 2055.7998\n",
      "Epoch 4/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1241.2532 - val_loss: 1564.6917\n",
      "Epoch 5/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1105.4438 - val_loss: 1501.1470\n",
      "Epoch 6/179\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1036.6306 - val_loss: 1563.4987\n",
      "Epoch 7/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 971.2323 - val_loss: 1441.6703\n",
      "Epoch 8/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 918.3188 - val_loss: 1278.6318\n",
      "Epoch 9/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 876.8992 - val_loss: 1055.2268\n",
      "Epoch 10/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 846.5746 - val_loss: 1006.3219\n",
      "Epoch 11/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 805.2791 - val_loss: 829.7275\n",
      "Epoch 12/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 769.5018 - val_loss: 668.8503\n",
      "Epoch 13/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 742.2317 - val_loss: 633.1885\n",
      "Epoch 14/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 702.6714 - val_loss: 632.3842\n",
      "Epoch 15/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 677.7634 - val_loss: 685.5450\n",
      "Epoch 16/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 657.1746 - val_loss: 605.5652\n",
      "Epoch 17/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 640.0818 - val_loss: 610.8154\n",
      "Epoch 18/179\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 613.6414 - val_loss: 558.7501\n",
      "Epoch 19/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 585.1827 - val_loss: 479.2535\n",
      "Epoch 20/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 571.9373 - val_loss: 497.9854\n",
      "Epoch 21/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 551.1086 - val_loss: 493.8687\n",
      "Epoch 22/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 541.0193 - val_loss: 446.3600\n",
      "Epoch 23/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 504.8254 - val_loss: 454.9350\n",
      "Epoch 24/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 492.4620 - val_loss: 541.1016\n",
      "Epoch 25/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 491.1500 - val_loss: 457.5522\n",
      "Epoch 26/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 480.1099 - val_loss: 414.9726\n",
      "Epoch 27/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 454.8336 - val_loss: 500.4446\n",
      "Epoch 28/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 448.0484 - val_loss: 414.3183\n",
      "Epoch 29/179\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 435.9976 - val_loss: 447.8900\n",
      "Epoch 30/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 424.8190 - val_loss: 425.8197\n",
      "Epoch 31/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 421.3617 - val_loss: 466.0677\n",
      "Epoch 32/179\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 403.3589 - val_loss: 423.8819\n",
      "Epoch 33/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 395.7813 - val_loss: 402.1345\n",
      "Epoch 34/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 394.0005 - val_loss: 459.0393\n",
      "Epoch 35/179\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 376.4732 - val_loss: 494.1840\n",
      "Epoch 36/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 369.7336 - val_loss: 396.7128\n",
      "Epoch 37/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 365.2304 - val_loss: 403.4810\n",
      "Epoch 38/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 361.0039 - val_loss: 416.4496\n",
      "Epoch 39/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 353.9303 - val_loss: 367.1558\n",
      "Epoch 40/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 344.9900 - val_loss: 392.9352\n",
      "Epoch 41/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 334.7091 - val_loss: 368.7831\n",
      "Epoch 42/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 336.0272 - val_loss: 438.2025\n",
      "Epoch 43/179\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 328.3041 - val_loss: 385.2967\n",
      "Epoch 44/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 321.0777 - val_loss: 394.8632\n",
      "Epoch 45/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 318.8122 - val_loss: 378.9907\n",
      "Epoch 46/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 311.9107 - val_loss: 409.4912\n",
      "Epoch 47/179\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 304.1660 - val_loss: 401.8759\n",
      "Epoch 48/179\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 308.7828 - val_loss: 408.9920\n",
      "Epoch 49/179\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 308.9661 - val_loss: 380.5817\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1094.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 16:59:30,816] Trial 14 finished with value: 1094.1221923828125 and parameters: {'head_size': 223, 'num_heads': 8, 'ff_dim': 11, 'num_transformer_blocks': 5, 'mlp_units': 89, 'mlp_dropout': 0.24039131960996263, 'dropout': 0.30180120323895326, 'learning_rate': 0.00029249579359547084, 'n_epochs': 179}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_156 (L  (None, 8, 5)                 10        ['input_16[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_78 (M  (None, 8, 5)                 34620     ['layer_normalization_156[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_156[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_171 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_78[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_156 (  (None, 8, 5)                 0         ['dropout_171[0][0]',         \n",
      " TFOpLambda)                                                         'input_16[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_157 (L  (None, 8, 5)                 10        ['tf.__operators__.add_156[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_156 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_157[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_172 (Dropout)       (None, 8, 12)                0         ['conv1d_156[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_157 (Conv1D)         (None, 8, 5)                 65        ['dropout_172[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_157 (  (None, 8, 5)                 0         ['conv1d_157[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_156[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_158 (L  (None, 8, 5)                 10        ['tf.__operators__.add_157[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_79 (M  (None, 8, 5)                 34620     ['layer_normalization_158[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_158[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_173 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_79[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_158 (  (None, 8, 5)                 0         ['dropout_173[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_157[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_159 (L  (None, 8, 5)                 10        ['tf.__operators__.add_158[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_158 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_159[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_174 (Dropout)       (None, 8, 12)                0         ['conv1d_158[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_159 (Conv1D)         (None, 8, 5)                 65        ['dropout_174[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_159 (  (None, 8, 5)                 0         ['conv1d_159[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_158[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_160 (L  (None, 8, 5)                 10        ['tf.__operators__.add_159[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_80 (M  (None, 8, 5)                 34620     ['layer_normalization_160[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_160[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_175 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_80[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_160 (  (None, 8, 5)                 0         ['dropout_175[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_159[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_161 (L  (None, 8, 5)                 10        ['tf.__operators__.add_160[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_160 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_161[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_176 (Dropout)       (None, 8, 12)                0         ['conv1d_160[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_161 (Conv1D)         (None, 8, 5)                 65        ['dropout_176[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_161 (  (None, 8, 5)                 0         ['conv1d_161[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_160[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_162 (L  (None, 8, 5)                 10        ['tf.__operators__.add_161[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_81 (M  (None, 8, 5)                 34620     ['layer_normalization_162[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_162[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_177 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_81[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_162 (  (None, 8, 5)                 0         ['dropout_177[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_161[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_163 (L  (None, 8, 5)                 10        ['tf.__operators__.add_162[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_162 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_163[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_178 (Dropout)       (None, 8, 12)                0         ['conv1d_162[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_163 (Conv1D)         (None, 8, 5)                 65        ['dropout_178[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_163 (  (None, 8, 5)                 0         ['conv1d_163[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_162[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_164 (L  (None, 8, 5)                 10        ['tf.__operators__.add_163[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_82 (M  (None, 8, 5)                 34620     ['layer_normalization_164[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_164[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_179 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_82[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_164 (  (None, 8, 5)                 0         ['dropout_179[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_163[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_165 (L  (None, 8, 5)                 10        ['tf.__operators__.add_164[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_164 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_165[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_180 (Dropout)       (None, 8, 12)                0         ['conv1d_164[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_165 (Conv1D)         (None, 8, 5)                 65        ['dropout_180[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_165 (  (None, 8, 5)                 0         ['conv1d_165[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_164[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_165[0][\n",
      " 5 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_45 (Dense)            (None, 108)                  972       ['global_average_pooling1d_15[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_181 (Dropout)       (None, 108)                  0         ['dense_45[0][0]']            \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 16)                   1744      ['dropout_181[0][0]']         \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 8)                    136       ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 176737 (690.38 KB)\n",
      "Trainable params: 176737 (690.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/198\n",
      "20/20 [==============================] - 2s 51ms/step - loss: 2248.1816 - val_loss: 4264.9038\n",
      "Epoch 2/198\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1723.6178 - val_loss: 2767.5220\n",
      "Epoch 3/198\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1396.0077 - val_loss: 2435.5303\n",
      "Epoch 4/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1133.8724 - val_loss: 1905.9495\n",
      "Epoch 5/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 876.5903 - val_loss: 1213.1616\n",
      "Epoch 6/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 683.7058 - val_loss: 975.2333\n",
      "Epoch 7/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 606.0515 - val_loss: 697.8306\n",
      "Epoch 8/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 576.6824 - val_loss: 595.2508\n",
      "Epoch 9/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 555.1714 - val_loss: 575.7409\n",
      "Epoch 10/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 540.5882 - val_loss: 444.0918\n",
      "Epoch 11/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 521.8344 - val_loss: 627.4216\n",
      "Epoch 12/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 512.5981 - val_loss: 444.4627\n",
      "Epoch 13/198\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 499.5486 - val_loss: 451.6248\n",
      "Epoch 14/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 474.8076 - val_loss: 534.8578\n",
      "Epoch 15/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 462.4048 - val_loss: 476.3978\n",
      "Epoch 16/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 451.4616 - val_loss: 395.1880\n",
      "Epoch 17/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 437.6684 - val_loss: 536.3025\n",
      "Epoch 18/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 423.8850 - val_loss: 454.8920\n",
      "Epoch 19/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 403.0128 - val_loss: 478.7082\n",
      "Epoch 20/198\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 387.8620 - val_loss: 474.7192\n",
      "Epoch 21/198\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 386.3845 - val_loss: 412.6495\n",
      "Epoch 22/198\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 380.0190 - val_loss: 464.5759\n",
      "Epoch 23/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 362.2443 - val_loss: 362.9963\n",
      "Epoch 24/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 353.0075 - val_loss: 445.4234\n",
      "Epoch 25/198\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 345.3524 - val_loss: 455.4103\n",
      "Epoch 26/198\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 336.4944 - val_loss: 429.9630\n",
      "Epoch 27/198\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 323.8717 - val_loss: 381.4970\n",
      "Epoch 28/198\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 314.9577 - val_loss: 396.3202\n",
      "Epoch 29/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 320.5157 - val_loss: 370.1570\n",
      "Epoch 30/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 300.2954 - val_loss: 368.5473\n",
      "Epoch 31/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 300.1407 - val_loss: 438.7717\n",
      "Epoch 32/198\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 291.4713 - val_loss: 363.6662\n",
      "Epoch 33/198\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 277.9441 - val_loss: 384.4324\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1233.6310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:00:00,334] Trial 15 finished with value: 1233.6309814453125 and parameters: {'head_size': 215, 'num_heads': 7, 'ff_dim': 12, 'num_transformer_blocks': 5, 'mlp_units': 108, 'mlp_dropout': 0.13271289536229092, 'dropout': 0.29460374269419676, 'learning_rate': 0.0003230045806324201, 'n_epochs': 198}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_166 (L  (None, 8, 5)                 10        ['input_17[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_83 (M  (None, 8, 5)                 46925     ['layer_normalization_166[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_166[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_182 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_83[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_166 (  (None, 8, 5)                 0         ['dropout_182[0][0]',         \n",
      " TFOpLambda)                                                         'input_17[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_167 (L  (None, 8, 5)                 10        ['tf.__operators__.add_166[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_166 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_167[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_183 (Dropout)       (None, 8, 10)                0         ['conv1d_166[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_167 (Conv1D)         (None, 8, 5)                 55        ['dropout_183[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_167 (  (None, 8, 5)                 0         ['conv1d_167[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_166[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_168 (L  (None, 8, 5)                 10        ['tf.__operators__.add_167[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_84 (M  (None, 8, 5)                 46925     ['layer_normalization_168[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_168[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_184 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_84[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_168 (  (None, 8, 5)                 0         ['dropout_184[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_167[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_169 (L  (None, 8, 5)                 10        ['tf.__operators__.add_168[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_168 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_169[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_185 (Dropout)       (None, 8, 10)                0         ['conv1d_168[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_169 (Conv1D)         (None, 8, 5)                 55        ['dropout_185[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_169 (  (None, 8, 5)                 0         ['conv1d_169[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_168[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_170 (L  (None, 8, 5)                 10        ['tf.__operators__.add_169[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_85 (M  (None, 8, 5)                 46925     ['layer_normalization_170[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_170[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_186 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_85[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_170 (  (None, 8, 5)                 0         ['dropout_186[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_169[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_171 (L  (None, 8, 5)                 10        ['tf.__operators__.add_170[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_170 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_171[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_187 (Dropout)       (None, 8, 10)                0         ['conv1d_170[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_171 (Conv1D)         (None, 8, 5)                 55        ['dropout_187[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_171 (  (None, 8, 5)                 0         ['conv1d_171[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_170[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_172 (L  (None, 8, 5)                 10        ['tf.__operators__.add_171[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_86 (M  (None, 8, 5)                 46925     ['layer_normalization_172[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_172[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_188 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_86[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_172 (  (None, 8, 5)                 0         ['dropout_188[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_171[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_173 (L  (None, 8, 5)                 10        ['tf.__operators__.add_172[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_172 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_173[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_189 (Dropout)       (None, 8, 10)                0         ['conv1d_172[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_173 (Conv1D)         (None, 8, 5)                 55        ['dropout_189[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_173 (  (None, 8, 5)                 0         ['conv1d_173[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_172[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_174 (L  (None, 8, 5)                 10        ['tf.__operators__.add_173[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_87 (M  (None, 8, 5)                 46925     ['layer_normalization_174[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_174[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_190 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_87[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_174 (  (None, 8, 5)                 0         ['dropout_190[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_173[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_175 (L  (None, 8, 5)                 10        ['tf.__operators__.add_174[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_174 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_175[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_191 (Dropout)       (None, 8, 10)                0         ['conv1d_174[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_175 (Conv1D)         (None, 8, 5)                 55        ['dropout_191[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_175 (  (None, 8, 5)                 0         ['conv1d_175[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_174[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_176 (L  (None, 8, 5)                 10        ['tf.__operators__.add_175[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_88 (M  (None, 8, 5)                 46925     ['layer_normalization_176[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_176[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_192 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_88[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_176 (  (None, 8, 5)                 0         ['dropout_192[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_175[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_177 (L  (None, 8, 5)                 10        ['tf.__operators__.add_176[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_176 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_177[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_193 (Dropout)       (None, 8, 10)                0         ['conv1d_176[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_177 (Conv1D)         (None, 8, 5)                 55        ['dropout_193[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_177 (  (None, 8, 5)                 0         ['conv1d_177[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_176[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_178 (L  (None, 8, 5)                 10        ['tf.__operators__.add_177[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_89 (M  (None, 8, 5)                 46925     ['layer_normalization_178[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_178[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_194 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_89[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_178 (  (None, 8, 5)                 0         ['dropout_194[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_177[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_179 (L  (None, 8, 5)                 10        ['tf.__operators__.add_178[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_178 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_179[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_195 (Dropout)       (None, 8, 10)                0         ['conv1d_178[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_179 (Conv1D)         (None, 8, 5)                 55        ['dropout_195[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_179 (  (None, 8, 5)                 0         ['conv1d_179[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_178[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_179[0][\n",
      " 6 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 114)                  1026      ['global_average_pooling1d_16[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_196 (Dropout)       (None, 114)                  0         ['dense_48[0][0]']            \n",
      "                                                                                                  \n",
      " dense_49 (Dense)            (None, 16)                   1840      ['dropout_196[0][0]']         \n",
      "                                                                                                  \n",
      " dense_50 (Dense)            (None, 8)                    136       ['dense_49[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 332422 (1.27 MB)\n",
      "Trainable params: 332422 (1.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/149\n",
      "20/20 [==============================] - 3s 76ms/step - loss: 2777.1370 - val_loss: 6267.4585\n",
      "Epoch 2/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 2075.7170 - val_loss: 4474.9302\n",
      "Epoch 3/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 1467.3906 - val_loss: 2920.7698\n",
      "Epoch 4/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 1076.4120 - val_loss: 2171.3779\n",
      "Epoch 5/149\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 862.8237 - val_loss: 1631.0602\n",
      "Epoch 6/149\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 767.8598 - val_loss: 1135.1779\n",
      "Epoch 7/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 682.6759 - val_loss: 864.3902\n",
      "Epoch 8/149\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 628.1962 - val_loss: 552.2126\n",
      "Epoch 9/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 592.3110 - val_loss: 674.5673\n",
      "Epoch 10/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 573.5473 - val_loss: 507.8849\n",
      "Epoch 11/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 548.8773 - val_loss: 529.7961\n",
      "Epoch 12/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 528.2154 - val_loss: 608.5157\n",
      "Epoch 13/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 503.5480 - val_loss: 622.5016\n",
      "Epoch 14/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 494.0187 - val_loss: 526.0282\n",
      "Epoch 15/149\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 474.8869 - val_loss: 557.3688\n",
      "Epoch 16/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 459.4155 - val_loss: 465.4703\n",
      "Epoch 17/149\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 439.9059 - val_loss: 599.7837\n",
      "Epoch 18/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 434.5326 - val_loss: 468.9390\n",
      "Epoch 19/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 419.4256 - val_loss: 588.1318\n",
      "Epoch 20/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 408.5168 - val_loss: 485.1094\n",
      "Epoch 21/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 390.7908 - val_loss: 480.6749\n",
      "Epoch 22/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 382.1922 - val_loss: 443.0688\n",
      "Epoch 23/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 367.0284 - val_loss: 479.5967\n",
      "Epoch 24/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 357.7388 - val_loss: 435.5218\n",
      "Epoch 25/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 346.8553 - val_loss: 465.9737\n",
      "Epoch 26/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 336.7278 - val_loss: 510.8765\n",
      "Epoch 27/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 324.6748 - val_loss: 432.7922\n",
      "Epoch 28/149\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 322.6850 - val_loss: 431.5247\n",
      "Epoch 29/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 313.5581 - val_loss: 456.9605\n",
      "Epoch 30/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 301.8033 - val_loss: 447.7459\n",
      "Epoch 31/149\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 290.3032 - val_loss: 460.0379\n",
      "Epoch 32/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 288.7903 - val_loss: 438.2231\n",
      "Epoch 33/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 281.7596 - val_loss: 401.0183\n",
      "Epoch 34/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 276.2715 - val_loss: 440.2552\n",
      "Epoch 35/149\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 268.6883 - val_loss: 444.9449\n",
      "Epoch 36/149\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 268.7776 - val_loss: 404.5016\n",
      "Epoch 37/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 254.1886 - val_loss: 430.0780\n",
      "Epoch 38/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 251.5749 - val_loss: 376.8185\n",
      "Epoch 39/149\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 242.1697 - val_loss: 456.9150\n",
      "Epoch 40/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 245.8309 - val_loss: 429.3330\n",
      "Epoch 41/149\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 243.4242 - val_loss: 370.9326\n",
      "Epoch 42/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 237.5412 - val_loss: 424.9631\n",
      "Epoch 43/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 232.6214 - val_loss: 395.1175\n",
      "Epoch 44/149\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 230.1815 - val_loss: 362.2863\n",
      "Epoch 45/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 223.3464 - val_loss: 379.3015\n",
      "Epoch 46/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 221.1864 - val_loss: 371.8145\n",
      "Epoch 47/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 212.8233 - val_loss: 381.4486\n",
      "Epoch 48/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 212.2257 - val_loss: 363.6154\n",
      "Epoch 49/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 206.9615 - val_loss: 395.9043\n",
      "Epoch 50/149\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 211.3759 - val_loss: 441.3463\n",
      "Epoch 51/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 206.1005 - val_loss: 362.0574\n",
      "Epoch 52/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 203.4118 - val_loss: 385.3243\n",
      "Epoch 53/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 198.3276 - val_loss: 358.0896\n",
      "Epoch 54/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 199.5392 - val_loss: 369.2182\n",
      "Epoch 55/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 194.5214 - val_loss: 372.1656\n",
      "Epoch 56/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 191.8659 - val_loss: 341.1296\n",
      "Epoch 57/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 193.9589 - val_loss: 334.4412\n",
      "Epoch 58/149\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 189.1479 - val_loss: 342.2457\n",
      "Epoch 59/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 190.5559 - val_loss: 368.2045\n",
      "Epoch 60/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 190.5877 - val_loss: 380.2848\n",
      "Epoch 61/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 183.2506 - val_loss: 344.2578\n",
      "Epoch 62/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 179.4354 - val_loss: 352.6985\n",
      "Epoch 63/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 180.3147 - val_loss: 392.2359\n",
      "Epoch 64/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 174.5871 - val_loss: 371.5498\n",
      "Epoch 65/149\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 181.8744 - val_loss: 323.8529\n",
      "Epoch 66/149\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 177.0732 - val_loss: 376.8520\n",
      "Epoch 67/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 173.2348 - val_loss: 342.7342\n",
      "Epoch 68/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 171.3779 - val_loss: 331.0037\n",
      "Epoch 69/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 163.1124 - val_loss: 375.3018\n",
      "Epoch 70/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 169.8156 - val_loss: 340.4361\n",
      "Epoch 71/149\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 163.7576 - val_loss: 296.7963\n",
      "Epoch 72/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 167.9153 - val_loss: 301.7880\n",
      "Epoch 73/149\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 161.7578 - val_loss: 317.9692\n",
      "Epoch 74/149\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 164.8383 - val_loss: 336.4327\n",
      "Epoch 75/149\n",
      "20/20 [==============================] - 1s 66ms/step - loss: 165.3362 - val_loss: 343.5510\n",
      "Epoch 76/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 166.4744 - val_loss: 310.8110\n",
      "Epoch 77/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 160.5908 - val_loss: 334.9209\n",
      "Epoch 78/149\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 160.8266 - val_loss: 343.9863\n",
      "Epoch 79/149\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 157.7929 - val_loss: 336.4127\n",
      "Epoch 80/149\n",
      "20/20 [==============================] - 1s 65ms/step - loss: 160.9962 - val_loss: 338.2661\n",
      "Epoch 81/149\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 157.0547 - val_loss: 321.4265\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 938.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:01:49,886] Trial 16 finished with value: 938.3599243164062 and parameters: {'head_size': 255, 'num_heads': 8, 'ff_dim': 10, 'num_transformer_blocks': 7, 'mlp_units': 114, 'mlp_dropout': 0.17324804970605878, 'dropout': 0.336179712343327, 'learning_rate': 0.00032888987846491254, 'n_epochs': 149}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_180 (L  (None, 8, 5)                 10        ['input_18[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_90 (M  (None, 8, 5)                 29146     ['layer_normalization_180[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_180[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_197 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_90[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_180 (  (None, 8, 5)                 0         ['dropout_197[0][0]',         \n",
      " TFOpLambda)                                                         'input_18[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_181 (L  (None, 8, 5)                 10        ['tf.__operators__.add_180[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_180 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_181[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_198 (Dropout)       (None, 8, 14)                0         ['conv1d_180[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_181 (Conv1D)         (None, 8, 5)                 75        ['dropout_198[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_181 (  (None, 8, 5)                 0         ['conv1d_181[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_180[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_182 (L  (None, 8, 5)                 10        ['tf.__operators__.add_181[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_91 (M  (None, 8, 5)                 29146     ['layer_normalization_182[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_182[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_199 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_91[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_182 (  (None, 8, 5)                 0         ['dropout_199[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_181[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_183 (L  (None, 8, 5)                 10        ['tf.__operators__.add_182[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_182 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_183[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_200 (Dropout)       (None, 8, 14)                0         ['conv1d_182[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_183 (Conv1D)         (None, 8, 5)                 75        ['dropout_200[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_183 (  (None, 8, 5)                 0         ['conv1d_183[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_182[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_184 (L  (None, 8, 5)                 10        ['tf.__operators__.add_183[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_92 (M  (None, 8, 5)                 29146     ['layer_normalization_184[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_184[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_201 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_92[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_184 (  (None, 8, 5)                 0         ['dropout_201[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_183[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_185 (L  (None, 8, 5)                 10        ['tf.__operators__.add_184[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_184 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_185[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_202 (Dropout)       (None, 8, 14)                0         ['conv1d_184[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_185 (Conv1D)         (None, 8, 5)                 75        ['dropout_202[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_185 (  (None, 8, 5)                 0         ['conv1d_185[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_184[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_186 (L  (None, 8, 5)                 10        ['tf.__operators__.add_185[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_93 (M  (None, 8, 5)                 29146     ['layer_normalization_186[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_186[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_203 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_93[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_186 (  (None, 8, 5)                 0         ['dropout_203[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_185[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_187 (L  (None, 8, 5)                 10        ['tf.__operators__.add_186[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_186 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_187[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_204 (Dropout)       (None, 8, 14)                0         ['conv1d_186[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_187 (Conv1D)         (None, 8, 5)                 75        ['dropout_204[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_187 (  (None, 8, 5)                 0         ['conv1d_187[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_186[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_187[0][\n",
      " 7 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_51 (Dense)            (None, 105)                  945       ['global_average_pooling1d_17[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_205 (Dropout)       (None, 105)                  0         ['dense_51[0][0]']            \n",
      "                                                                                                  \n",
      " dense_52 (Dense)            (None, 16)                   1696      ['dropout_205[0][0]']         \n",
      "                                                                                                  \n",
      " dense_53 (Dense)            (None, 8)                    136       ['dense_52[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120077 (469.05 KB)\n",
      "Trainable params: 120077 (469.05 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/167\n",
      "20/20 [==============================] - 2s 41ms/step - loss: 2050.4050 - val_loss: 2497.5891\n",
      "Epoch 2/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 838.2572 - val_loss: 1194.2329\n",
      "Epoch 3/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 616.5049 - val_loss: 631.2571\n",
      "Epoch 4/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 529.9708 - val_loss: 415.3232\n",
      "Epoch 5/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 486.9846 - val_loss: 465.5210\n",
      "Epoch 6/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 452.9980 - val_loss: 345.2375\n",
      "Epoch 7/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 427.0656 - val_loss: 390.1517\n",
      "Epoch 8/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 391.1663 - val_loss: 468.8582\n",
      "Epoch 9/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 371.7805 - val_loss: 494.0203\n",
      "Epoch 10/167\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 347.0213 - val_loss: 392.6073\n",
      "Epoch 11/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 322.6758 - val_loss: 390.4419\n",
      "Epoch 12/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 310.3932 - val_loss: 422.3145\n",
      "Epoch 13/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 297.6567 - val_loss: 513.4157\n",
      "Epoch 14/167\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 294.2102 - val_loss: 421.0428\n",
      "Epoch 15/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 274.7385 - val_loss: 440.9711\n",
      "Epoch 16/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 262.6104 - val_loss: 324.8909\n",
      "Epoch 17/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 248.4387 - val_loss: 307.9235\n",
      "Epoch 18/167\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 241.0600 - val_loss: 334.0053\n",
      "Epoch 19/167\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 227.1974 - val_loss: 463.7427\n",
      "Epoch 20/167\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 226.6324 - val_loss: 287.7361\n",
      "Epoch 21/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 218.7423 - val_loss: 283.5710\n",
      "Epoch 22/167\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 214.6942 - val_loss: 337.6964\n",
      "Epoch 23/167\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 203.4746 - val_loss: 325.3448\n",
      "Epoch 24/167\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 199.1997 - val_loss: 307.5588\n",
      "Epoch 25/167\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 191.0232 - val_loss: 350.3567\n",
      "Epoch 26/167\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 190.8787 - val_loss: 403.4244\n",
      "Epoch 27/167\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 191.9389 - val_loss: 324.9570\n",
      "Epoch 28/167\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 179.5998 - val_loss: 328.6148\n",
      "Epoch 29/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 178.5536 - val_loss: 366.5988\n",
      "Epoch 30/167\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 174.4684 - val_loss: 321.8877\n",
      "Epoch 31/167\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 170.6083 - val_loss: 363.0463\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 870.4816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:02:12,357] Trial 17 finished with value: 870.4815673828125 and parameters: {'head_size': 181, 'num_heads': 7, 'ff_dim': 14, 'num_transformer_blocks': 4, 'mlp_units': 105, 'mlp_dropout': 0.15183822884174442, 'dropout': 0.10115130646855976, 'learning_rate': 0.0008910410552458527, 'n_epochs': 167}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_188 (L  (None, 8, 5)                 10        ['input_19[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_94 (M  (None, 8, 5)                 27858     ['layer_normalization_188[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_188[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_206 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_94[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_188 (  (None, 8, 5)                 0         ['dropout_206[0][0]',         \n",
      " TFOpLambda)                                                         'input_19[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_189 (L  (None, 8, 5)                 10        ['tf.__operators__.add_188[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_188 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_189[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_207 (Dropout)       (None, 8, 14)                0         ['conv1d_188[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_189 (Conv1D)         (None, 8, 5)                 75        ['dropout_207[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_189 (  (None, 8, 5)                 0         ['conv1d_189[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_188[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_190 (L  (None, 8, 5)                 10        ['tf.__operators__.add_189[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_95 (M  (None, 8, 5)                 27858     ['layer_normalization_190[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_190[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_208 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_95[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_190 (  (None, 8, 5)                 0         ['dropout_208[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_189[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_191 (L  (None, 8, 5)                 10        ['tf.__operators__.add_190[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_190 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_191[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_209 (Dropout)       (None, 8, 14)                0         ['conv1d_190[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_191 (Conv1D)         (None, 8, 5)                 75        ['dropout_209[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_191 (  (None, 8, 5)                 0         ['conv1d_191[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_190[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_192 (L  (None, 8, 5)                 10        ['tf.__operators__.add_191[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_96 (M  (None, 8, 5)                 27858     ['layer_normalization_192[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_192[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_210 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_96[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_192 (  (None, 8, 5)                 0         ['dropout_210[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_191[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_193 (L  (None, 8, 5)                 10        ['tf.__operators__.add_192[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_192 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_193[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_211 (Dropout)       (None, 8, 14)                0         ['conv1d_192[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_193 (Conv1D)         (None, 8, 5)                 75        ['dropout_211[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_193 (  (None, 8, 5)                 0         ['conv1d_193[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_192[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_194 (L  (None, 8, 5)                 10        ['tf.__operators__.add_193[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_97 (M  (None, 8, 5)                 27858     ['layer_normalization_194[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_194[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_212 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_97[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_194 (  (None, 8, 5)                 0         ['dropout_212[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_193[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_195 (L  (None, 8, 5)                 10        ['tf.__operators__.add_194[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_194 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_195[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_213 (Dropout)       (None, 8, 14)                0         ['conv1d_194[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_195 (Conv1D)         (None, 8, 5)                 75        ['dropout_213[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_195 (  (None, 8, 5)                 0         ['conv1d_195[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_194[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_196 (L  (None, 8, 5)                 10        ['tf.__operators__.add_195[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_98 (M  (None, 8, 5)                 27858     ['layer_normalization_196[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_196[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_214 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_98[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_196 (  (None, 8, 5)                 0         ['dropout_214[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_195[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_197 (L  (None, 8, 5)                 10        ['tf.__operators__.add_196[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_196 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_197[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_215 (Dropout)       (None, 8, 14)                0         ['conv1d_196[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_197 (Conv1D)         (None, 8, 5)                 75        ['dropout_215[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_197 (  (None, 8, 5)                 0         ['conv1d_197[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_196[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_197[0][\n",
      " 8 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_54 (Dense)            (None, 90)                   810       ['global_average_pooling1d_18[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_216 (Dropout)       (None, 90)                   0         ['dense_54[0][0]']            \n",
      "                                                                                                  \n",
      " dense_55 (Dense)            (None, 16)                   1456      ['dropout_216[0][0]']         \n",
      "                                                                                                  \n",
      " dense_56 (Dense)            (None, 8)                    136       ['dense_55[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142587 (556.98 KB)\n",
      "Trainable params: 142587 (556.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/166\n",
      "20/20 [==============================] - 2s 44ms/step - loss: 2939.3320 - val_loss: 8298.9180\n",
      "Epoch 2/166\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 2513.4417 - val_loss: 6794.9590\n",
      "Epoch 3/166\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 2172.9177 - val_loss: 5683.0708\n",
      "Epoch 4/166\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1838.8647 - val_loss: 4676.9961\n",
      "Epoch 5/166\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1590.5322 - val_loss: 4206.1948\n",
      "Epoch 6/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1359.6196 - val_loss: 3790.3975\n",
      "Epoch 7/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1208.2063 - val_loss: 3354.3643\n",
      "Epoch 8/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1113.2504 - val_loss: 2975.1067\n",
      "Epoch 9/166\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1037.3781 - val_loss: 2600.4370\n",
      "Epoch 10/166\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 970.4448 - val_loss: 2181.6541\n",
      "Epoch 11/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 922.3444 - val_loss: 1980.0586\n",
      "Epoch 12/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 879.2197 - val_loss: 1818.5308\n",
      "Epoch 13/166\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 833.1870 - val_loss: 1569.4601\n",
      "Epoch 14/166\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 793.2733 - val_loss: 1455.3070\n",
      "Epoch 15/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 758.3902 - val_loss: 1218.8516\n",
      "Epoch 16/166\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 732.2936 - val_loss: 1145.0300\n",
      "Epoch 17/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 706.4061 - val_loss: 1002.1945\n",
      "Epoch 18/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 671.6583 - val_loss: 878.9888\n",
      "Epoch 19/166\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 667.3419 - val_loss: 776.3362\n",
      "Epoch 20/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 637.6791 - val_loss: 787.5126\n",
      "Epoch 21/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 627.0686 - val_loss: 679.7344\n",
      "Epoch 22/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 604.8521 - val_loss: 661.3763\n",
      "Epoch 23/166\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 604.1450 - val_loss: 525.0397\n",
      "Epoch 24/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 576.5469 - val_loss: 559.5864\n",
      "Epoch 25/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 570.1534 - val_loss: 536.8581\n",
      "Epoch 26/166\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 554.5403 - val_loss: 516.4858\n",
      "Epoch 27/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 548.7581 - val_loss: 544.1731\n",
      "Epoch 28/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 542.5679 - val_loss: 569.7640\n",
      "Epoch 29/166\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 529.4537 - val_loss: 544.2219\n",
      "Epoch 30/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 513.8114 - val_loss: 451.9537\n",
      "Epoch 31/166\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 495.4503 - val_loss: 495.4745\n",
      "Epoch 32/166\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 485.0898 - val_loss: 521.5259\n",
      "Epoch 33/166\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 491.0089 - val_loss: 556.2900\n",
      "Epoch 34/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 474.7359 - val_loss: 543.7325\n",
      "Epoch 35/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 465.2039 - val_loss: 495.1074\n",
      "Epoch 36/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 458.6202 - val_loss: 511.3674\n",
      "Epoch 37/166\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 446.1433 - val_loss: 524.2605\n",
      "Epoch 38/166\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 438.1718 - val_loss: 491.3901\n",
      "Epoch 39/166\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 423.6062 - val_loss: 556.9872\n",
      "Epoch 40/166\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 435.3197 - val_loss: 492.2576\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1415.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:02:45,849] Trial 18 finished with value: 1415.5924072265625 and parameters: {'head_size': 173, 'num_heads': 7, 'ff_dim': 14, 'num_transformer_blocks': 5, 'mlp_units': 90, 'mlp_dropout': 0.22793713295754092, 'dropout': 0.12099063341112579, 'learning_rate': 0.00017857383378345145, 'n_epochs': 166}. Best is trial 12 with value: 845.5035400390625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_198 (L  (None, 8, 5)                 10        ['input_20[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_99 (M  (None, 8, 5)                 14495     ['layer_normalization_198[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'layer_normalization_198[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_217 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_99[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_198 (  (None, 8, 5)                 0         ['dropout_217[0][0]',         \n",
      " TFOpLambda)                                                         'input_20[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_199 (L  (None, 8, 5)                 10        ['tf.__operators__.add_198[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_198 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_199[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_218 (Dropout)       (None, 8, 18)                0         ['conv1d_198[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_199 (Conv1D)         (None, 8, 5)                 95        ['dropout_218[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_199 (  (None, 8, 5)                 0         ['conv1d_199[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_198[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_200 (L  (None, 8, 5)                 10        ['tf.__operators__.add_199[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_100 (  (None, 8, 5)                 14495     ['layer_normalization_200[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_200[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_219 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_100[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_200 (  (None, 8, 5)                 0         ['dropout_219[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_199[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_201 (L  (None, 8, 5)                 10        ['tf.__operators__.add_200[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_200 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_201[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_220 (Dropout)       (None, 8, 18)                0         ['conv1d_200[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_201 (Conv1D)         (None, 8, 5)                 95        ['dropout_220[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_201 (  (None, 8, 5)                 0         ['conv1d_201[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_200[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_202 (L  (None, 8, 5)                 10        ['tf.__operators__.add_201[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_101 (  (None, 8, 5)                 14495     ['layer_normalization_202[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_202[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_221 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_101[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_202 (  (None, 8, 5)                 0         ['dropout_221[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_201[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_203 (L  (None, 8, 5)                 10        ['tf.__operators__.add_202[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_202 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_203[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_222 (Dropout)       (None, 8, 18)                0         ['conv1d_202[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_203 (Conv1D)         (None, 8, 5)                 95        ['dropout_222[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_203 (  (None, 8, 5)                 0         ['conv1d_203[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_202[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_204 (L  (None, 8, 5)                 10        ['tf.__operators__.add_203[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_102 (  (None, 8, 5)                 14495     ['layer_normalization_204[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_204[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_223 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_102[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_204 (  (None, 8, 5)                 0         ['dropout_223[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_203[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_205 (L  (None, 8, 5)                 10        ['tf.__operators__.add_204[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_204 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_205[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_224 (Dropout)       (None, 8, 18)                0         ['conv1d_204[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_205 (Conv1D)         (None, 8, 5)                 95        ['dropout_224[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_205 (  (None, 8, 5)                 0         ['conv1d_205[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_204[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_206 (L  (None, 8, 5)                 10        ['tf.__operators__.add_205[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_103 (  (None, 8, 5)                 14495     ['layer_normalization_206[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_206[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_225 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_103[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_206 (  (None, 8, 5)                 0         ['dropout_225[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_205[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_207 (L  (None, 8, 5)                 10        ['tf.__operators__.add_206[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_206 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_207[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_226 (Dropout)       (None, 8, 18)                0         ['conv1d_206[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_207 (Conv1D)         (None, 8, 5)                 95        ['dropout_226[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_207 (  (None, 8, 5)                 0         ['conv1d_207[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_206[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_208 (L  (None, 8, 5)                 10        ['tf.__operators__.add_207[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_104 (  (None, 8, 5)                 14495     ['layer_normalization_208[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_208[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_227 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_104[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_208 (  (None, 8, 5)                 0         ['dropout_227[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_207[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_209 (L  (None, 8, 5)                 10        ['tf.__operators__.add_208[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_208 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_209[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_228 (Dropout)       (None, 8, 18)                0         ['conv1d_208[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_209 (Conv1D)         (None, 8, 5)                 95        ['dropout_228[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_209 (  (None, 8, 5)                 0         ['conv1d_209[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_208[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_210 (L  (None, 8, 5)                 10        ['tf.__operators__.add_209[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_105 (  (None, 8, 5)                 14495     ['layer_normalization_210[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_210[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_229 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_105[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_210 (  (None, 8, 5)                 0         ['dropout_229[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_209[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_211 (L  (None, 8, 5)                 10        ['tf.__operators__.add_210[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_210 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_211[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_230 (Dropout)       (None, 8, 18)                0         ['conv1d_210[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_211 (Conv1D)         (None, 8, 5)                 95        ['dropout_230[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_211 (  (None, 8, 5)                 0         ['conv1d_211[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_210[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 8)                    0         ['tf.__operators__.add_211[0][\n",
      " 9 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_57 (Dense)            (None, 103)                  927       ['global_average_pooling1d_19[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_231 (Dropout)       (None, 103)                  0         ['dense_57[0][0]']            \n",
      "                                                                                                  \n",
      " dense_58 (Dense)            (None, 16)                   1664      ['dropout_231[0][0]']         \n",
      "                                                                                                  \n",
      " dense_59 (Dense)            (None, 8)                    136       ['dense_58[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 105753 (413.10 KB)\n",
      "Trainable params: 105753 (413.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/137\n",
      "20/20 [==============================] - 3s 43ms/step - loss: 1957.4792 - val_loss: 3212.2671\n",
      "Epoch 2/137\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1077.2583 - val_loss: 2040.2998\n",
      "Epoch 3/137\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 864.6849 - val_loss: 1696.5452\n",
      "Epoch 4/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 754.6270 - val_loss: 1349.9172\n",
      "Epoch 5/137\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 646.5641 - val_loss: 617.6774\n",
      "Epoch 6/137\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 575.7990 - val_loss: 518.4404\n",
      "Epoch 7/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 537.3374 - val_loss: 368.7709\n",
      "Epoch 8/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 507.5143 - val_loss: 559.8944\n",
      "Epoch 9/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 470.1371 - val_loss: 442.8126\n",
      "Epoch 10/137\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 448.1451 - val_loss: 486.6545\n",
      "Epoch 11/137\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 434.1470 - val_loss: 491.8146\n",
      "Epoch 12/137\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 398.2716 - val_loss: 416.7527\n",
      "Epoch 13/137\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 384.2740 - val_loss: 457.6432\n",
      "Epoch 14/137\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 364.4142 - val_loss: 507.5471\n",
      "Epoch 15/137\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 347.1225 - val_loss: 406.3297\n",
      "Epoch 16/137\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 350.4414 - val_loss: 276.6262\n",
      "Epoch 17/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 321.6192 - val_loss: 384.1977\n",
      "Epoch 18/137\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 313.6535 - val_loss: 396.4547\n",
      "Epoch 19/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 294.7603 - val_loss: 349.3992\n",
      "Epoch 20/137\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 296.5412 - val_loss: 422.7459\n",
      "Epoch 21/137\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 281.2594 - val_loss: 391.4020\n",
      "Epoch 22/137\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 272.6825 - val_loss: 453.9844\n",
      "Epoch 23/137\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 259.0666 - val_loss: 350.6618\n",
      "Epoch 24/137\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 253.2691 - val_loss: 381.7623\n",
      "Epoch 25/137\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 247.2533 - val_loss: 389.7597\n",
      "Epoch 26/137\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 246.6875 - val_loss: 485.8224\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 826.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:03:08,204] Trial 19 finished with value: 826.89697265625 and parameters: {'head_size': 105, 'num_heads': 6, 'ff_dim': 18, 'num_transformer_blocks': 7, 'mlp_units': 103, 'mlp_dropout': 0.16404481058556958, 'dropout': 0.10857267405799768, 'learning_rate': 0.0005181493936545735, 'n_epochs': 137}. Best is trial 19 with value: 826.89697265625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_212 (L  (None, 8, 5)                 10        ['input_21[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_106 (  (None, 8, 5)                 11965     ['layer_normalization_212[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_212[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_232 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_106[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_212 (  (None, 8, 5)                 0         ['dropout_232[0][0]',         \n",
      " TFOpLambda)                                                         'input_21[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_213 (L  (None, 8, 5)                 10        ['tf.__operators__.add_212[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_212 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_213[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_233 (Dropout)       (None, 8, 18)                0         ['conv1d_212[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_213 (Conv1D)         (None, 8, 5)                 95        ['dropout_233[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_213 (  (None, 8, 5)                 0         ['conv1d_213[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_212[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_214 (L  (None, 8, 5)                 10        ['tf.__operators__.add_213[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_107 (  (None, 8, 5)                 11965     ['layer_normalization_214[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_214[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_234 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_107[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_214 (  (None, 8, 5)                 0         ['dropout_234[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_213[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_215 (L  (None, 8, 5)                 10        ['tf.__operators__.add_214[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_214 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_215[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_235 (Dropout)       (None, 8, 18)                0         ['conv1d_214[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_215 (Conv1D)         (None, 8, 5)                 95        ['dropout_235[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_215 (  (None, 8, 5)                 0         ['conv1d_215[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_214[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_216 (L  (None, 8, 5)                 10        ['tf.__operators__.add_215[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_108 (  (None, 8, 5)                 11965     ['layer_normalization_216[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_216[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_236 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_108[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_216 (  (None, 8, 5)                 0         ['dropout_236[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_215[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_217 (L  (None, 8, 5)                 10        ['tf.__operators__.add_216[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_216 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_217[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_237 (Dropout)       (None, 8, 18)                0         ['conv1d_216[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_217 (Conv1D)         (None, 8, 5)                 95        ['dropout_237[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_217 (  (None, 8, 5)                 0         ['conv1d_217[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_216[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_218 (L  (None, 8, 5)                 10        ['tf.__operators__.add_217[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_109 (  (None, 8, 5)                 11965     ['layer_normalization_218[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_218[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_238 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_109[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_218 (  (None, 8, 5)                 0         ['dropout_238[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_217[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_219 (L  (None, 8, 5)                 10        ['tf.__operators__.add_218[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_218 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_219[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_239 (Dropout)       (None, 8, 18)                0         ['conv1d_218[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_219 (Conv1D)         (None, 8, 5)                 95        ['dropout_239[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_219 (  (None, 8, 5)                 0         ['conv1d_219[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_218[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_220 (L  (None, 8, 5)                 10        ['tf.__operators__.add_219[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_110 (  (None, 8, 5)                 11965     ['layer_normalization_220[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_220[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_240 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_110[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_220 (  (None, 8, 5)                 0         ['dropout_240[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_219[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_221 (L  (None, 8, 5)                 10        ['tf.__operators__.add_220[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_220 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_221[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_241 (Dropout)       (None, 8, 18)                0         ['conv1d_220[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_221 (Conv1D)         (None, 8, 5)                 95        ['dropout_241[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_221 (  (None, 8, 5)                 0         ['conv1d_221[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_220[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_222 (L  (None, 8, 5)                 10        ['tf.__operators__.add_221[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_111 (  (None, 8, 5)                 11965     ['layer_normalization_222[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_222[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_242 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_111[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_222 (  (None, 8, 5)                 0         ['dropout_242[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_221[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_223 (L  (None, 8, 5)                 10        ['tf.__operators__.add_222[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_222 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_223[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_243 (Dropout)       (None, 8, 18)                0         ['conv1d_222[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_223 (Conv1D)         (None, 8, 5)                 95        ['dropout_243[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_223 (  (None, 8, 5)                 0         ['conv1d_223[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_222[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_224 (L  (None, 8, 5)                 10        ['tf.__operators__.add_223[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_112 (  (None, 8, 5)                 11965     ['layer_normalization_224[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_224[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_244 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_112[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_224 (  (None, 8, 5)                 0         ['dropout_244[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_223[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_225 (L  (None, 8, 5)                 10        ['tf.__operators__.add_224[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_224 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_225[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_245 (Dropout)       (None, 8, 18)                0         ['conv1d_224[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_225 (Conv1D)         (None, 8, 5)                 95        ['dropout_245[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_225 (  (None, 8, 5)                 0         ['conv1d_225[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_224[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_225[0][\n",
      " 0 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_60 (Dense)            (None, 94)                   846       ['global_average_pooling1d_20[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_246 (Dropout)       (None, 94)                   0         ['dense_60[0][0]']            \n",
      "                                                                                                  \n",
      " dense_61 (Dense)            (None, 16)                   1520      ['dropout_246[0][0]']         \n",
      "                                                                                                  \n",
      " dense_62 (Dense)            (None, 8)                    136       ['dense_61[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 87818 (343.04 KB)\n",
      "Trainable params: 87818 (343.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/135\n",
      "20/20 [==============================] - 2s 39ms/step - loss: 3567.5562 - val_loss: 10660.2793\n",
      "Epoch 2/135\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 3078.1099 - val_loss: 9326.8428\n",
      "Epoch 3/135\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2716.9854 - val_loss: 8067.6021\n",
      "Epoch 4/135\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2364.2102 - val_loss: 6917.5918\n",
      "Epoch 5/135\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 2055.4932 - val_loss: 5830.1113\n",
      "Epoch 6/135\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1801.8160 - val_loss: 4774.4458\n",
      "Epoch 7/135\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 1586.7495 - val_loss: 3829.5491\n",
      "Epoch 8/135\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1426.6993 - val_loss: 3037.1658\n",
      "Epoch 9/135\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1313.4634 - val_loss: 2494.9707\n",
      "Epoch 10/135\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1206.7861 - val_loss: 2139.0608\n",
      "Epoch 11/135\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1102.3403 - val_loss: 1903.1124\n",
      "Epoch 12/135\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1015.6454 - val_loss: 1668.1947\n",
      "Epoch 13/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 941.2252 - val_loss: 1452.8518\n",
      "Epoch 14/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 889.4058 - val_loss: 1250.5251\n",
      "Epoch 15/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 847.2328 - val_loss: 1143.1172\n",
      "Epoch 16/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 813.3982 - val_loss: 1048.7449\n",
      "Epoch 17/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 774.1173 - val_loss: 978.3116\n",
      "Epoch 18/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 762.2279 - val_loss: 983.2184\n",
      "Epoch 19/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 738.3783 - val_loss: 869.0367\n",
      "Epoch 20/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 725.4056 - val_loss: 753.4837\n",
      "Epoch 21/135\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 700.1501 - val_loss: 666.6447\n",
      "Epoch 22/135\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 689.5900 - val_loss: 617.9510\n",
      "Epoch 23/135\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 670.2348 - val_loss: 479.3387\n",
      "Epoch 24/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 651.4903 - val_loss: 547.5028\n",
      "Epoch 25/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 650.4155 - val_loss: 489.1591\n",
      "Epoch 26/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 640.1661 - val_loss: 467.4583\n",
      "Epoch 27/135\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 623.1335 - val_loss: 455.2512\n",
      "Epoch 28/135\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 616.1154 - val_loss: 493.7705\n",
      "Epoch 29/135\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 600.4323 - val_loss: 449.3884\n",
      "Epoch 30/135\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 595.2560 - val_loss: 454.5206\n",
      "Epoch 31/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 591.8088 - val_loss: 448.3417\n",
      "Epoch 32/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 577.9885 - val_loss: 473.9174\n",
      "Epoch 33/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 578.4392 - val_loss: 503.3068\n",
      "Epoch 34/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 558.0095 - val_loss: 505.0199\n",
      "Epoch 35/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 543.7302 - val_loss: 429.3068\n",
      "Epoch 36/135\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 548.5831 - val_loss: 417.0415\n",
      "Epoch 37/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 531.1631 - val_loss: 437.1591\n",
      "Epoch 38/135\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 531.8057 - val_loss: 441.5592\n",
      "Epoch 39/135\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 528.6882 - val_loss: 423.6247\n",
      "Epoch 40/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 519.1846 - val_loss: 362.0140\n",
      "Epoch 41/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 509.7982 - val_loss: 422.3291\n",
      "Epoch 42/135\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 499.4187 - val_loss: 451.9956\n",
      "Epoch 43/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 499.5800 - val_loss: 501.6101\n",
      "Epoch 44/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 486.7025 - val_loss: 489.6833\n",
      "Epoch 45/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 480.2818 - val_loss: 509.4551\n",
      "Epoch 46/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 472.8949 - val_loss: 470.6810\n",
      "Epoch 47/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 469.1503 - val_loss: 472.8214\n",
      "Epoch 48/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 462.6843 - val_loss: 445.4308\n",
      "Epoch 49/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 449.6562 - val_loss: 481.9082\n",
      "Epoch 50/135\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 448.1480 - val_loss: 435.0607\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1180.4874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:03:50,661] Trial 20 finished with value: 1180.4874267578125 and parameters: {'head_size': 104, 'num_heads': 5, 'ff_dim': 18, 'num_transformer_blocks': 7, 'mlp_units': 94, 'mlp_dropout': 0.19214655971356415, 'dropout': 0.1639271862860279, 'learning_rate': 0.00014534984533241683, 'n_epochs': 135}. Best is trial 19 with value: 826.89697265625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_226 (L  (None, 8, 5)                 10        ['input_22[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_113 (  (None, 8, 5)                 15599     ['layer_normalization_226[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_226[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_247 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_113[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_226 (  (None, 8, 5)                 0         ['dropout_247[0][0]',         \n",
      " TFOpLambda)                                                         'input_22[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_227 (L  (None, 8, 5)                 10        ['tf.__operators__.add_226[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_226 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_227[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_248 (Dropout)       (None, 8, 18)                0         ['conv1d_226[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_227 (Conv1D)         (None, 8, 5)                 95        ['dropout_248[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_227 (  (None, 8, 5)                 0         ['conv1d_227[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_226[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_228 (L  (None, 8, 5)                 10        ['tf.__operators__.add_227[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_114 (  (None, 8, 5)                 15599     ['layer_normalization_228[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_228[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_249 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_114[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_228 (  (None, 8, 5)                 0         ['dropout_249[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_227[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_229 (L  (None, 8, 5)                 10        ['tf.__operators__.add_228[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_228 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_229[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_250 (Dropout)       (None, 8, 18)                0         ['conv1d_228[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_229 (Conv1D)         (None, 8, 5)                 95        ['dropout_250[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_229 (  (None, 8, 5)                 0         ['conv1d_229[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_228[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_230 (L  (None, 8, 5)                 10        ['tf.__operators__.add_229[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_115 (  (None, 8, 5)                 15599     ['layer_normalization_230[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_230[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_251 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_115[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_230 (  (None, 8, 5)                 0         ['dropout_251[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_229[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_231 (L  (None, 8, 5)                 10        ['tf.__operators__.add_230[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_230 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_231[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_252 (Dropout)       (None, 8, 18)                0         ['conv1d_230[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_231 (Conv1D)         (None, 8, 5)                 95        ['dropout_252[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_231 (  (None, 8, 5)                 0         ['conv1d_231[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_230[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_232 (L  (None, 8, 5)                 10        ['tf.__operators__.add_231[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_116 (  (None, 8, 5)                 15599     ['layer_normalization_232[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_232[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_253 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_116[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_232 (  (None, 8, 5)                 0         ['dropout_253[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_231[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_233 (L  (None, 8, 5)                 10        ['tf.__operators__.add_232[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_232 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_233[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_254 (Dropout)       (None, 8, 18)                0         ['conv1d_232[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_233 (Conv1D)         (None, 8, 5)                 95        ['dropout_254[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_233 (  (None, 8, 5)                 0         ['conv1d_233[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_232[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_234 (L  (None, 8, 5)                 10        ['tf.__operators__.add_233[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_117 (  (None, 8, 5)                 15599     ['layer_normalization_234[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_234[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_255 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_117[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_234 (  (None, 8, 5)                 0         ['dropout_255[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_233[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_235 (L  (None, 8, 5)                 10        ['tf.__operators__.add_234[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_234 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_235[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_256 (Dropout)       (None, 8, 18)                0         ['conv1d_234[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_235 (Conv1D)         (None, 8, 5)                 95        ['dropout_256[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_235 (  (None, 8, 5)                 0         ['conv1d_235[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_234[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_236 (L  (None, 8, 5)                 10        ['tf.__operators__.add_235[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_118 (  (None, 8, 5)                 15599     ['layer_normalization_236[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_236[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_257 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_118[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_236 (  (None, 8, 5)                 0         ['dropout_257[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_235[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_237 (L  (None, 8, 5)                 10        ['tf.__operators__.add_236[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_236 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_237[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_258 (Dropout)       (None, 8, 18)                0         ['conv1d_236[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_237 (Conv1D)         (None, 8, 5)                 95        ['dropout_258[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_237 (  (None, 8, 5)                 0         ['conv1d_237[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_236[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_238 (L  (None, 8, 5)                 10        ['tf.__operators__.add_237[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_119 (  (None, 8, 5)                 15599     ['layer_normalization_238[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_238[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_259 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_119[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_238 (  (None, 8, 5)                 0         ['dropout_259[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_237[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_239 (L  (None, 8, 5)                 10        ['tf.__operators__.add_238[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_238 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_239[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_260 (Dropout)       (None, 8, 18)                0         ['conv1d_238[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_239 (Conv1D)         (None, 8, 5)                 95        ['dropout_260[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_239 (  (None, 8, 5)                 0         ['conv1d_239[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_238[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_240 (L  (None, 8, 5)                 10        ['tf.__operators__.add_239[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_120 (  (None, 8, 5)                 15599     ['layer_normalization_240[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_240[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_261 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_120[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_240 (  (None, 8, 5)                 0         ['dropout_261[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_239[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_241 (L  (None, 8, 5)                 10        ['tf.__operators__.add_240[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_240 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_241[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_262 (Dropout)       (None, 8, 18)                0         ['conv1d_240[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_241 (Conv1D)         (None, 8, 5)                 95        ['dropout_262[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_241 (  (None, 8, 5)                 0         ['conv1d_241[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_240[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_241[0][\n",
      " 1 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_63 (Dense)            (None, 104)                  936       ['global_average_pooling1d_21[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_263 (Dropout)       (None, 104)                  0         ['dense_63[0][0]']            \n",
      "                                                                                                  \n",
      " dense_64 (Dense)            (None, 16)                   1680      ['dropout_263[0][0]']         \n",
      "                                                                                                  \n",
      " dense_65 (Dense)            (None, 8)                    136       ['dense_64[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 129328 (505.19 KB)\n",
      "Trainable params: 129328 (505.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/148\n",
      "20/20 [==============================] - 3s 46ms/step - loss: 1942.4761 - val_loss: 2706.8650\n",
      "Epoch 2/148\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 943.8351 - val_loss: 1485.1877\n",
      "Epoch 3/148\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 784.2438 - val_loss: 1575.8192\n",
      "Epoch 4/148\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 675.1669 - val_loss: 1244.4803\n",
      "Epoch 5/148\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 616.7797 - val_loss: 927.4673\n",
      "Epoch 6/148\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 569.0854 - val_loss: 668.8544\n",
      "Epoch 7/148\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 541.4907 - val_loss: 499.7513\n",
      "Epoch 8/148\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 495.1560 - val_loss: 554.2581\n",
      "Epoch 9/148\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 465.2955 - val_loss: 508.3389\n",
      "Epoch 10/148\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 448.8452 - val_loss: 393.2594\n",
      "Epoch 11/148\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 422.0508 - val_loss: 535.9458\n",
      "Epoch 12/148\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 402.1373 - val_loss: 569.5972\n",
      "Epoch 13/148\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 395.7559 - val_loss: 412.9725\n",
      "Epoch 14/148\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 366.7168 - val_loss: 511.4884\n",
      "Epoch 15/148\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 359.3761 - val_loss: 329.7451\n",
      "Epoch 16/148\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 338.7531 - val_loss: 364.9222\n",
      "Epoch 17/148\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 327.1259 - val_loss: 399.3619\n",
      "Epoch 18/148\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 310.0632 - val_loss: 504.5967\n",
      "Epoch 19/148\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 306.9704 - val_loss: 354.5232\n",
      "Epoch 20/148\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 297.5299 - val_loss: 425.7296\n",
      "Epoch 21/148\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 283.5375 - val_loss: 406.3705\n",
      "Epoch 22/148\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 271.7957 - val_loss: 421.5365\n",
      "Epoch 23/148\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 268.0602 - val_loss: 441.8495\n",
      "Epoch 24/148\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 258.1763 - val_loss: 437.0053\n",
      "Epoch 25/148\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 250.8875 - val_loss: 358.3596\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1080.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:04:14,405] Trial 21 finished with value: 1080.125732421875 and parameters: {'head_size': 113, 'num_heads': 6, 'ff_dim': 18, 'num_transformer_blocks': 8, 'mlp_units': 104, 'mlp_dropout': 0.15645057915078253, 'dropout': 0.10228637412287699, 'learning_rate': 0.0005444831356127258, 'n_epochs': 148}. Best is trial 19 with value: 826.89697265625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_242 (L  (None, 8, 5)                 10        ['input_23[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_121 (  (None, 8, 5)                 9251      ['layer_normalization_242[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_242[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_264 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_121[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_242 (  (None, 8, 5)                 0         ['dropout_264[0][0]',         \n",
      " TFOpLambda)                                                         'input_23[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_243 (L  (None, 8, 5)                 10        ['tf.__operators__.add_242[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_242 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_243[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_265 (Dropout)       (None, 8, 18)                0         ['conv1d_242[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_243 (Conv1D)         (None, 8, 5)                 95        ['dropout_265[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_243 (  (None, 8, 5)                 0         ['conv1d_243[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_242[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_244 (L  (None, 8, 5)                 10        ['tf.__operators__.add_243[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_122 (  (None, 8, 5)                 9251      ['layer_normalization_244[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_244[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_266 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_122[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_244 (  (None, 8, 5)                 0         ['dropout_266[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_243[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_245 (L  (None, 8, 5)                 10        ['tf.__operators__.add_244[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_244 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_245[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_267 (Dropout)       (None, 8, 18)                0         ['conv1d_244[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_245 (Conv1D)         (None, 8, 5)                 95        ['dropout_267[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_245 (  (None, 8, 5)                 0         ['conv1d_245[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_244[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_246 (L  (None, 8, 5)                 10        ['tf.__operators__.add_245[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_123 (  (None, 8, 5)                 9251      ['layer_normalization_246[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_246[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_268 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_123[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_246 (  (None, 8, 5)                 0         ['dropout_268[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_245[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_247 (L  (None, 8, 5)                 10        ['tf.__operators__.add_246[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_246 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_247[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_269 (Dropout)       (None, 8, 18)                0         ['conv1d_246[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_247 (Conv1D)         (None, 8, 5)                 95        ['dropout_269[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_247 (  (None, 8, 5)                 0         ['conv1d_247[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_246[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_248 (L  (None, 8, 5)                 10        ['tf.__operators__.add_247[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_124 (  (None, 8, 5)                 9251      ['layer_normalization_248[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_248[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_270 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_124[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_248 (  (None, 8, 5)                 0         ['dropout_270[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_247[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_249 (L  (None, 8, 5)                 10        ['tf.__operators__.add_248[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_248 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_249[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_271 (Dropout)       (None, 8, 18)                0         ['conv1d_248[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_249 (Conv1D)         (None, 8, 5)                 95        ['dropout_271[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_249 (  (None, 8, 5)                 0         ['conv1d_249[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_248[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_250 (L  (None, 8, 5)                 10        ['tf.__operators__.add_249[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_125 (  (None, 8, 5)                 9251      ['layer_normalization_250[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_250[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_272 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_125[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_250 (  (None, 8, 5)                 0         ['dropout_272[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_249[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_251 (L  (None, 8, 5)                 10        ['tf.__operators__.add_250[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_250 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_251[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_273 (Dropout)       (None, 8, 18)                0         ['conv1d_250[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_251 (Conv1D)         (None, 8, 5)                 95        ['dropout_273[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_251 (  (None, 8, 5)                 0         ['conv1d_251[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_250[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_252 (L  (None, 8, 5)                 10        ['tf.__operators__.add_251[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_126 (  (None, 8, 5)                 9251      ['layer_normalization_252[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_252[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_274 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_126[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_252 (  (None, 8, 5)                 0         ['dropout_274[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_251[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_253 (L  (None, 8, 5)                 10        ['tf.__operators__.add_252[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_252 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_253[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_275 (Dropout)       (None, 8, 18)                0         ['conv1d_252[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_253 (Conv1D)         (None, 8, 5)                 95        ['dropout_275[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_253 (  (None, 8, 5)                 0         ['conv1d_253[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_252[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_254 (L  (None, 8, 5)                 10        ['tf.__operators__.add_253[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_127 (  (None, 8, 5)                 9251      ['layer_normalization_254[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_254[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_276 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_127[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_254 (  (None, 8, 5)                 0         ['dropout_276[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_253[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_255 (L  (None, 8, 5)                 10        ['tf.__operators__.add_254[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_254 (Conv1D)         (None, 8, 18)                108       ['layer_normalization_255[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_277 (Dropout)       (None, 8, 18)                0         ['conv1d_254[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_255 (Conv1D)         (None, 8, 5)                 95        ['dropout_277[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_255 (  (None, 8, 5)                 0         ['conv1d_255[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_254[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_255[0][\n",
      " 2 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_66 (Dense)            (None, 104)                  936       ['global_average_pooling1d_22[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_278 (Dropout)       (None, 104)                  0         ['dense_66[0][0]']            \n",
      "                                                                                                  \n",
      " dense_67 (Dense)            (None, 16)                   1680      ['dropout_278[0][0]']         \n",
      "                                                                                                  \n",
      " dense_68 (Dense)            (None, 8)                    136       ['dense_67[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69070 (269.80 KB)\n",
      "Trainable params: 69070 (269.80 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/166\n",
      "20/20 [==============================] - 2s 39ms/step - loss: 1453.4978 - val_loss: 2250.2441\n",
      "Epoch 2/166\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 757.0151 - val_loss: 648.0610\n",
      "Epoch 3/166\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 603.0891 - val_loss: 809.1577\n",
      "Epoch 4/166\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 550.9823 - val_loss: 932.7946\n",
      "Epoch 5/166\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 496.7976 - val_loss: 845.6714\n",
      "Epoch 6/166\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 456.4398 - val_loss: 610.1770\n",
      "Epoch 7/166\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 422.4763 - val_loss: 608.0085\n",
      "Epoch 8/166\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 396.8709 - val_loss: 411.2496\n",
      "Epoch 9/166\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 360.0473 - val_loss: 565.1877\n",
      "Epoch 10/166\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 335.7212 - val_loss: 518.2484\n",
      "Epoch 11/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 320.6764 - val_loss: 411.7130\n",
      "Epoch 12/166\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 302.7163 - val_loss: 387.2031\n",
      "Epoch 13/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 273.2781 - val_loss: 476.7340\n",
      "Epoch 14/166\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 269.2695 - val_loss: 490.8154\n",
      "Epoch 15/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 251.5023 - val_loss: 561.2573\n",
      "Epoch 16/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 247.7633 - val_loss: 390.7124\n",
      "Epoch 17/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 234.0943 - val_loss: 464.7414\n",
      "Epoch 18/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 219.8948 - val_loss: 446.6262\n",
      "Epoch 19/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 216.9341 - val_loss: 483.4636\n",
      "Epoch 20/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 209.9142 - val_loss: 394.1435\n",
      "Epoch 21/166\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 210.8243 - val_loss: 380.0398\n",
      "Epoch 22/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 195.2922 - val_loss: 436.0545\n",
      "Epoch 23/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 194.9920 - val_loss: 286.8390\n",
      "Epoch 24/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 183.8182 - val_loss: 382.6310\n",
      "Epoch 25/166\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 173.3627 - val_loss: 385.1960\n",
      "Epoch 26/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 172.0501 - val_loss: 408.2028\n",
      "Epoch 27/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 170.1134 - val_loss: 430.9337\n",
      "Epoch 28/166\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 166.4763 - val_loss: 400.3778\n",
      "Epoch 29/166\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 162.8948 - val_loss: 414.0603\n",
      "Epoch 30/166\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 164.0060 - val_loss: 399.5262\n",
      "Epoch 31/166\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 157.4011 - val_loss: 411.8920\n",
      "Epoch 32/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 155.9272 - val_loss: 378.7136\n",
      "Epoch 33/166\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 150.2141 - val_loss: 395.3795\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 992.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:04:43,909] Trial 22 finished with value: 992.69482421875 and parameters: {'head_size': 67, 'num_heads': 6, 'ff_dim': 18, 'num_transformer_blocks': 7, 'mlp_units': 104, 'mlp_dropout': 0.13640571515174382, 'dropout': 0.10422147080080481, 'learning_rate': 0.000978358208960888, 'n_epochs': 166}. Best is trial 19 with value: 826.89697265625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_256 (L  (None, 8, 5)                 10        ['input_24[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_128 (  (None, 8, 5)                 28824     ['layer_normalization_256[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_256[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_279 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_128[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_256 (  (None, 8, 5)                 0         ['dropout_279[0][0]',         \n",
      " TFOpLambda)                                                         'input_24[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_257 (L  (None, 8, 5)                 10        ['tf.__operators__.add_256[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_256 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_257[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_280 (Dropout)       (None, 8, 14)                0         ['conv1d_256[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_257 (Conv1D)         (None, 8, 5)                 75        ['dropout_280[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_257 (  (None, 8, 5)                 0         ['conv1d_257[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_256[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_258 (L  (None, 8, 5)                 10        ['tf.__operators__.add_257[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_129 (  (None, 8, 5)                 28824     ['layer_normalization_258[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_258[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_281 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_129[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_258 (  (None, 8, 5)                 0         ['dropout_281[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_257[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_259 (L  (None, 8, 5)                 10        ['tf.__operators__.add_258[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_258 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_259[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_282 (Dropout)       (None, 8, 14)                0         ['conv1d_258[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_259 (Conv1D)         (None, 8, 5)                 75        ['dropout_282[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_259 (  (None, 8, 5)                 0         ['conv1d_259[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_258[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_260 (L  (None, 8, 5)                 10        ['tf.__operators__.add_259[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_130 (  (None, 8, 5)                 28824     ['layer_normalization_260[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_260[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_283 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_130[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_260 (  (None, 8, 5)                 0         ['dropout_283[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_259[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_261 (L  (None, 8, 5)                 10        ['tf.__operators__.add_260[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_260 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_261[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_284 (Dropout)       (None, 8, 14)                0         ['conv1d_260[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_261 (Conv1D)         (None, 8, 5)                 75        ['dropout_284[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_261 (  (None, 8, 5)                 0         ['conv1d_261[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_260[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_262 (L  (None, 8, 5)                 10        ['tf.__operators__.add_261[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_131 (  (None, 8, 5)                 28824     ['layer_normalization_262[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_262[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_285 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_131[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_262 (  (None, 8, 5)                 0         ['dropout_285[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_261[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_263 (L  (None, 8, 5)                 10        ['tf.__operators__.add_262[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_262 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_263[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_286 (Dropout)       (None, 8, 14)                0         ['conv1d_262[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_263 (Conv1D)         (None, 8, 5)                 75        ['dropout_286[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_263 (  (None, 8, 5)                 0         ['conv1d_263[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_262[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_264 (L  (None, 8, 5)                 10        ['tf.__operators__.add_263[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_132 (  (None, 8, 5)                 28824     ['layer_normalization_264[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_264[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_287 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_132[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_264 (  (None, 8, 5)                 0         ['dropout_287[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_263[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_265 (L  (None, 8, 5)                 10        ['tf.__operators__.add_264[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_264 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_265[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_288 (Dropout)       (None, 8, 14)                0         ['conv1d_264[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_265 (Conv1D)         (None, 8, 5)                 75        ['dropout_288[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_265 (  (None, 8, 5)                 0         ['conv1d_265[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_264[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_266 (L  (None, 8, 5)                 10        ['tf.__operators__.add_265[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_133 (  (None, 8, 5)                 28824     ['layer_normalization_266[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_266[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_289 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_133[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_266 (  (None, 8, 5)                 0         ['dropout_289[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_265[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_267 (L  (None, 8, 5)                 10        ['tf.__operators__.add_266[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_266 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_267[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_290 (Dropout)       (None, 8, 14)                0         ['conv1d_266[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_267 (Conv1D)         (None, 8, 5)                 75        ['dropout_290[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_267 (  (None, 8, 5)                 0         ['conv1d_267[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_266[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_267[0][\n",
      " 3 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_69 (Dense)            (None, 118)                  1062      ['global_average_pooling1d_23[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_291 (Dropout)       (None, 118)                  0         ['dense_69[0][0]']            \n",
      "                                                                                                  \n",
      " dense_70 (Dense)            (None, 16)                   1904      ['dropout_291[0][0]']         \n",
      "                                                                                                  \n",
      " dense_71 (Dense)            (None, 8)                    136       ['dense_70[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177120 (691.88 KB)\n",
      "Trainable params: 177120 (691.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/123\n",
      "20/20 [==============================] - 3s 49ms/step - loss: 2012.2816 - val_loss: 4719.9307\n",
      "Epoch 2/123\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1416.6335 - val_loss: 3584.5767\n",
      "Epoch 3/123\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1081.4482 - val_loss: 2504.0339\n",
      "Epoch 4/123\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 808.4290 - val_loss: 1507.7388\n",
      "Epoch 5/123\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 665.9935 - val_loss: 1046.7991\n",
      "Epoch 6/123\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 606.0683 - val_loss: 575.8732\n",
      "Epoch 7/123\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 552.1080 - val_loss: 464.9829\n",
      "Epoch 8/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 524.3796 - val_loss: 448.8269\n",
      "Epoch 9/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 484.9342 - val_loss: 547.9280\n",
      "Epoch 10/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 461.9680 - val_loss: 469.1219\n",
      "Epoch 11/123\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 438.8502 - val_loss: 551.5507\n",
      "Epoch 12/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 419.7317 - val_loss: 505.7693\n",
      "Epoch 13/123\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 405.5067 - val_loss: 512.5324\n",
      "Epoch 14/123\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 385.0995 - val_loss: 366.7470\n",
      "Epoch 15/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 360.9436 - val_loss: 412.2335\n",
      "Epoch 16/123\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 345.3115 - val_loss: 347.9519\n",
      "Epoch 17/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 338.0676 - val_loss: 338.7343\n",
      "Epoch 18/123\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 321.4607 - val_loss: 419.3849\n",
      "Epoch 19/123\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 310.5804 - val_loss: 415.6159\n",
      "Epoch 20/123\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 303.3274 - val_loss: 321.8064\n",
      "Epoch 21/123\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 293.9136 - val_loss: 252.7556\n",
      "Epoch 22/123\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 287.6719 - val_loss: 379.0929\n",
      "Epoch 23/123\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 273.0734 - val_loss: 372.5757\n",
      "Epoch 24/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 270.2272 - val_loss: 363.2462\n",
      "Epoch 25/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 257.7807 - val_loss: 405.3557\n",
      "Epoch 26/123\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 249.8200 - val_loss: 360.4812\n",
      "Epoch 27/123\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 239.4489 - val_loss: 374.3129\n",
      "Epoch 28/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 242.7954 - val_loss: 349.4320\n",
      "Epoch 29/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 233.4567 - val_loss: 316.7397\n",
      "Epoch 30/123\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 231.3555 - val_loss: 363.2321\n",
      "Epoch 31/123\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 225.1327 - val_loss: 311.7173\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 802.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:05:14,341] Trial 23 finished with value: 802.0839233398438 and parameters: {'head_size': 179, 'num_heads': 7, 'ff_dim': 14, 'num_transformer_blocks': 6, 'mlp_units': 118, 'mlp_dropout': 0.17581215834101643, 'dropout': 0.13736365988904972, 'learning_rate': 0.0004578242212281878, 'n_epochs': 123}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_268 (L  (None, 8, 5)                 10        ['input_25[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_134 (  (None, 8, 5)                 23994     ['layer_normalization_268[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_268[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_292 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_134[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_268 (  (None, 8, 5)                 0         ['dropout_292[0][0]',         \n",
      " TFOpLambda)                                                         'input_25[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_269 (L  (None, 8, 5)                 10        ['tf.__operators__.add_268[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_268 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_269[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_293 (Dropout)       (None, 8, 15)                0         ['conv1d_268[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_269 (Conv1D)         (None, 8, 5)                 80        ['dropout_293[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_269 (  (None, 8, 5)                 0         ['conv1d_269[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_268[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_270 (L  (None, 8, 5)                 10        ['tf.__operators__.add_269[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_135 (  (None, 8, 5)                 23994     ['layer_normalization_270[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_270[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_294 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_135[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_270 (  (None, 8, 5)                 0         ['dropout_294[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_269[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_271 (L  (None, 8, 5)                 10        ['tf.__operators__.add_270[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_270 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_271[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_295 (Dropout)       (None, 8, 15)                0         ['conv1d_270[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_271 (Conv1D)         (None, 8, 5)                 80        ['dropout_295[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_271 (  (None, 8, 5)                 0         ['conv1d_271[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_270[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_272 (L  (None, 8, 5)                 10        ['tf.__operators__.add_271[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_136 (  (None, 8, 5)                 23994     ['layer_normalization_272[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_272[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_296 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_136[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_272 (  (None, 8, 5)                 0         ['dropout_296[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_271[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_273 (L  (None, 8, 5)                 10        ['tf.__operators__.add_272[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_272 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_273[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_297 (Dropout)       (None, 8, 15)                0         ['conv1d_272[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_273 (Conv1D)         (None, 8, 5)                 80        ['dropout_297[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_273 (  (None, 8, 5)                 0         ['conv1d_273[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_272[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_274 (L  (None, 8, 5)                 10        ['tf.__operators__.add_273[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_137 (  (None, 8, 5)                 23994     ['layer_normalization_274[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_274[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_298 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_137[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_274 (  (None, 8, 5)                 0         ['dropout_298[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_273[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_275 (L  (None, 8, 5)                 10        ['tf.__operators__.add_274[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_274 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_275[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_299 (Dropout)       (None, 8, 15)                0         ['conv1d_274[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_275 (Conv1D)         (None, 8, 5)                 80        ['dropout_299[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_275 (  (None, 8, 5)                 0         ['conv1d_275[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_274[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_276 (L  (None, 8, 5)                 10        ['tf.__operators__.add_275[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_138 (  (None, 8, 5)                 23994     ['layer_normalization_276[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_276[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_300 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_138[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_276 (  (None, 8, 5)                 0         ['dropout_300[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_275[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_277 (L  (None, 8, 5)                 10        ['tf.__operators__.add_276[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_276 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_277[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_301 (Dropout)       (None, 8, 15)                0         ['conv1d_276[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_277 (Conv1D)         (None, 8, 5)                 80        ['dropout_301[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_277 (  (None, 8, 5)                 0         ['conv1d_277[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_276[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_278 (L  (None, 8, 5)                 10        ['tf.__operators__.add_277[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_139 (  (None, 8, 5)                 23994     ['layer_normalization_278[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_278[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_302 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_139[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_278 (  (None, 8, 5)                 0         ['dropout_302[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_277[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_279 (L  (None, 8, 5)                 10        ['tf.__operators__.add_278[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_278 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_279[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_303 (Dropout)       (None, 8, 15)                0         ['conv1d_278[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_279 (Conv1D)         (None, 8, 5)                 80        ['dropout_303[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_279 (  (None, 8, 5)                 0         ['conv1d_279[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_278[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_279[0][\n",
      " 4 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_72 (Dense)            (None, 119)                  1071      ['global_average_pooling1d_24[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_304 (Dropout)       (None, 119)                  0         ['dense_72[0][0]']            \n",
      "                                                                                                  \n",
      " dense_73 (Dense)            (None, 16)                   1920      ['dropout_304[0][0]']         \n",
      "                                                                                                  \n",
      " dense_74 (Dense)            (None, 8)                    136       ['dense_73[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 148231 (579.03 KB)\n",
      "Trainable params: 148231 (579.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/119\n",
      "20/20 [==============================] - 2s 51ms/step - loss: 1664.5316 - val_loss: 4342.0117\n",
      "Epoch 2/119\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1217.4445 - val_loss: 3286.5154\n",
      "Epoch 3/119\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 1009.6628 - val_loss: 2219.5881\n",
      "Epoch 4/119\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 907.9395 - val_loss: 1940.8185\n",
      "Epoch 5/119\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 824.6212 - val_loss: 1678.5447\n",
      "Epoch 6/119\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 754.8168 - val_loss: 1503.5618\n",
      "Epoch 7/119\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 691.0506 - val_loss: 1073.5177\n",
      "Epoch 8/119\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 624.1912 - val_loss: 846.7308\n",
      "Epoch 9/119\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 567.5560 - val_loss: 599.0278\n",
      "Epoch 10/119\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 536.1594 - val_loss: 461.6699\n",
      "Epoch 11/119\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 498.7935 - val_loss: 443.7247\n",
      "Epoch 12/119\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 484.8496 - val_loss: 344.6115\n",
      "Epoch 13/119\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 467.8172 - val_loss: 347.7707\n",
      "Epoch 14/119\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 438.1981 - val_loss: 484.8026\n",
      "Epoch 15/119\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 426.3262 - val_loss: 402.2180\n",
      "Epoch 16/119\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 405.0748 - val_loss: 558.9324\n",
      "Epoch 17/119\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 390.9048 - val_loss: 416.5119\n",
      "Epoch 18/119\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 375.7801 - val_loss: 409.8248\n",
      "Epoch 19/119\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 355.1278 - val_loss: 348.4549\n",
      "Epoch 20/119\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 347.2712 - val_loss: 426.2855\n",
      "Epoch 21/119\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 337.2943 - val_loss: 377.8848\n",
      "Epoch 22/119\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 318.5889 - val_loss: 499.6840\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1236.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:05:34,469] Trial 24 finished with value: 1236.9083251953125 and parameters: {'head_size': 149, 'num_heads': 7, 'ff_dim': 15, 'num_transformer_blocks': 6, 'mlp_units': 119, 'mlp_dropout': 0.17841113079564597, 'dropout': 0.1457001395909785, 'learning_rate': 0.0003762304745968449, 'n_epochs': 119}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_280 (L  (None, 8, 5)                 10        ['input_26[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_140 (  (None, 8, 5)                 31607     ['layer_normalization_280[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_280[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_305 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_140[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_280 (  (None, 8, 5)                 0         ['dropout_305[0][0]',         \n",
      " TFOpLambda)                                                         'input_26[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_281 (L  (None, 8, 5)                 10        ['tf.__operators__.add_280[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_280 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_281[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_306 (Dropout)       (None, 8, 19)                0         ['conv1d_280[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_281 (Conv1D)         (None, 8, 5)                 100       ['dropout_306[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_281 (  (None, 8, 5)                 0         ['conv1d_281[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_280[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_282 (L  (None, 8, 5)                 10        ['tf.__operators__.add_281[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_141 (  (None, 8, 5)                 31607     ['layer_normalization_282[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_282[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_307 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_141[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_282 (  (None, 8, 5)                 0         ['dropout_307[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_281[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_283 (L  (None, 8, 5)                 10        ['tf.__operators__.add_282[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_282 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_283[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_308 (Dropout)       (None, 8, 19)                0         ['conv1d_282[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_283 (Conv1D)         (None, 8, 5)                 100       ['dropout_308[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_283 (  (None, 8, 5)                 0         ['conv1d_283[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_282[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_284 (L  (None, 8, 5)                 10        ['tf.__operators__.add_283[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_142 (  (None, 8, 5)                 31607     ['layer_normalization_284[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_284[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_309 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_142[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_284 (  (None, 8, 5)                 0         ['dropout_309[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_283[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_285 (L  (None, 8, 5)                 10        ['tf.__operators__.add_284[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_284 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_285[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_310 (Dropout)       (None, 8, 19)                0         ['conv1d_284[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_285 (Conv1D)         (None, 8, 5)                 100       ['dropout_310[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_285 (  (None, 8, 5)                 0         ['conv1d_285[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_284[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_286 (L  (None, 8, 5)                 10        ['tf.__operators__.add_285[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_143 (  (None, 8, 5)                 31607     ['layer_normalization_286[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_286[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_311 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_143[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_286 (  (None, 8, 5)                 0         ['dropout_311[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_285[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_287 (L  (None, 8, 5)                 10        ['tf.__operators__.add_286[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_286 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_287[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_312 (Dropout)       (None, 8, 19)                0         ['conv1d_286[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_287 (Conv1D)         (None, 8, 5)                 100       ['dropout_312[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_287 (  (None, 8, 5)                 0         ['conv1d_287[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_286[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_288 (L  (None, 8, 5)                 10        ['tf.__operators__.add_287[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_144 (  (None, 8, 5)                 31607     ['layer_normalization_288[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_288[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_313 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_144[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_288 (  (None, 8, 5)                 0         ['dropout_313[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_287[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_289 (L  (None, 8, 5)                 10        ['tf.__operators__.add_288[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_288 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_289[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_314 (Dropout)       (None, 8, 19)                0         ['conv1d_288[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_289 (Conv1D)         (None, 8, 5)                 100       ['dropout_314[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_289 (  (None, 8, 5)                 0         ['conv1d_289[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_288[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_290 (L  (None, 8, 5)                 10        ['tf.__operators__.add_289[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_145 (  (None, 8, 5)                 31607     ['layer_normalization_290[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_290[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_315 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_145[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_290 (  (None, 8, 5)                 0         ['dropout_315[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_289[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_291 (L  (None, 8, 5)                 10        ['tf.__operators__.add_290[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_290 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_291[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_316 (Dropout)       (None, 8, 19)                0         ['conv1d_290[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_291 (Conv1D)         (None, 8, 5)                 100       ['dropout_316[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_291 (  (None, 8, 5)                 0         ['conv1d_291[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_290[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_292 (L  (None, 8, 5)                 10        ['tf.__operators__.add_291[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_146 (  (None, 8, 5)                 31607     ['layer_normalization_292[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_292[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_317 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_146[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_292 (  (None, 8, 5)                 0         ['dropout_317[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_291[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_293 (L  (None, 8, 5)                 10        ['tf.__operators__.add_292[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_292 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_293[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_318 (Dropout)       (None, 8, 19)                0         ['conv1d_292[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_293 (Conv1D)         (None, 8, 5)                 100       ['dropout_318[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_293 (  (None, 8, 5)                 0         ['conv1d_293[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_292[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_293[0][\n",
      " 5 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_75 (Dense)            (None, 116)                  1044      ['global_average_pooling1d_25[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_319 (Dropout)       (None, 116)                  0         ['dense_75[0][0]']            \n",
      "                                                                                                  \n",
      " dense_76 (Dense)            (None, 16)                   1872      ['dropout_319[0][0]']         \n",
      "                                                                                                  \n",
      " dense_77 (Dense)            (None, 8)                    136       ['dense_76[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 225939 (882.57 KB)\n",
      "Trainable params: 225939 (882.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/93\n",
      "20/20 [==============================] - 3s 60ms/step - loss: 2105.0491 - val_loss: 2842.4568\n",
      "Epoch 2/93\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 1244.9652 - val_loss: 1355.6412\n",
      "Epoch 3/93\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 1003.8181 - val_loss: 1122.8936\n",
      "Epoch 4/93\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 872.5850 - val_loss: 1467.6283\n",
      "Epoch 5/93\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 781.1456 - val_loss: 1332.5931\n",
      "Epoch 6/93\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 700.8050 - val_loss: 1229.2300\n",
      "Epoch 7/93\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 636.3476 - val_loss: 1011.7545\n",
      "Epoch 8/93\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 598.4737 - val_loss: 817.2003\n",
      "Epoch 9/93\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 550.1708 - val_loss: 701.2115\n",
      "Epoch 10/93\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 515.8912 - val_loss: 600.8507\n",
      "Epoch 11/93\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 488.1098 - val_loss: 622.5004\n",
      "Epoch 12/93\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 458.0275 - val_loss: 490.7576\n",
      "Epoch 13/93\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 445.6916 - val_loss: 414.3914\n",
      "Epoch 14/93\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 408.1061 - val_loss: 557.6995\n",
      "Epoch 15/93\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 404.2621 - val_loss: 400.7847\n",
      "Epoch 16/93\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 378.7943 - val_loss: 427.5902\n",
      "Epoch 17/93\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 362.1815 - val_loss: 546.8890\n",
      "Epoch 18/93\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 353.3585 - val_loss: 448.4018\n",
      "Epoch 19/93\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 339.3853 - val_loss: 525.2863\n",
      "Epoch 20/93\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 315.9666 - val_loss: 407.0182\n",
      "Epoch 21/93\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 318.4564 - val_loss: 393.6678\n",
      "Epoch 22/93\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 302.2746 - val_loss: 427.3781\n",
      "Epoch 23/93\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 291.6446 - val_loss: 453.6092\n",
      "Epoch 24/93\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 285.8141 - val_loss: 427.3553\n",
      "Epoch 25/93\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 280.1313 - val_loss: 382.0338\n",
      "Epoch 26/93\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 267.3448 - val_loss: 370.0364\n",
      "Epoch 27/93\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 260.7454 - val_loss: 397.7804\n",
      "Epoch 28/93\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 250.7611 - val_loss: 374.8358\n",
      "Epoch 29/93\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 257.1351 - val_loss: 376.7520\n",
      "Epoch 30/93\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 242.9905 - val_loss: 412.2697\n",
      "Epoch 31/93\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 242.2656 - val_loss: 386.5789\n",
      "Epoch 32/93\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 233.6586 - val_loss: 465.4168\n",
      "Epoch 33/93\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 228.7976 - val_loss: 425.4723\n",
      "Epoch 34/93\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 224.6051 - val_loss: 374.1758\n",
      "Epoch 35/93\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 218.4872 - val_loss: 385.1407\n",
      "Epoch 36/93\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 210.6224 - val_loss: 452.2197\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 971.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:06:18,431] Trial 25 finished with value: 971.1444702148438 and parameters: {'head_size': 229, 'num_heads': 6, 'ff_dim': 19, 'num_transformer_blocks': 7, 'mlp_units': 116, 'mlp_dropout': 0.20102734350755594, 'dropout': 0.13418907533700367, 'learning_rate': 0.00048536471282492025, 'n_epochs': 93}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_294 (L  (None, 8, 5)                 10        ['input_27[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_147 (  (None, 8, 5)                 21671     ['layer_normalization_294[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_294[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_320 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_147[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_294 (  (None, 8, 5)                 0         ['dropout_320[0][0]',         \n",
      " TFOpLambda)                                                         'input_27[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_295 (L  (None, 8, 5)                 10        ['tf.__operators__.add_294[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_294 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_295[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_321 (Dropout)       (None, 8, 17)                0         ['conv1d_294[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_295 (Conv1D)         (None, 8, 5)                 90        ['dropout_321[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_295 (  (None, 8, 5)                 0         ['conv1d_295[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_294[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_296 (L  (None, 8, 5)                 10        ['tf.__operators__.add_295[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_148 (  (None, 8, 5)                 21671     ['layer_normalization_296[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_296[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_322 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_148[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_296 (  (None, 8, 5)                 0         ['dropout_322[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_295[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_297 (L  (None, 8, 5)                 10        ['tf.__operators__.add_296[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_296 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_297[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_323 (Dropout)       (None, 8, 17)                0         ['conv1d_296[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_297 (Conv1D)         (None, 8, 5)                 90        ['dropout_323[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_297 (  (None, 8, 5)                 0         ['conv1d_297[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_296[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_298 (L  (None, 8, 5)                 10        ['tf.__operators__.add_297[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_149 (  (None, 8, 5)                 21671     ['layer_normalization_298[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_298[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_324 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_149[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_298 (  (None, 8, 5)                 0         ['dropout_324[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_297[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_299 (L  (None, 8, 5)                 10        ['tf.__operators__.add_298[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_298 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_299[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_325 (Dropout)       (None, 8, 17)                0         ['conv1d_298[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_299 (Conv1D)         (None, 8, 5)                 90        ['dropout_325[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_299 (  (None, 8, 5)                 0         ['conv1d_299[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_298[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_300 (L  (None, 8, 5)                 10        ['tf.__operators__.add_299[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_150 (  (None, 8, 5)                 21671     ['layer_normalization_300[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_300[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_326 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_150[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_300 (  (None, 8, 5)                 0         ['dropout_326[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_299[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_301 (L  (None, 8, 5)                 10        ['tf.__operators__.add_300[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_300 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_301[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_327 (Dropout)       (None, 8, 17)                0         ['conv1d_300[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_301 (Conv1D)         (None, 8, 5)                 90        ['dropout_327[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_301 (  (None, 8, 5)                 0         ['conv1d_301[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_300[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_302 (L  (None, 8, 5)                 10        ['tf.__operators__.add_301[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_151 (  (None, 8, 5)                 21671     ['layer_normalization_302[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_302[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_328 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_151[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_302 (  (None, 8, 5)                 0         ['dropout_328[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_301[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_303 (L  (None, 8, 5)                 10        ['tf.__operators__.add_302[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_302 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_303[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_329 (Dropout)       (None, 8, 17)                0         ['conv1d_302[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_303 (Conv1D)         (None, 8, 5)                 90        ['dropout_329[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_303 (  (None, 8, 5)                 0         ['conv1d_303[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_302[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_304 (L  (None, 8, 5)                 10        ['tf.__operators__.add_303[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_152 (  (None, 8, 5)                 21671     ['layer_normalization_304[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_304[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_330 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_152[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_304 (  (None, 8, 5)                 0         ['dropout_330[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_303[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_305 (L  (None, 8, 5)                 10        ['tf.__operators__.add_304[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_304 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_305[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_331 (Dropout)       (None, 8, 17)                0         ['conv1d_304[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_305 (Conv1D)         (None, 8, 5)                 90        ['dropout_331[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_305 (  (None, 8, 5)                 0         ['conv1d_305[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_304[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_305[0][\n",
      " 6 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_78 (Dense)            (None, 128)                  1152      ['global_average_pooling1d_26[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_332 (Dropout)       (None, 128)                  0         ['dense_78[0][0]']            \n",
      "                                                                                                  \n",
      " dense_79 (Dense)            (None, 16)                   2064      ['dropout_332[0][0]']         \n",
      "                                                                                                  \n",
      " dense_80 (Dense)            (None, 8)                    136       ['dense_79[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134650 (525.98 KB)\n",
      "Trainable params: 134650 (525.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/124\n",
      "20/20 [==============================] - 2s 49ms/step - loss: 2324.2634 - val_loss: 5588.5059\n",
      "Epoch 2/124\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1909.6730 - val_loss: 4553.4766\n",
      "Epoch 3/124\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1607.2639 - val_loss: 3765.2881\n",
      "Epoch 4/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1391.8834 - val_loss: 3067.3452\n",
      "Epoch 5/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1190.6719 - val_loss: 2426.2932\n",
      "Epoch 6/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1029.9690 - val_loss: 1817.2681\n",
      "Epoch 7/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 892.8809 - val_loss: 1503.8005\n",
      "Epoch 8/124\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 811.7200 - val_loss: 1204.6710\n",
      "Epoch 9/124\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 743.7838 - val_loss: 1023.4434\n",
      "Epoch 10/124\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 673.6801 - val_loss: 932.2083\n",
      "Epoch 11/124\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 640.0594 - val_loss: 712.7503\n",
      "Epoch 12/124\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 614.8807 - val_loss: 598.7905\n",
      "Epoch 13/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 587.1703 - val_loss: 467.0275\n",
      "Epoch 14/124\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 568.7816 - val_loss: 412.2042\n",
      "Epoch 15/124\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 556.4692 - val_loss: 367.7571\n",
      "Epoch 16/124\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 536.0103 - val_loss: 362.5824\n",
      "Epoch 17/124\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 526.3183 - val_loss: 378.4601\n",
      "Epoch 18/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 521.7263 - val_loss: 338.6978\n",
      "Epoch 19/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 503.1675 - val_loss: 320.5096\n",
      "Epoch 20/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 493.5299 - val_loss: 303.3921\n",
      "Epoch 21/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 477.7545 - val_loss: 386.5101\n",
      "Epoch 22/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 470.5615 - val_loss: 327.5573\n",
      "Epoch 23/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 461.2906 - val_loss: 371.7834\n",
      "Epoch 24/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 443.9480 - val_loss: 288.0146\n",
      "Epoch 25/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 429.8019 - val_loss: 351.7874\n",
      "Epoch 26/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 424.4413 - val_loss: 325.4475\n",
      "Epoch 27/124\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 426.0877 - val_loss: 364.9074\n",
      "Epoch 28/124\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 413.8918 - val_loss: 377.0758\n",
      "Epoch 29/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 405.1964 - val_loss: 352.6979\n",
      "Epoch 30/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 393.3580 - val_loss: 301.8413\n",
      "Epoch 31/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 393.6285 - val_loss: 384.0583\n",
      "Epoch 32/124\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 387.5544 - val_loss: 383.3476\n",
      "Epoch 33/124\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 380.7311 - val_loss: 343.1654\n",
      "Epoch 34/124\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 369.0817 - val_loss: 335.6531\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1182.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:06:49,388] Trial 26 finished with value: 1182.7470703125 and parameters: {'head_size': 157, 'num_heads': 6, 'ff_dim': 17, 'num_transformer_blocks': 6, 'mlp_units': 128, 'mlp_dropout': 0.17905192492074964, 'dropout': 0.1642460921174124, 'learning_rate': 0.00019478325928432987, 'n_epochs': 124}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_306 (L  (None, 8, 5)                 10        ['input_28[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_153 (  (None, 8, 5)                 23465     ['layer_normalization_306[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_306[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_333 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_153[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_306 (  (None, 8, 5)                 0         ['dropout_333[0][0]',         \n",
      " TFOpLambda)                                                         'input_28[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_307 (L  (None, 8, 5)                 10        ['tf.__operators__.add_306[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_306 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_307[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_334 (Dropout)       (None, 8, 11)                0         ['conv1d_306[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_307 (Conv1D)         (None, 8, 5)                 60        ['dropout_334[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_307 (  (None, 8, 5)                 0         ['conv1d_307[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_306[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_308 (L  (None, 8, 5)                 10        ['tf.__operators__.add_307[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_154 (  (None, 8, 5)                 23465     ['layer_normalization_308[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_308[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_335 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_154[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_308 (  (None, 8, 5)                 0         ['dropout_335[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_307[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_309 (L  (None, 8, 5)                 10        ['tf.__operators__.add_308[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_308 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_309[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_336 (Dropout)       (None, 8, 11)                0         ['conv1d_308[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_309 (Conv1D)         (None, 8, 5)                 60        ['dropout_336[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_309 (  (None, 8, 5)                 0         ['conv1d_309[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_308[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_310 (L  (None, 8, 5)                 10        ['tf.__operators__.add_309[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_155 (  (None, 8, 5)                 23465     ['layer_normalization_310[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_310[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_337 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_155[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_310 (  (None, 8, 5)                 0         ['dropout_337[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_309[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_311 (L  (None, 8, 5)                 10        ['tf.__operators__.add_310[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_310 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_311[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_338 (Dropout)       (None, 8, 11)                0         ['conv1d_310[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_311 (Conv1D)         (None, 8, 5)                 60        ['dropout_338[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_311 (  (None, 8, 5)                 0         ['conv1d_311[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_310[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_312 (L  (None, 8, 5)                 10        ['tf.__operators__.add_311[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_156 (  (None, 8, 5)                 23465     ['layer_normalization_312[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_312[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_339 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_156[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_312 (  (None, 8, 5)                 0         ['dropout_339[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_311[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_313 (L  (None, 8, 5)                 10        ['tf.__operators__.add_312[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_312 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_313[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_340 (Dropout)       (None, 8, 11)                0         ['conv1d_312[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_313 (Conv1D)         (None, 8, 5)                 60        ['dropout_340[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_313 (  (None, 8, 5)                 0         ['conv1d_313[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_312[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_314 (L  (None, 8, 5)                 10        ['tf.__operators__.add_313[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_157 (  (None, 8, 5)                 23465     ['layer_normalization_314[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_314[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_341 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_157[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_314 (  (None, 8, 5)                 0         ['dropout_341[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_313[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_315 (L  (None, 8, 5)                 10        ['tf.__operators__.add_314[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_314 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_315[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_342 (Dropout)       (None, 8, 11)                0         ['conv1d_314[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_315 (Conv1D)         (None, 8, 5)                 60        ['dropout_342[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_315 (  (None, 8, 5)                 0         ['conv1d_315[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_314[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_316 (L  (None, 8, 5)                 10        ['tf.__operators__.add_315[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_158 (  (None, 8, 5)                 23465     ['layer_normalization_316[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_316[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_343 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_158[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_316 (  (None, 8, 5)                 0         ['dropout_343[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_315[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_317 (L  (None, 8, 5)                 10        ['tf.__operators__.add_316[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_316 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_317[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_344 (Dropout)       (None, 8, 11)                0         ['conv1d_316[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_317 (Conv1D)         (None, 8, 5)                 60        ['dropout_344[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_317 (  (None, 8, 5)                 0         ['conv1d_317[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_316[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_318 (L  (None, 8, 5)                 10        ['tf.__operators__.add_317[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_159 (  (None, 8, 5)                 23465     ['layer_normalization_318[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_318[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_345 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_159[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_318 (  (None, 8, 5)                 0         ['dropout_345[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_317[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_319 (L  (None, 8, 5)                 10        ['tf.__operators__.add_318[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_318 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_319[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_346 (Dropout)       (None, 8, 11)                0         ['conv1d_318[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_319 (Conv1D)         (None, 8, 5)                 60        ['dropout_346[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_319 (  (None, 8, 5)                 0         ['conv1d_319[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_318[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_320 (L  (None, 8, 5)                 10        ['tf.__operators__.add_319[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_160 (  (None, 8, 5)                 23465     ['layer_normalization_320[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_320[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_347 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_160[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_320 (  (None, 8, 5)                 0         ['dropout_347[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_319[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_321 (L  (None, 8, 5)                 10        ['tf.__operators__.add_320[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_320 (Conv1D)         (None, 8, 11)                66        ['layer_normalization_321[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_348 (Dropout)       (None, 8, 11)                0         ['conv1d_320[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_321 (Conv1D)         (None, 8, 5)                 60        ['dropout_348[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_321 (  (None, 8, 5)                 0         ['conv1d_321[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_320[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_321[0][\n",
      " 7 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_81 (Dense)            (None, 94)                   846       ['global_average_pooling1d_27[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_349 (Dropout)       (None, 94)                   0         ['dense_81[0][0]']            \n",
      "                                                                                                  \n",
      " dense_82 (Dense)            (None, 16)                   1520      ['dropout_349[0][0]']         \n",
      "                                                                                                  \n",
      " dense_83 (Dense)            (None, 8)                    136       ['dense_82[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 191390 (747.62 KB)\n",
      "Trainable params: 191390 (747.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/102\n",
      "20/20 [==============================] - 3s 60ms/step - loss: 2629.2732 - val_loss: 4469.4634\n",
      "Epoch 2/102\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1748.6498 - val_loss: 2337.8254\n",
      "Epoch 3/102\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1246.0916 - val_loss: 1558.6315\n",
      "Epoch 4/102\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 920.2053 - val_loss: 1023.3741\n",
      "Epoch 5/102\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 770.2415 - val_loss: 669.9064\n",
      "Epoch 6/102\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 697.4141 - val_loss: 720.3474\n",
      "Epoch 7/102\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 652.7872 - val_loss: 581.6628\n",
      "Epoch 8/102\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 590.0419 - val_loss: 530.6378\n",
      "Epoch 9/102\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 565.0696 - val_loss: 601.3077\n",
      "Epoch 10/102\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 526.9383 - val_loss: 634.9138\n",
      "Epoch 11/102\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 496.3089 - val_loss: 400.8281\n",
      "Epoch 12/102\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 469.9433 - val_loss: 454.0465\n",
      "Epoch 13/102\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 444.8308 - val_loss: 439.2753\n",
      "Epoch 14/102\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 438.6790 - val_loss: 460.9220\n",
      "Epoch 15/102\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 402.9270 - val_loss: 390.2161\n",
      "Epoch 16/102\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 385.6490 - val_loss: 517.3777\n",
      "Epoch 17/102\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 370.0668 - val_loss: 513.5255\n",
      "Epoch 18/102\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 356.1642 - val_loss: 510.8589\n",
      "Epoch 19/102\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 344.1778 - val_loss: 473.3363\n",
      "Epoch 20/102\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 331.5409 - val_loss: 520.2349\n",
      "Epoch 21/102\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 326.7805 - val_loss: 515.2323\n",
      "Epoch 22/102\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 320.1232 - val_loss: 460.7971\n",
      "Epoch 23/102\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 301.9991 - val_loss: 383.1859\n",
      "Epoch 24/102\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 292.3246 - val_loss: 456.2886\n",
      "Epoch 25/102\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 289.8979 - val_loss: 426.6434\n",
      "Epoch 26/102\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 279.8065 - val_loss: 430.9126\n",
      "Epoch 27/102\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 271.7206 - val_loss: 461.7774\n",
      "Epoch 28/102\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 268.0518 - val_loss: 500.7419\n",
      "Epoch 29/102\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 267.2516 - val_loss: 447.2533\n",
      "Epoch 30/102\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 254.3201 - val_loss: 449.6226\n",
      "Epoch 31/102\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 245.5763 - val_loss: 469.3329\n",
      "Epoch 32/102\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 240.7188 - val_loss: 403.6029\n",
      "Epoch 33/102\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 234.8783 - val_loss: 459.5189\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1295.3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:07:28,073] Trial 27 finished with value: 1295.3319091796875 and parameters: {'head_size': 204, 'num_heads': 5, 'ff_dim': 11, 'num_transformer_blocks': 8, 'mlp_units': 94, 'mlp_dropout': 0.21892212489240162, 'dropout': 0.12175778651692828, 'learning_rate': 0.0005608570343975424, 'n_epochs': 102}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_322 (L  (None, 8, 5)                 10        ['input_29[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_161 (  (None, 8, 5)                 18842     ['layer_normalization_322[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_322[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_350 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_161[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_322 (  (None, 8, 5)                 0         ['dropout_350[0][0]',         \n",
      " TFOpLambda)                                                         'input_29[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_323 (L  (None, 8, 5)                 10        ['tf.__operators__.add_322[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_322 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_323[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_351 (Dropout)       (None, 8, 12)                0         ['conv1d_322[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_323 (Conv1D)         (None, 8, 5)                 65        ['dropout_351[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_323 (  (None, 8, 5)                 0         ['conv1d_323[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_322[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_324 (L  (None, 8, 5)                 10        ['tf.__operators__.add_323[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_162 (  (None, 8, 5)                 18842     ['layer_normalization_324[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_324[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_352 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_162[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_324 (  (None, 8, 5)                 0         ['dropout_352[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_323[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_325 (L  (None, 8, 5)                 10        ['tf.__operators__.add_324[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_324 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_325[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_353 (Dropout)       (None, 8, 12)                0         ['conv1d_324[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_325 (Conv1D)         (None, 8, 5)                 65        ['dropout_353[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_325 (  (None, 8, 5)                 0         ['conv1d_325[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_324[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_326 (L  (None, 8, 5)                 10        ['tf.__operators__.add_325[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_163 (  (None, 8, 5)                 18842     ['layer_normalization_326[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_326[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_354 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_163[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_326 (  (None, 8, 5)                 0         ['dropout_354[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_325[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_327 (L  (None, 8, 5)                 10        ['tf.__operators__.add_326[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_326 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_327[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_355 (Dropout)       (None, 8, 12)                0         ['conv1d_326[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_327 (Conv1D)         (None, 8, 5)                 65        ['dropout_355[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_327 (  (None, 8, 5)                 0         ['conv1d_327[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_326[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_328 (L  (None, 8, 5)                 10        ['tf.__operators__.add_327[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_164 (  (None, 8, 5)                 18842     ['layer_normalization_328[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_328[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_356 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_164[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_328 (  (None, 8, 5)                 0         ['dropout_356[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_327[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_329 (L  (None, 8, 5)                 10        ['tf.__operators__.add_328[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_328 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_329[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_357 (Dropout)       (None, 8, 12)                0         ['conv1d_328[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_329 (Conv1D)         (None, 8, 5)                 65        ['dropout_357[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_329 (  (None, 8, 5)                 0         ['conv1d_329[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_328[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_330 (L  (None, 8, 5)                 10        ['tf.__operators__.add_329[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_165 (  (None, 8, 5)                 18842     ['layer_normalization_330[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_330[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_358 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_165[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_330 (  (None, 8, 5)                 0         ['dropout_358[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_329[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_331 (L  (None, 8, 5)                 10        ['tf.__operators__.add_330[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_330 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_331[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_359 (Dropout)       (None, 8, 12)                0         ['conv1d_330[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_331 (Conv1D)         (None, 8, 5)                 65        ['dropout_359[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_331 (  (None, 8, 5)                 0         ['conv1d_331[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_330[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_332 (L  (None, 8, 5)                 10        ['tf.__operators__.add_331[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_166 (  (None, 8, 5)                 18842     ['layer_normalization_332[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_332[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_360 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_166[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_332 (  (None, 8, 5)                 0         ['dropout_360[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_331[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_333 (L  (None, 8, 5)                 10        ['tf.__operators__.add_332[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_332 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_333[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_361 (Dropout)       (None, 8, 12)                0         ['conv1d_332[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_333 (Conv1D)         (None, 8, 5)                 65        ['dropout_361[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_333 (  (None, 8, 5)                 0         ['conv1d_333[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_332[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_334 (L  (None, 8, 5)                 10        ['tf.__operators__.add_333[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_167 (  (None, 8, 5)                 18842     ['layer_normalization_334[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_334[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_362 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_167[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_334 (  (None, 8, 5)                 0         ['dropout_362[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_333[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_335 (L  (None, 8, 5)                 10        ['tf.__operators__.add_334[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_334 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_335[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_363 (Dropout)       (None, 8, 12)                0         ['conv1d_334[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_335 (Conv1D)         (None, 8, 5)                 65        ['dropout_363[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_335 (  (None, 8, 5)                 0         ['conv1d_335[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_334[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_335[0][\n",
      " 8 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_84 (Dense)            (None, 83)                   747       ['global_average_pooling1d_28[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_364 (Dropout)       (None, 83)                   0         ['dense_84[0][0]']            \n",
      "                                                                                                  \n",
      " dense_85 (Dense)            (None, 16)                   1344      ['dropout_364[0][0]']         \n",
      "                                                                                                  \n",
      " dense_86 (Dense)            (None, 8)                    136       ['dense_85[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135220 (528.20 KB)\n",
      "Trainable params: 135220 (528.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/85\n",
      "20/20 [==============================] - 3s 54ms/step - loss: 2659.4414 - val_loss: 6171.0762\n",
      "Epoch 2/85\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 2170.2026 - val_loss: 5130.2148\n",
      "Epoch 3/85\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1732.2605 - val_loss: 3969.1313\n",
      "Epoch 4/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 1294.3556 - val_loss: 3236.6545\n",
      "Epoch 5/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 975.7349 - val_loss: 2677.7791\n",
      "Epoch 6/85\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 780.8655 - val_loss: 1925.3131\n",
      "Epoch 7/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 679.2618 - val_loss: 1323.5862\n",
      "Epoch 8/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 608.3306 - val_loss: 934.0854\n",
      "Epoch 9/85\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 576.6108 - val_loss: 758.8853\n",
      "Epoch 10/85\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 547.8557 - val_loss: 611.6118\n",
      "Epoch 11/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 526.0838 - val_loss: 625.4582\n",
      "Epoch 12/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 495.2966 - val_loss: 578.7651\n",
      "Epoch 13/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 481.5683 - val_loss: 589.2214\n",
      "Epoch 14/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 476.8076 - val_loss: 412.4705\n",
      "Epoch 15/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 465.9642 - val_loss: 393.7440\n",
      "Epoch 16/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 452.5782 - val_loss: 511.7545\n",
      "Epoch 17/85\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 442.5223 - val_loss: 449.6428\n",
      "Epoch 18/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 434.4843 - val_loss: 338.1622\n",
      "Epoch 19/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 424.4580 - val_loss: 523.5394\n",
      "Epoch 20/85\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 425.1577 - val_loss: 477.3433\n",
      "Epoch 21/85\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 411.5433 - val_loss: 479.5186\n",
      "Epoch 22/85\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 390.5700 - val_loss: 409.9089\n",
      "Epoch 23/85\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 386.0737 - val_loss: 453.4013\n",
      "Epoch 24/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 379.5089 - val_loss: 429.4989\n",
      "Epoch 25/85\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 371.0326 - val_loss: 497.9984\n",
      "Epoch 26/85\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 363.8861 - val_loss: 347.6854\n",
      "Epoch 27/85\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 357.2464 - val_loss: 462.7508\n",
      "Epoch 28/85\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 346.7256 - val_loss: 382.4760\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1120.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:07:54,425] Trial 28 finished with value: 1120.0992431640625 and parameters: {'head_size': 117, 'num_heads': 7, 'ff_dim': 12, 'num_transformer_blocks': 7, 'mlp_units': 83, 'mlp_dropout': 0.12617391874683118, 'dropout': 0.14577132955624694, 'learning_rate': 0.0002697814686979456, 'n_epochs': 85}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_336 (L  (None, 8, 5)                 10        ['input_30[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_168 (  (None, 8, 5)                 33493     ['layer_normalization_336[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_336[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_365 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_168[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_336 (  (None, 8, 5)                 0         ['dropout_365[0][0]',         \n",
      " TFOpLambda)                                                         'input_30[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_337 (L  (None, 8, 5)                 10        ['tf.__operators__.add_336[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_336 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_337[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_366 (Dropout)       (None, 8, 15)                0         ['conv1d_336[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_337 (Conv1D)         (None, 8, 5)                 80        ['dropout_366[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_337 (  (None, 8, 5)                 0         ['conv1d_337[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_336[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_338 (L  (None, 8, 5)                 10        ['tf.__operators__.add_337[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_169 (  (None, 8, 5)                 33493     ['layer_normalization_338[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_338[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_367 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_169[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_338 (  (None, 8, 5)                 0         ['dropout_367[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_337[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_339 (L  (None, 8, 5)                 10        ['tf.__operators__.add_338[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_338 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_339[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_368 (Dropout)       (None, 8, 15)                0         ['conv1d_338[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_339 (Conv1D)         (None, 8, 5)                 80        ['dropout_368[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_339 (  (None, 8, 5)                 0         ['conv1d_339[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_338[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_340 (L  (None, 8, 5)                 10        ['tf.__operators__.add_339[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_170 (  (None, 8, 5)                 33493     ['layer_normalization_340[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_340[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_369 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_170[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_340 (  (None, 8, 5)                 0         ['dropout_369[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_339[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_341 (L  (None, 8, 5)                 10        ['tf.__operators__.add_340[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_340 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_341[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_370 (Dropout)       (None, 8, 15)                0         ['conv1d_340[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_341 (Conv1D)         (None, 8, 5)                 80        ['dropout_370[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_341 (  (None, 8, 5)                 0         ['conv1d_341[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_340[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_342 (L  (None, 8, 5)                 10        ['tf.__operators__.add_341[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_171 (  (None, 8, 5)                 33493     ['layer_normalization_342[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_342[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_371 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_171[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_342 (  (None, 8, 5)                 0         ['dropout_371[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_341[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_343 (L  (None, 8, 5)                 10        ['tf.__operators__.add_342[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_342 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_343[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_372 (Dropout)       (None, 8, 15)                0         ['conv1d_342[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_343 (Conv1D)         (None, 8, 5)                 80        ['dropout_372[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_343 (  (None, 8, 5)                 0         ['conv1d_343[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_342[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_344 (L  (None, 8, 5)                 10        ['tf.__operators__.add_343[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_172 (  (None, 8, 5)                 33493     ['layer_normalization_344[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_344[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_373 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_172[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_344 (  (None, 8, 5)                 0         ['dropout_373[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_343[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_345 (L  (None, 8, 5)                 10        ['tf.__operators__.add_344[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_344 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_345[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_374 (Dropout)       (None, 8, 15)                0         ['conv1d_344[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_345 (Conv1D)         (None, 8, 5)                 80        ['dropout_374[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_345 (  (None, 8, 5)                 0         ['conv1d_345[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_344[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_346 (L  (None, 8, 5)                 10        ['tf.__operators__.add_345[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_173 (  (None, 8, 5)                 33493     ['layer_normalization_346[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_346[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_375 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_173[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_346 (  (None, 8, 5)                 0         ['dropout_375[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_345[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_347 (L  (None, 8, 5)                 10        ['tf.__operators__.add_346[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_346 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_347[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_376 (Dropout)       (None, 8, 15)                0         ['conv1d_346[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_347 (Conv1D)         (None, 8, 5)                 80        ['dropout_376[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_347 (  (None, 8, 5)                 0         ['conv1d_347[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_346[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 8)                    0         ['tf.__operators__.add_347[0][\n",
      " 9 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_87 (Dense)            (None, 123)                  1107      ['global_average_pooling1d_29[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_377 (Dropout)       (None, 123)                  0         ['dense_87[0][0]']            \n",
      "                                                                                                  \n",
      " dense_88 (Dense)            (None, 16)                   1984      ['dropout_377[0][0]']         \n",
      "                                                                                                  \n",
      " dense_89 (Dense)            (None, 8)                    136       ['dense_88[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 205325 (802.05 KB)\n",
      "Trainable params: 205325 (802.05 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/126\n",
      "20/20 [==============================] - 3s 57ms/step - loss: 2085.5039 - val_loss: 5896.9805\n",
      "Epoch 2/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 1900.6573 - val_loss: 5457.5874\n",
      "Epoch 3/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1745.8772 - val_loss: 5117.9531\n",
      "Epoch 4/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 1653.0807 - val_loss: 4800.4409\n",
      "Epoch 5/126\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1510.8894 - val_loss: 4504.6836\n",
      "Epoch 6/126\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1416.9302 - val_loss: 4216.4893\n",
      "Epoch 7/126\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1300.2596 - val_loss: 3933.1716\n",
      "Epoch 8/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1215.1257 - val_loss: 3662.5828\n",
      "Epoch 9/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1143.6610 - val_loss: 3417.8855\n",
      "Epoch 10/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1088.0818 - val_loss: 3213.7964\n",
      "Epoch 11/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1035.2361 - val_loss: 3067.4277\n",
      "Epoch 12/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 994.0500 - val_loss: 2923.0278\n",
      "Epoch 13/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 953.5802 - val_loss: 2796.2051\n",
      "Epoch 14/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 935.9153 - val_loss: 2704.2783\n",
      "Epoch 15/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 924.3568 - val_loss: 2593.7146\n",
      "Epoch 16/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 888.7494 - val_loss: 2470.4136\n",
      "Epoch 17/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 879.5856 - val_loss: 2387.7375\n",
      "Epoch 18/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 855.7184 - val_loss: 2314.6035\n",
      "Epoch 19/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 835.3490 - val_loss: 2218.2676\n",
      "Epoch 20/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 816.3595 - val_loss: 2152.3840\n",
      "Epoch 21/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 802.7855 - val_loss: 2065.0535\n",
      "Epoch 22/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 793.4143 - val_loss: 1972.0048\n",
      "Epoch 23/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 781.6353 - val_loss: 1916.5522\n",
      "Epoch 24/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 768.2138 - val_loss: 1815.7838\n",
      "Epoch 25/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 749.4587 - val_loss: 1730.0977\n",
      "Epoch 26/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 739.3959 - val_loss: 1670.1705\n",
      "Epoch 27/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 742.1470 - val_loss: 1632.3389\n",
      "Epoch 28/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 712.5936 - val_loss: 1560.1776\n",
      "Epoch 29/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 705.2820 - val_loss: 1499.4771\n",
      "Epoch 30/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 699.5482 - val_loss: 1450.7758\n",
      "Epoch 31/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 702.8064 - val_loss: 1409.4860\n",
      "Epoch 32/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 688.5845 - val_loss: 1334.3455\n",
      "Epoch 33/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 693.6265 - val_loss: 1282.2059\n",
      "Epoch 34/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 673.1029 - val_loss: 1237.5881\n",
      "Epoch 35/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 672.7024 - val_loss: 1208.3892\n",
      "Epoch 36/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 664.8708 - val_loss: 1171.9291\n",
      "Epoch 37/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 651.0312 - val_loss: 1159.3835\n",
      "Epoch 38/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 654.0837 - val_loss: 1143.5967\n",
      "Epoch 39/126\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 652.6165 - val_loss: 1084.8409\n",
      "Epoch 40/126\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 630.6315 - val_loss: 1008.4097\n",
      "Epoch 41/126\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 642.7001 - val_loss: 993.4598\n",
      "Epoch 42/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 619.4619 - val_loss: 986.2003\n",
      "Epoch 43/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 612.9502 - val_loss: 939.1790\n",
      "Epoch 44/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 618.2340 - val_loss: 937.0630\n",
      "Epoch 45/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 609.8362 - val_loss: 895.2099\n",
      "Epoch 46/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 606.6771 - val_loss: 841.9902\n",
      "Epoch 47/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 593.9707 - val_loss: 836.3083\n",
      "Epoch 48/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 589.9827 - val_loss: 805.0587\n",
      "Epoch 49/126\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 589.3911 - val_loss: 762.8843\n",
      "Epoch 50/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 583.9042 - val_loss: 753.2750\n",
      "Epoch 51/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 580.2064 - val_loss: 757.9770\n",
      "Epoch 52/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 580.7970 - val_loss: 733.4778\n",
      "Epoch 53/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 562.6807 - val_loss: 739.2281\n",
      "Epoch 54/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 571.3331 - val_loss: 722.9708\n",
      "Epoch 55/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 559.6310 - val_loss: 685.2661\n",
      "Epoch 56/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 549.9684 - val_loss: 672.9538\n",
      "Epoch 57/126\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 550.4295 - val_loss: 663.1204\n",
      "Epoch 58/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 544.3344 - val_loss: 664.6945\n",
      "Epoch 59/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 552.2350 - val_loss: 617.9097\n",
      "Epoch 60/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 544.7971 - val_loss: 626.0334\n",
      "Epoch 61/126\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 536.7390 - val_loss: 624.1968\n",
      "Epoch 62/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 533.9115 - val_loss: 612.0095\n",
      "Epoch 63/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 517.3903 - val_loss: 568.7834\n",
      "Epoch 64/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 519.1754 - val_loss: 560.1064\n",
      "Epoch 65/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 530.8240 - val_loss: 570.8174\n",
      "Epoch 66/126\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 529.3299 - val_loss: 537.7148\n",
      "Epoch 67/126\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 515.5311 - val_loss: 502.8602\n",
      "Epoch 68/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 512.3744 - val_loss: 482.1566\n",
      "Epoch 69/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 505.8060 - val_loss: 483.7719\n",
      "Epoch 70/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 498.8748 - val_loss: 522.9765\n",
      "Epoch 71/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 503.3560 - val_loss: 523.3989\n",
      "Epoch 72/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 495.9830 - val_loss: 500.0392\n",
      "Epoch 73/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 497.8093 - val_loss: 487.0472\n",
      "Epoch 74/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 498.6944 - val_loss: 484.9797\n",
      "Epoch 75/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 493.2727 - val_loss: 477.2571\n",
      "Epoch 76/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 491.6025 - val_loss: 451.5612\n",
      "Epoch 77/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 485.2723 - val_loss: 455.6866\n",
      "Epoch 78/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 484.3586 - val_loss: 459.8393\n",
      "Epoch 79/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 477.9368 - val_loss: 456.6014\n",
      "Epoch 80/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 476.8049 - val_loss: 459.9390\n",
      "Epoch 81/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 470.7410 - val_loss: 461.6531\n",
      "Epoch 82/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 466.1048 - val_loss: 453.3743\n",
      "Epoch 83/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 470.6606 - val_loss: 444.4652\n",
      "Epoch 84/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 463.8041 - val_loss: 429.7672\n",
      "Epoch 85/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 464.5537 - val_loss: 443.2826\n",
      "Epoch 86/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 463.0781 - val_loss: 433.2275\n",
      "Epoch 87/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 462.5565 - val_loss: 424.5213\n",
      "Epoch 88/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 455.1240 - val_loss: 435.8869\n",
      "Epoch 89/126\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 455.1560 - val_loss: 447.2933\n",
      "Epoch 90/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 453.4916 - val_loss: 453.8308\n",
      "Epoch 91/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 442.6790 - val_loss: 457.4371\n",
      "Epoch 92/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 445.1395 - val_loss: 468.9649\n",
      "Epoch 93/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 445.1855 - val_loss: 458.0803\n",
      "Epoch 94/126\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 440.9192 - val_loss: 447.9898\n",
      "Epoch 95/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 429.0395 - val_loss: 419.7948\n",
      "Epoch 96/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 435.9100 - val_loss: 430.4120\n",
      "Epoch 97/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 424.8706 - val_loss: 417.8527\n",
      "Epoch 98/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 428.1358 - val_loss: 413.7784\n",
      "Epoch 99/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 433.2603 - val_loss: 440.0014\n",
      "Epoch 100/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 423.6873 - val_loss: 424.9742\n",
      "Epoch 101/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 426.9837 - val_loss: 414.4181\n",
      "Epoch 102/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 419.0240 - val_loss: 429.8165\n",
      "Epoch 103/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 414.8320 - val_loss: 416.8392\n",
      "Epoch 104/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 419.2132 - val_loss: 422.2346\n",
      "Epoch 105/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 410.7778 - val_loss: 426.0017\n",
      "Epoch 106/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 408.6195 - val_loss: 418.8554\n",
      "Epoch 107/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 415.2556 - val_loss: 419.3301\n",
      "Epoch 108/126\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 391.8242 - val_loss: 410.7477\n",
      "Epoch 109/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 400.8083 - val_loss: 403.1795\n",
      "Epoch 110/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 403.0577 - val_loss: 410.1381\n",
      "Epoch 111/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 410.0115 - val_loss: 399.3138\n",
      "Epoch 112/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 390.4371 - val_loss: 406.2332\n",
      "Epoch 113/126\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 396.1791 - val_loss: 411.0301\n",
      "Epoch 114/126\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 389.3862 - val_loss: 407.3539\n",
      "Epoch 115/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 392.4113 - val_loss: 403.7691\n",
      "Epoch 116/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 383.6669 - val_loss: 392.2188\n",
      "Epoch 117/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 391.0758 - val_loss: 396.6720\n",
      "Epoch 118/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 393.0625 - val_loss: 424.2960\n",
      "Epoch 119/126\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 392.6829 - val_loss: 413.1255\n",
      "Epoch 120/126\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 378.9884 - val_loss: 397.7166\n",
      "Epoch 121/126\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 381.3874 - val_loss: 390.9919\n",
      "Epoch 122/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 374.2682 - val_loss: 408.2020\n",
      "Epoch 123/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 374.8547 - val_loss: 412.4689\n",
      "Epoch 124/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 377.5989 - val_loss: 397.0783\n",
      "Epoch 125/126\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 376.9909 - val_loss: 410.2790\n",
      "Epoch 126/126\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 364.1101 - val_loss: 425.8293\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1440.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:10:02,411] Trial 29 finished with value: 1440.08642578125 and parameters: {'head_size': 182, 'num_heads': 8, 'ff_dim': 15, 'num_transformer_blocks': 6, 'mlp_units': 123, 'mlp_dropout': 0.16093802613244682, 'dropout': 0.23153752070726416, 'learning_rate': 4.388997929775955e-05, 'n_epochs': 126}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_348 (L  (None, 8, 5)                 10        ['input_31[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_174 (  (None, 8, 5)                 16266     ['layer_normalization_348[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_348[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_378 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_174[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_348 (  (None, 8, 5)                 0         ['dropout_378[0][0]',         \n",
      " TFOpLambda)                                                         'input_31[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_349 (L  (None, 8, 5)                 10        ['tf.__operators__.add_348[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_348 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_349[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_379 (Dropout)       (None, 8, 10)                0         ['conv1d_348[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_349 (Conv1D)         (None, 8, 5)                 55        ['dropout_379[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_349 (  (None, 8, 5)                 0         ['conv1d_349[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_348[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_350 (L  (None, 8, 5)                 10        ['tf.__operators__.add_349[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_175 (  (None, 8, 5)                 16266     ['layer_normalization_350[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_350[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_380 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_175[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_350 (  (None, 8, 5)                 0         ['dropout_380[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_349[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_351 (L  (None, 8, 5)                 10        ['tf.__operators__.add_350[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_350 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_351[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_381 (Dropout)       (None, 8, 10)                0         ['conv1d_350[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_351 (Conv1D)         (None, 8, 5)                 55        ['dropout_381[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_351 (  (None, 8, 5)                 0         ['conv1d_351[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_350[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_352 (L  (None, 8, 5)                 10        ['tf.__operators__.add_351[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_176 (  (None, 8, 5)                 16266     ['layer_normalization_352[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_352[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_382 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_176[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_352 (  (None, 8, 5)                 0         ['dropout_382[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_351[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_353 (L  (None, 8, 5)                 10        ['tf.__operators__.add_352[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_352 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_353[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_383 (Dropout)       (None, 8, 10)                0         ['conv1d_352[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_353 (Conv1D)         (None, 8, 5)                 55        ['dropout_383[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_353 (  (None, 8, 5)                 0         ['conv1d_353[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_352[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_354 (L  (None, 8, 5)                 10        ['tf.__operators__.add_353[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_177 (  (None, 8, 5)                 16266     ['layer_normalization_354[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_354[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_384 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_177[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_354 (  (None, 8, 5)                 0         ['dropout_384[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_353[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_355 (L  (None, 8, 5)                 10        ['tf.__operators__.add_354[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_354 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_355[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_385 (Dropout)       (None, 8, 10)                0         ['conv1d_354[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_355 (Conv1D)         (None, 8, 5)                 55        ['dropout_385[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_355 (  (None, 8, 5)                 0         ['conv1d_355[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_354[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_356 (L  (None, 8, 5)                 10        ['tf.__operators__.add_355[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_178 (  (None, 8, 5)                 16266     ['layer_normalization_356[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_356[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_386 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_178[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_356 (  (None, 8, 5)                 0         ['dropout_386[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_355[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_357 (L  (None, 8, 5)                 10        ['tf.__operators__.add_356[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_356 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_357[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_387 (Dropout)       (None, 8, 10)                0         ['conv1d_356[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_357 (Conv1D)         (None, 8, 5)                 55        ['dropout_387[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_357 (  (None, 8, 5)                 0         ['conv1d_357[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_356[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_358 (L  (None, 8, 5)                 10        ['tf.__operators__.add_357[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_179 (  (None, 8, 5)                 16266     ['layer_normalization_358[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_358[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_388 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_179[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_358 (  (None, 8, 5)                 0         ['dropout_388[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_357[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_359 (L  (None, 8, 5)                 10        ['tf.__operators__.add_358[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_358 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_359[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_389 (Dropout)       (None, 8, 10)                0         ['conv1d_358[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_359 (Conv1D)         (None, 8, 5)                 55        ['dropout_389[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_359 (  (None, 8, 5)                 0         ['conv1d_359[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_358[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_359[0][\n",
      " 0 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_90 (Dense)            (None, 128)                  1152      ['global_average_pooling1d_30[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_390 (Dropout)       (None, 128)                  0         ['dense_90[0][0]']            \n",
      "                                                                                                  \n",
      " dense_91 (Dense)            (None, 16)                   2064      ['dropout_390[0][0]']         \n",
      "                                                                                                  \n",
      " dense_92 (Dense)            (None, 8)                    136       ['dense_91[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 101758 (397.49 KB)\n",
      "Trainable params: 101758 (397.49 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/140\n",
      "20/20 [==============================] - 2s 39ms/step - loss: 2065.0005 - val_loss: 2951.2285\n",
      "Epoch 2/140\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1185.8927 - val_loss: 872.4575\n",
      "Epoch 3/140\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 845.2081 - val_loss: 717.9415\n",
      "Epoch 4/140\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 740.9438 - val_loss: 953.0880\n",
      "Epoch 5/140\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 649.4509 - val_loss: 1078.7682\n",
      "Epoch 6/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 586.0995 - val_loss: 1185.7646\n",
      "Epoch 7/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 544.7407 - val_loss: 984.4047\n",
      "Epoch 8/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 472.8249 - val_loss: 749.1749\n",
      "Epoch 9/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 433.0052 - val_loss: 655.0338\n",
      "Epoch 10/140\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 408.1201 - val_loss: 503.5672\n",
      "Epoch 11/140\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 397.0412 - val_loss: 476.9781\n",
      "Epoch 12/140\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 374.8990 - val_loss: 449.2959\n",
      "Epoch 13/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 352.3948 - val_loss: 476.1609\n",
      "Epoch 14/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 343.1848 - val_loss: 474.1465\n",
      "Epoch 15/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 325.3488 - val_loss: 451.6369\n",
      "Epoch 16/140\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 316.8590 - val_loss: 528.8927\n",
      "Epoch 17/140\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 311.0961 - val_loss: 454.0060\n",
      "Epoch 18/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 297.5991 - val_loss: 434.3284\n",
      "Epoch 19/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 292.5533 - val_loss: 456.7836\n",
      "Epoch 20/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 279.3172 - val_loss: 473.6971\n",
      "Epoch 21/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 266.2990 - val_loss: 451.8216\n",
      "Epoch 22/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 254.3956 - val_loss: 447.6770\n",
      "Epoch 23/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 257.1605 - val_loss: 453.7155\n",
      "Epoch 24/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 237.6882 - val_loss: 383.3369\n",
      "Epoch 25/140\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 233.9647 - val_loss: 441.6979\n",
      "Epoch 26/140\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 229.1993 - val_loss: 443.2656\n",
      "Epoch 27/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 225.7102 - val_loss: 391.3411\n",
      "Epoch 28/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 219.3968 - val_loss: 409.7901\n",
      "Epoch 29/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 214.8867 - val_loss: 381.6254\n",
      "Epoch 30/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 207.5302 - val_loss: 423.7697\n",
      "Epoch 31/140\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 206.3466 - val_loss: 375.5396\n",
      "Epoch 32/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 202.4238 - val_loss: 412.0473\n",
      "Epoch 33/140\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 196.5491 - val_loss: 360.3664\n",
      "Epoch 34/140\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 191.3387 - val_loss: 415.5242\n",
      "Epoch 35/140\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 189.8452 - val_loss: 412.5902\n",
      "Epoch 36/140\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 192.0473 - val_loss: 395.0098\n",
      "Epoch 37/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 186.4803 - val_loss: 343.5731\n",
      "Epoch 38/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 181.9426 - val_loss: 405.6572\n",
      "Epoch 39/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 178.4833 - val_loss: 406.1533\n",
      "Epoch 40/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 176.6591 - val_loss: 379.2351\n",
      "Epoch 41/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 172.5257 - val_loss: 356.1068\n",
      "Epoch 42/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 176.9442 - val_loss: 362.6240\n",
      "Epoch 43/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 171.2667 - val_loss: 353.3696\n",
      "Epoch 44/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 168.0748 - val_loss: 424.3531\n",
      "Epoch 45/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 166.6321 - val_loss: 389.9630\n",
      "Epoch 46/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 164.0935 - val_loss: 290.4821\n",
      "Epoch 47/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 164.3771 - val_loss: 357.6194\n",
      "Epoch 48/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 158.3924 - val_loss: 359.6267\n",
      "Epoch 49/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 155.7692 - val_loss: 378.9187\n",
      "Epoch 50/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 156.3154 - val_loss: 373.7582\n",
      "Epoch 51/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 154.2652 - val_loss: 362.3107\n",
      "Epoch 52/140\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 148.9973 - val_loss: 336.4947\n",
      "Epoch 53/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 150.5358 - val_loss: 371.6791\n",
      "Epoch 54/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 152.9677 - val_loss: 337.1191\n",
      "Epoch 55/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 150.5561 - val_loss: 368.7769\n",
      "Epoch 56/140\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 147.0565 - val_loss: 304.7781\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 968.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:10:40,846] Trial 30 finished with value: 968.7874145507812 and parameters: {'head_size': 101, 'num_heads': 7, 'ff_dim': 10, 'num_transformer_blocks': 6, 'mlp_units': 128, 'mlp_dropout': 0.14717644004142227, 'dropout': 0.12215579291053837, 'learning_rate': 0.00048377162036344055, 'n_epochs': 140}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_360 (L  (None, 8, 5)                 10        ['input_32[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_180 (  (None, 8, 5)                 28985     ['layer_normalization_360[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_360[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_391 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_180[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_360 (  (None, 8, 5)                 0         ['dropout_391[0][0]',         \n",
      " TFOpLambda)                                                         'input_32[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_361 (L  (None, 8, 5)                 10        ['tf.__operators__.add_360[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_360 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_361[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_392 (Dropout)       (None, 8, 14)                0         ['conv1d_360[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_361 (Conv1D)         (None, 8, 5)                 75        ['dropout_392[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_361 (  (None, 8, 5)                 0         ['conv1d_361[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_360[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_362 (L  (None, 8, 5)                 10        ['tf.__operators__.add_361[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_181 (  (None, 8, 5)                 28985     ['layer_normalization_362[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_362[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_393 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_181[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_362 (  (None, 8, 5)                 0         ['dropout_393[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_361[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_363 (L  (None, 8, 5)                 10        ['tf.__operators__.add_362[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_362 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_363[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_394 (Dropout)       (None, 8, 14)                0         ['conv1d_362[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_363 (Conv1D)         (None, 8, 5)                 75        ['dropout_394[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_363 (  (None, 8, 5)                 0         ['conv1d_363[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_362[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_364 (L  (None, 8, 5)                 10        ['tf.__operators__.add_363[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_182 (  (None, 8, 5)                 28985     ['layer_normalization_364[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_364[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_395 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_182[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_364 (  (None, 8, 5)                 0         ['dropout_395[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_363[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_365 (L  (None, 8, 5)                 10        ['tf.__operators__.add_364[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_364 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_365[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_396 (Dropout)       (None, 8, 14)                0         ['conv1d_364[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_365 (Conv1D)         (None, 8, 5)                 75        ['dropout_396[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_365 (  (None, 8, 5)                 0         ['conv1d_365[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_364[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_366 (L  (None, 8, 5)                 10        ['tf.__operators__.add_365[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_183 (  (None, 8, 5)                 28985     ['layer_normalization_366[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_366[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_397 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_183[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_366 (  (None, 8, 5)                 0         ['dropout_397[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_365[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_367 (L  (None, 8, 5)                 10        ['tf.__operators__.add_366[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_366 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_367[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_398 (Dropout)       (None, 8, 14)                0         ['conv1d_366[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_367 (Conv1D)         (None, 8, 5)                 75        ['dropout_398[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_367 (  (None, 8, 5)                 0         ['conv1d_367[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_366[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_367[0][\n",
      " 1 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_93 (Dense)            (None, 99)                   891       ['global_average_pooling1d_31[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_399 (Dropout)       (None, 99)                   0         ['dense_93[0][0]']            \n",
      "                                                                                                  \n",
      " dense_94 (Dense)            (None, 16)                   1600      ['dropout_399[0][0]']         \n",
      "                                                                                                  \n",
      " dense_95 (Dense)            (None, 8)                    136       ['dense_94[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 119283 (465.95 KB)\n",
      "Trainable params: 119283 (465.95 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/157\n",
      "20/20 [==============================] - 2s 36ms/step - loss: 2275.2048 - val_loss: 3719.2358\n",
      "Epoch 2/157\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 1463.7219 - val_loss: 2845.4592\n",
      "Epoch 3/157\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 942.3690 - val_loss: 1646.2993\n",
      "Epoch 4/157\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 699.5100 - val_loss: 668.3880\n",
      "Epoch 5/157\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 628.2878 - val_loss: 541.7297\n",
      "Epoch 6/157\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 580.3783 - val_loss: 573.6048\n",
      "Epoch 7/157\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 545.4958 - val_loss: 442.4872\n",
      "Epoch 8/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 498.1518 - val_loss: 542.5579\n",
      "Epoch 9/157\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 466.8689 - val_loss: 421.7437\n",
      "Epoch 10/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 443.8546 - val_loss: 623.5051\n",
      "Epoch 11/157\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 420.7615 - val_loss: 506.6936\n",
      "Epoch 12/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 397.0316 - val_loss: 530.1389\n",
      "Epoch 13/157\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 382.8212 - val_loss: 458.2463\n",
      "Epoch 14/157\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 352.5005 - val_loss: 543.6222\n",
      "Epoch 15/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 345.1068 - val_loss: 361.9506\n",
      "Epoch 16/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 318.7781 - val_loss: 475.7616\n",
      "Epoch 17/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 304.0854 - val_loss: 451.0972\n",
      "Epoch 18/157\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 299.6530 - val_loss: 415.1170\n",
      "Epoch 19/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 285.5478 - val_loss: 482.0082\n",
      "Epoch 20/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 276.6741 - val_loss: 413.2262\n",
      "Epoch 21/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 268.5941 - val_loss: 396.6434\n",
      "Epoch 22/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 258.9345 - val_loss: 370.9504\n",
      "Epoch 23/157\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 245.7481 - val_loss: 436.5040\n",
      "Epoch 24/157\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 240.7368 - val_loss: 439.7802\n",
      "Epoch 25/157\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 229.5934 - val_loss: 370.3038\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1288.3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:10:58,037] Trial 31 finished with value: 1288.37060546875 and parameters: {'head_size': 180, 'num_heads': 7, 'ff_dim': 14, 'num_transformer_blocks': 4, 'mlp_units': 99, 'mlp_dropout': 0.15412138269478864, 'dropout': 0.1103829408255929, 'learning_rate': 0.0006557674258798695, 'n_epochs': 157}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_368 (L  (None, 8, 5)                 10        ['input_33[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_184 (  (None, 8, 5)                 23741     ['layer_normalization_368[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_368[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_400 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_184[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_368 (  (None, 8, 5)                 0         ['dropout_400[0][0]',         \n",
      " TFOpLambda)                                                         'input_33[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_369 (L  (None, 8, 5)                 10        ['tf.__operators__.add_368[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_368 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_369[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_401 (Dropout)       (None, 8, 17)                0         ['conv1d_368[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_369 (Conv1D)         (None, 8, 5)                 90        ['dropout_401[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_369 (  (None, 8, 5)                 0         ['conv1d_369[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_368[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_370 (L  (None, 8, 5)                 10        ['tf.__operators__.add_369[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_185 (  (None, 8, 5)                 23741     ['layer_normalization_370[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_370[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_402 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_185[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_370 (  (None, 8, 5)                 0         ['dropout_402[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_369[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_371 (L  (None, 8, 5)                 10        ['tf.__operators__.add_370[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_370 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_371[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_403 (Dropout)       (None, 8, 17)                0         ['conv1d_370[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_371 (Conv1D)         (None, 8, 5)                 90        ['dropout_403[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_371 (  (None, 8, 5)                 0         ['conv1d_371[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_370[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_372 (L  (None, 8, 5)                 10        ['tf.__operators__.add_371[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_186 (  (None, 8, 5)                 23741     ['layer_normalization_372[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_372[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_404 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_186[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_372 (  (None, 8, 5)                 0         ['dropout_404[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_371[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_373 (L  (None, 8, 5)                 10        ['tf.__operators__.add_372[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_372 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_373[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_405 (Dropout)       (None, 8, 17)                0         ['conv1d_372[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_373 (Conv1D)         (None, 8, 5)                 90        ['dropout_405[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_373 (  (None, 8, 5)                 0         ['conv1d_373[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_372[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_374 (L  (None, 8, 5)                 10        ['tf.__operators__.add_373[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_187 (  (None, 8, 5)                 23741     ['layer_normalization_374[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_374[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_406 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_187[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_374 (  (None, 8, 5)                 0         ['dropout_406[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_373[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_375 (L  (None, 8, 5)                 10        ['tf.__operators__.add_374[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_374 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_375[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_407 (Dropout)       (None, 8, 17)                0         ['conv1d_374[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_375 (Conv1D)         (None, 8, 5)                 90        ['dropout_407[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_375 (  (None, 8, 5)                 0         ['conv1d_375[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_374[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_376 (L  (None, 8, 5)                 10        ['tf.__operators__.add_375[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_188 (  (None, 8, 5)                 23741     ['layer_normalization_376[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_376[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_408 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_188[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_376 (  (None, 8, 5)                 0         ['dropout_408[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_375[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_377 (L  (None, 8, 5)                 10        ['tf.__operators__.add_376[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_376 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_377[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_409 (Dropout)       (None, 8, 17)                0         ['conv1d_376[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_377 (Conv1D)         (None, 8, 5)                 90        ['dropout_409[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_377 (  (None, 8, 5)                 0         ['conv1d_377[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_376[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_377[0][\n",
      " 2 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_96 (Dense)            (None, 108)                  972       ['global_average_pooling1d_32[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_410 (Dropout)       (None, 108)                  0         ['dense_96[0][0]']            \n",
      "                                                                                                  \n",
      " dense_97 (Dense)            (None, 16)                   1744      ['dropout_410[0][0]']         \n",
      "                                                                                                  \n",
      " dense_98 (Dense)            (None, 8)                    136       ['dense_97[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 122617 (478.97 KB)\n",
      "Trainable params: 122617 (478.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/176\n",
      "20/20 [==============================] - 2s 40ms/step - loss: 1760.9973 - val_loss: 2004.6704\n",
      "Epoch 2/176\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 939.0859 - val_loss: 1666.7542\n",
      "Epoch 3/176\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 744.3163 - val_loss: 1011.1957\n",
      "Epoch 4/176\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 620.5516 - val_loss: 521.9969\n",
      "Epoch 5/176\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 540.0237 - val_loss: 528.0852\n",
      "Epoch 6/176\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 488.8910 - val_loss: 389.5699\n",
      "Epoch 7/176\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 451.7570 - val_loss: 435.6616\n",
      "Epoch 8/176\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 420.4866 - val_loss: 430.7675\n",
      "Epoch 9/176\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 382.2842 - val_loss: 370.0488\n",
      "Epoch 10/176\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 357.4690 - val_loss: 392.0470\n",
      "Epoch 11/176\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 340.7551 - val_loss: 327.8466\n",
      "Epoch 12/176\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 315.9893 - val_loss: 449.8178\n",
      "Epoch 13/176\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 317.3287 - val_loss: 372.9125\n",
      "Epoch 14/176\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 293.1104 - val_loss: 393.4074\n",
      "Epoch 15/176\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 277.0023 - val_loss: 413.4034\n",
      "Epoch 16/176\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 266.5816 - val_loss: 435.3604\n",
      "Epoch 17/176\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 256.6042 - val_loss: 349.4492\n",
      "Epoch 18/176\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 244.7563 - val_loss: 395.9435\n",
      "Epoch 19/176\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 232.7129 - val_loss: 428.1747\n",
      "Epoch 20/176\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 229.2289 - val_loss: 330.2890\n",
      "Epoch 21/176\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 219.7533 - val_loss: 337.9211\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1161.7162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:11:14,606] Trial 32 finished with value: 1161.7161865234375 and parameters: {'head_size': 129, 'num_heads': 8, 'ff_dim': 17, 'num_transformer_blocks': 5, 'mlp_units': 108, 'mlp_dropout': 0.18284195068775294, 'dropout': 0.10169604404671008, 'learning_rate': 0.0009387459881159941, 'n_epochs': 176}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_378 (L  (None, 8, 5)                 10        ['input_34[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_189 (  (None, 8, 5)                 30595     ['layer_normalization_378[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_378[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_411 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_189[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_378 (  (None, 8, 5)                 0         ['dropout_411[0][0]',         \n",
      " TFOpLambda)                                                         'input_34[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_379 (L  (None, 8, 5)                 10        ['tf.__operators__.add_378[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_378 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_379[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_412 (Dropout)       (None, 8, 14)                0         ['conv1d_378[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_379 (Conv1D)         (None, 8, 5)                 75        ['dropout_412[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_379 (  (None, 8, 5)                 0         ['conv1d_379[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_378[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_380 (L  (None, 8, 5)                 10        ['tf.__operators__.add_379[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_190 (  (None, 8, 5)                 30595     ['layer_normalization_380[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_380[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_413 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_190[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_380 (  (None, 8, 5)                 0         ['dropout_413[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_379[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_381 (L  (None, 8, 5)                 10        ['tf.__operators__.add_380[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_380 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_381[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_414 (Dropout)       (None, 8, 14)                0         ['conv1d_380[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_381 (Conv1D)         (None, 8, 5)                 75        ['dropout_414[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_381 (  (None, 8, 5)                 0         ['conv1d_381[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_380[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_382 (L  (None, 8, 5)                 10        ['tf.__operators__.add_381[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_191 (  (None, 8, 5)                 30595     ['layer_normalization_382[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_382[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_415 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_191[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_382 (  (None, 8, 5)                 0         ['dropout_415[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_381[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_383 (L  (None, 8, 5)                 10        ['tf.__operators__.add_382[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_382 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_383[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_416 (Dropout)       (None, 8, 14)                0         ['conv1d_382[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_383 (Conv1D)         (None, 8, 5)                 75        ['dropout_416[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_383 (  (None, 8, 5)                 0         ['conv1d_383[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_382[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_384 (L  (None, 8, 5)                 10        ['tf.__operators__.add_383[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_192 (  (None, 8, 5)                 30595     ['layer_normalization_384[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_384[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_417 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_192[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_384 (  (None, 8, 5)                 0         ['dropout_417[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_383[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_385 (L  (None, 8, 5)                 10        ['tf.__operators__.add_384[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_384 (Conv1D)         (None, 8, 14)                84        ['layer_normalization_385[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_418 (Dropout)       (None, 8, 14)                0         ['conv1d_384[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_385 (Conv1D)         (None, 8, 5)                 75        ['dropout_418[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_385 (  (None, 8, 5)                 0         ['conv1d_385[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_384[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_385[0][\n",
      " 3 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_99 (Dense)            (None, 103)                  927       ['global_average_pooling1d_33[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_419 (Dropout)       (None, 103)                  0         ['dense_99[0][0]']            \n",
      "                                                                                                  \n",
      " dense_100 (Dense)           (None, 16)                   1664      ['dropout_419[0][0]']         \n",
      "                                                                                                  \n",
      " dense_101 (Dense)           (None, 8)                    136       ['dense_100[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125823 (491.50 KB)\n",
      "Trainable params: 125823 (491.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/162\n",
      "20/20 [==============================] - 2s 38ms/step - loss: 2203.9966 - val_loss: 4884.3911\n",
      "Epoch 2/162\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1644.1254 - val_loss: 3134.1228\n",
      "Epoch 3/162\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1319.2335 - val_loss: 2429.7424\n",
      "Epoch 4/162\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 1059.6757 - val_loss: 2039.1760\n",
      "Epoch 5/162\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 860.5004 - val_loss: 1572.1271\n",
      "Epoch 6/162\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 725.1074 - val_loss: 1285.7346\n",
      "Epoch 7/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 657.0321 - val_loss: 767.9835\n",
      "Epoch 8/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 597.9810 - val_loss: 584.4116\n",
      "Epoch 9/162\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 555.9391 - val_loss: 551.6370\n",
      "Epoch 10/162\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 524.0638 - val_loss: 392.9830\n",
      "Epoch 11/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 496.2834 - val_loss: 409.3394\n",
      "Epoch 12/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 477.5907 - val_loss: 351.2064\n",
      "Epoch 13/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 456.1268 - val_loss: 335.9599\n",
      "Epoch 14/162\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 441.0028 - val_loss: 384.7013\n",
      "Epoch 15/162\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 420.9980 - val_loss: 351.6155\n",
      "Epoch 16/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 414.3113 - val_loss: 409.5361\n",
      "Epoch 17/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 384.3966 - val_loss: 402.9277\n",
      "Epoch 18/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 370.5981 - val_loss: 321.6081\n",
      "Epoch 19/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 365.9472 - val_loss: 380.5244\n",
      "Epoch 20/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 358.8331 - val_loss: 354.5294\n",
      "Epoch 21/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 340.8308 - val_loss: 300.9221\n",
      "Epoch 22/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 327.5742 - val_loss: 362.2363\n",
      "Epoch 23/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 314.8469 - val_loss: 354.5414\n",
      "Epoch 24/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 309.3256 - val_loss: 346.4738\n",
      "Epoch 25/162\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 302.9614 - val_loss: 347.0147\n",
      "Epoch 26/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 298.6180 - val_loss: 393.8407\n",
      "Epoch 27/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 282.8587 - val_loss: 369.2688\n",
      "Epoch 28/162\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 283.9229 - val_loss: 389.5893\n",
      "Epoch 29/162\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 266.9876 - val_loss: 334.2622\n",
      "Epoch 30/162\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 259.4667 - val_loss: 338.8763\n",
      "Epoch 31/162\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 257.6247 - val_loss: 372.9896\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 892.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:11:35,327] Trial 33 finished with value: 892.8955688476562 and parameters: {'head_size': 190, 'num_heads': 7, 'ff_dim': 14, 'num_transformer_blocks': 4, 'mlp_units': 103, 'mlp_dropout': 0.16775931201234054, 'dropout': 0.11821527352308128, 'learning_rate': 0.00042879152452888347, 'n_epochs': 162}. Best is trial 23 with value: 802.0839233398438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_34\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_386 (L  (None, 8, 5)                 10        ['input_35[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_193 (  (None, 8, 5)                 25121     ['layer_normalization_386[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_386[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_420 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_193[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_386 (  (None, 8, 5)                 0         ['dropout_420[0][0]',         \n",
      " TFOpLambda)                                                         'input_35[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_387 (L  (None, 8, 5)                 10        ['tf.__operators__.add_386[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_386 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_387[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_421 (Dropout)       (None, 8, 15)                0         ['conv1d_386[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_387 (Conv1D)         (None, 8, 5)                 80        ['dropout_421[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_387 (  (None, 8, 5)                 0         ['conv1d_387[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_386[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_388 (L  (None, 8, 5)                 10        ['tf.__operators__.add_387[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_194 (  (None, 8, 5)                 25121     ['layer_normalization_388[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_388[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_422 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_194[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_388 (  (None, 8, 5)                 0         ['dropout_422[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_387[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_389 (L  (None, 8, 5)                 10        ['tf.__operators__.add_388[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_388 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_389[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_423 (Dropout)       (None, 8, 15)                0         ['conv1d_388[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_389 (Conv1D)         (None, 8, 5)                 80        ['dropout_423[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_389 (  (None, 8, 5)                 0         ['conv1d_389[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_388[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_390 (L  (None, 8, 5)                 10        ['tf.__operators__.add_389[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_195 (  (None, 8, 5)                 25121     ['layer_normalization_390[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_390[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_424 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_195[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_390 (  (None, 8, 5)                 0         ['dropout_424[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_389[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_391 (L  (None, 8, 5)                 10        ['tf.__operators__.add_390[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_390 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_391[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_425 (Dropout)       (None, 8, 15)                0         ['conv1d_390[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_391 (Conv1D)         (None, 8, 5)                 80        ['dropout_425[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_391 (  (None, 8, 5)                 0         ['conv1d_391[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_390[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_392 (L  (None, 8, 5)                 10        ['tf.__operators__.add_391[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_196 (  (None, 8, 5)                 25121     ['layer_normalization_392[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_392[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_426 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_196[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_392 (  (None, 8, 5)                 0         ['dropout_426[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_391[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_393 (L  (None, 8, 5)                 10        ['tf.__operators__.add_392[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_392 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_393[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_427 (Dropout)       (None, 8, 15)                0         ['conv1d_392[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_393 (Conv1D)         (None, 8, 5)                 80        ['dropout_427[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_393 (  (None, 8, 5)                 0         ['conv1d_393[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_392[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_394 (L  (None, 8, 5)                 10        ['tf.__operators__.add_393[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_197 (  (None, 8, 5)                 25121     ['layer_normalization_394[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_394[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_428 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_197[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_394 (  (None, 8, 5)                 0         ['dropout_428[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_393[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_395 (L  (None, 8, 5)                 10        ['tf.__operators__.add_394[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_394 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_395[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_429 (Dropout)       (None, 8, 15)                0         ['conv1d_394[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_395 (Conv1D)         (None, 8, 5)                 80        ['dropout_429[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_395 (  (None, 8, 5)                 0         ['conv1d_395[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_394[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_395[0][\n",
      " 4 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_102 (Dense)           (None, 116)                  1044      ['global_average_pooling1d_34[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_430 (Dropout)       (None, 116)                  0         ['dense_102[0][0]']           \n",
      "                                                                                                  \n",
      " dense_103 (Dense)           (None, 16)                   1872      ['dropout_430[0][0]']         \n",
      "                                                                                                  \n",
      " dense_104 (Dense)           (None, 8)                    136       ['dense_103[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 129607 (506.28 KB)\n",
      "Trainable params: 129607 (506.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/187\n",
      "20/20 [==============================] - 2s 43ms/step - loss: 1887.8862 - val_loss: 2043.2042\n",
      "Epoch 2/187\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1019.5536 - val_loss: 1170.9398\n",
      "Epoch 3/187\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 772.4938 - val_loss: 1077.1796\n",
      "Epoch 4/187\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 655.5544 - val_loss: 1118.1622\n",
      "Epoch 5/187\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 553.6005 - val_loss: 662.9025\n",
      "Epoch 6/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 481.7939 - val_loss: 538.4017\n",
      "Epoch 7/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 448.1160 - val_loss: 459.9793\n",
      "Epoch 8/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 427.7932 - val_loss: 396.8132\n",
      "Epoch 9/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 414.3223 - val_loss: 599.7472\n",
      "Epoch 10/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 385.2867 - val_loss: 485.4761\n",
      "Epoch 11/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 364.1905 - val_loss: 334.4379\n",
      "Epoch 12/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 348.3976 - val_loss: 445.7093\n",
      "Epoch 13/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 335.4666 - val_loss: 372.6783\n",
      "Epoch 14/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 315.3152 - val_loss: 431.7641\n",
      "Epoch 15/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 304.2927 - val_loss: 365.8389\n",
      "Epoch 16/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 290.2814 - val_loss: 336.6052\n",
      "Epoch 17/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 278.8473 - val_loss: 331.8291\n",
      "Epoch 18/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 268.3410 - val_loss: 423.5963\n",
      "Epoch 19/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 255.9177 - val_loss: 338.2012\n",
      "Epoch 20/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 245.3261 - val_loss: 370.7035\n",
      "Epoch 21/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 244.5129 - val_loss: 368.9059\n",
      "Epoch 22/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 236.4629 - val_loss: 324.3129\n",
      "Epoch 23/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 221.4378 - val_loss: 288.9356\n",
      "Epoch 24/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 217.3333 - val_loss: 282.4753\n",
      "Epoch 25/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 216.1616 - val_loss: 313.5137\n",
      "Epoch 26/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 210.7063 - val_loss: 272.6396\n",
      "Epoch 27/187\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 199.3437 - val_loss: 370.3080\n",
      "Epoch 28/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 195.2390 - val_loss: 324.9595\n",
      "Epoch 29/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 190.5749 - val_loss: 281.2099\n",
      "Epoch 30/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 185.7088 - val_loss: 281.2013\n",
      "Epoch 31/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 187.8790 - val_loss: 282.6627\n",
      "Epoch 32/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 178.9998 - val_loss: 277.8073\n",
      "Epoch 33/187\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 180.4727 - val_loss: 285.3485\n",
      "Epoch 34/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 174.0656 - val_loss: 273.3283\n",
      "Epoch 35/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 168.5109 - val_loss: 235.3055\n",
      "Epoch 36/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 174.5237 - val_loss: 336.3114\n",
      "Epoch 37/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 163.1207 - val_loss: 246.0520\n",
      "Epoch 38/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 163.4204 - val_loss: 236.5571\n",
      "Epoch 39/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 155.3626 - val_loss: 310.3662\n",
      "Epoch 40/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 156.7060 - val_loss: 253.4529\n",
      "Epoch 41/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 153.6567 - val_loss: 301.2799\n",
      "Epoch 42/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 155.4549 - val_loss: 284.6186\n",
      "Epoch 43/187\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 145.4326 - val_loss: 242.6599\n",
      "Epoch 44/187\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 147.5154 - val_loss: 273.9496\n",
      "Epoch 45/187\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 147.2339 - val_loss: 287.8241\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 656.3905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:12:10,170] Trial 34 finished with value: 656.3905029296875 and parameters: {'head_size': 156, 'num_heads': 7, 'ff_dim': 15, 'num_transformer_blocks': 5, 'mlp_units': 116, 'mlp_dropout': 0.11726807327113829, 'dropout': 0.12768472299347952, 'learning_rate': 0.0006473993344407467, 'n_epochs': 187}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_396 (L  (None, 8, 5)                 10        ['input_36[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_198 (  (None, 8, 5)                 22223     ['layer_normalization_396[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_396[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_431 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_198[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_396 (  (None, 8, 5)                 0         ['dropout_431[0][0]',         \n",
      " TFOpLambda)                                                         'input_36[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_397 (L  (None, 8, 5)                 10        ['tf.__operators__.add_396[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_396 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_397[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_432 (Dropout)       (None, 8, 16)                0         ['conv1d_396[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_397 (Conv1D)         (None, 8, 5)                 85        ['dropout_432[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_397 (  (None, 8, 5)                 0         ['conv1d_397[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_396[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_398 (L  (None, 8, 5)                 10        ['tf.__operators__.add_397[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_199 (  (None, 8, 5)                 22223     ['layer_normalization_398[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_398[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_433 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_199[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_398 (  (None, 8, 5)                 0         ['dropout_433[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_397[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_399 (L  (None, 8, 5)                 10        ['tf.__operators__.add_398[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_398 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_399[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_434 (Dropout)       (None, 8, 16)                0         ['conv1d_398[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_399 (Conv1D)         (None, 8, 5)                 85        ['dropout_434[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_399 (  (None, 8, 5)                 0         ['conv1d_399[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_398[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_400 (L  (None, 8, 5)                 10        ['tf.__operators__.add_399[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_200 (  (None, 8, 5)                 22223     ['layer_normalization_400[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_400[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_435 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_200[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_400 (  (None, 8, 5)                 0         ['dropout_435[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_399[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_401 (L  (None, 8, 5)                 10        ['tf.__operators__.add_400[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_400 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_401[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_436 (Dropout)       (None, 8, 16)                0         ['conv1d_400[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_401 (Conv1D)         (None, 8, 5)                 85        ['dropout_436[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_401 (  (None, 8, 5)                 0         ['conv1d_401[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_400[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_402 (L  (None, 8, 5)                 10        ['tf.__operators__.add_401[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_201 (  (None, 8, 5)                 22223     ['layer_normalization_402[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_402[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_437 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_201[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_402 (  (None, 8, 5)                 0         ['dropout_437[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_401[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_403 (L  (None, 8, 5)                 10        ['tf.__operators__.add_402[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_402 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_403[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_438 (Dropout)       (None, 8, 16)                0         ['conv1d_402[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_403 (Conv1D)         (None, 8, 5)                 85        ['dropout_438[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_403 (  (None, 8, 5)                 0         ['conv1d_403[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_402[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_404 (L  (None, 8, 5)                 10        ['tf.__operators__.add_403[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_202 (  (None, 8, 5)                 22223     ['layer_normalization_404[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_404[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_439 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_202[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_404 (  (None, 8, 5)                 0         ['dropout_439[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_403[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_405 (L  (None, 8, 5)                 10        ['tf.__operators__.add_404[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_404 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_405[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_440 (Dropout)       (None, 8, 16)                0         ['conv1d_404[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_405 (Conv1D)         (None, 8, 5)                 85        ['dropout_440[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_405 (  (None, 8, 5)                 0         ['conv1d_405[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_404[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_405[0][\n",
      " 5 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_105 (Dense)           (None, 115)                  1035      ['global_average_pooling1d_35[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_441 (Dropout)       (None, 115)                  0         ['dense_105[0][0]']           \n",
      "                                                                                                  \n",
      " dense_106 (Dense)           (None, 16)                   1856      ['dropout_441[0][0]']         \n",
      "                                                                                                  \n",
      " dense_107 (Dense)           (None, 8)                    136       ['dense_106[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 115147 (449.79 KB)\n",
      "Trainable params: 115147 (449.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/187\n",
      "20/20 [==============================] - 2s 42ms/step - loss: 2526.8696 - val_loss: 6338.5439\n",
      "Epoch 2/187\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 2172.7454 - val_loss: 5325.6909\n",
      "Epoch 3/187\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1923.3964 - val_loss: 4607.3403\n",
      "Epoch 4/187\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1713.8395 - val_loss: 4043.9705\n",
      "Epoch 5/187\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1535.7108 - val_loss: 3543.7620\n",
      "Epoch 6/187\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1357.9958 - val_loss: 3161.7551\n",
      "Epoch 7/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1185.0718 - val_loss: 2815.5176\n",
      "Epoch 8/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1050.5814 - val_loss: 2614.2385\n",
      "Epoch 9/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 916.4355 - val_loss: 2316.4353\n",
      "Epoch 10/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 805.4992 - val_loss: 2006.5248\n",
      "Epoch 11/187\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 700.7137 - val_loss: 1674.8835\n",
      "Epoch 12/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 631.1777 - val_loss: 1348.0024\n",
      "Epoch 13/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 588.7830 - val_loss: 1032.0793\n",
      "Epoch 14/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 559.3748 - val_loss: 857.8633\n",
      "Epoch 15/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 531.4974 - val_loss: 705.5281\n",
      "Epoch 16/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 508.9481 - val_loss: 553.4126\n",
      "Epoch 17/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 505.1952 - val_loss: 514.8591\n",
      "Epoch 18/187\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 498.5925 - val_loss: 377.6145\n",
      "Epoch 19/187\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 493.0761 - val_loss: 506.8307\n",
      "Epoch 20/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 479.3553 - val_loss: 453.8926\n",
      "Epoch 21/187\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 472.6094 - val_loss: 418.1683\n",
      "Epoch 22/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 470.0128 - val_loss: 500.2248\n",
      "Epoch 23/187\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 463.5205 - val_loss: 480.5678\n",
      "Epoch 24/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 455.2963 - val_loss: 487.7704\n",
      "Epoch 25/187\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 457.6517 - val_loss: 432.3418\n",
      "Epoch 26/187\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 457.3637 - val_loss: 542.3602\n",
      "Epoch 27/187\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 446.3007 - val_loss: 454.1809\n",
      "Epoch 28/187\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 457.6246 - val_loss: 462.3469\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1367.7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:12:31,844] Trial 35 finished with value: 1367.78076171875 and parameters: {'head_size': 161, 'num_heads': 6, 'ff_dim': 16, 'num_transformer_blocks': 5, 'mlp_units': 115, 'mlp_dropout': 0.12026798258284352, 'dropout': 0.13444851746802522, 'learning_rate': 0.00011379241874480937, 'n_epochs': 187}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_36\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_406 (L  (None, 8, 5)                 10        ['input_37[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_203 (  (None, 8, 5)                 33976     ['layer_normalization_406[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_406[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_442 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_203[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_406 (  (None, 8, 5)                 0         ['dropout_442[0][0]',         \n",
      " TFOpLambda)                                                         'input_37[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_407 (L  (None, 8, 5)                 10        ['tf.__operators__.add_406[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_406 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_407[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_443 (Dropout)       (None, 8, 19)                0         ['conv1d_406[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_407 (Conv1D)         (None, 8, 5)                 100       ['dropout_443[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_407 (  (None, 8, 5)                 0         ['conv1d_407[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_406[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_408 (L  (None, 8, 5)                 10        ['tf.__operators__.add_407[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_204 (  (None, 8, 5)                 33976     ['layer_normalization_408[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_408[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_444 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_204[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_408 (  (None, 8, 5)                 0         ['dropout_444[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_407[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_409 (L  (None, 8, 5)                 10        ['tf.__operators__.add_408[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_408 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_409[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_445 (Dropout)       (None, 8, 19)                0         ['conv1d_408[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_409 (Conv1D)         (None, 8, 5)                 100       ['dropout_445[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_409 (  (None, 8, 5)                 0         ['conv1d_409[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_408[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_410 (L  (None, 8, 5)                 10        ['tf.__operators__.add_409[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_205 (  (None, 8, 5)                 33976     ['layer_normalization_410[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_410[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_446 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_205[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_410 (  (None, 8, 5)                 0         ['dropout_446[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_409[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_411 (L  (None, 8, 5)                 10        ['tf.__operators__.add_410[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_410 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_411[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_447 (Dropout)       (None, 8, 19)                0         ['conv1d_410[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_411 (Conv1D)         (None, 8, 5)                 100       ['dropout_447[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_411 (  (None, 8, 5)                 0         ['conv1d_411[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_410[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_412 (L  (None, 8, 5)                 10        ['tf.__operators__.add_411[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_206 (  (None, 8, 5)                 33976     ['layer_normalization_412[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_412[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_448 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_206[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_412 (  (None, 8, 5)                 0         ['dropout_448[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_411[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_413 (L  (None, 8, 5)                 10        ['tf.__operators__.add_412[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_412 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_413[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_449 (Dropout)       (None, 8, 19)                0         ['conv1d_412[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_413 (Conv1D)         (None, 8, 5)                 100       ['dropout_449[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_413 (  (None, 8, 5)                 0         ['conv1d_413[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_412[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_414 (L  (None, 8, 5)                 10        ['tf.__operators__.add_413[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_207 (  (None, 8, 5)                 33976     ['layer_normalization_414[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_414[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_450 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_207[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_414 (  (None, 8, 5)                 0         ['dropout_450[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_413[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_415 (L  (None, 8, 5)                 10        ['tf.__operators__.add_414[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_414 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_415[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_451 (Dropout)       (None, 8, 19)                0         ['conv1d_414[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_415 (Conv1D)         (None, 8, 5)                 100       ['dropout_451[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_415 (  (None, 8, 5)                 0         ['conv1d_415[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_414[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_416 (L  (None, 8, 5)                 10        ['tf.__operators__.add_415[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_208 (  (None, 8, 5)                 33976     ['layer_normalization_416[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_416[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_452 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_208[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_416 (  (None, 8, 5)                 0         ['dropout_452[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_415[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_417 (L  (None, 8, 5)                 10        ['tf.__operators__.add_416[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_416 (Conv1D)         (None, 8, 19)                114       ['layer_normalization_417[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_453 (Dropout)       (None, 8, 19)                0         ['conv1d_416[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_417 (Conv1D)         (None, 8, 5)                 100       ['dropout_453[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_417 (  (None, 8, 5)                 0         ['conv1d_417[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_416[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_417[0][\n",
      " 6 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_108 (Dense)           (None, 119)                  1071      ['global_average_pooling1d_36[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_454 (Dropout)       (None, 119)                  0         ['dense_108[0][0]']           \n",
      "                                                                                                  \n",
      " dense_109 (Dense)           (None, 16)                   1920      ['dropout_454[0][0]']         \n",
      "                                                                                                  \n",
      " dense_110 (Dense)           (None, 8)                    136       ['dense_109[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 208387 (814.01 KB)\n",
      "Trainable params: 208387 (814.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/185\n",
      "20/20 [==============================] - 3s 66ms/step - loss: 2612.1741 - val_loss: 6174.2275\n",
      "Epoch 2/185\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 1849.7605 - val_loss: 4884.5513\n",
      "Epoch 3/185\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 1367.8951 - val_loss: 3923.3748\n",
      "Epoch 4/185\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1030.2471 - val_loss: 3046.1567\n",
      "Epoch 5/185\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 869.2728 - val_loss: 2602.9849\n",
      "Epoch 6/185\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 808.4610 - val_loss: 2274.5022\n",
      "Epoch 7/185\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 742.0199 - val_loss: 1867.4094\n",
      "Epoch 8/185\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 695.6821 - val_loss: 1605.2850\n",
      "Epoch 9/185\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 657.1840 - val_loss: 1405.9664\n",
      "Epoch 10/185\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 609.3866 - val_loss: 1120.9882\n",
      "Epoch 11/185\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 573.7167 - val_loss: 1004.1030\n",
      "Epoch 12/185\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 537.2439 - val_loss: 735.5139\n",
      "Epoch 13/185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 504.0187 - val_loss: 601.2059\n",
      "Epoch 14/185\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 481.2905 - val_loss: 464.6758\n",
      "Epoch 15/185\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 467.7825 - val_loss: 391.9590\n",
      "Epoch 16/185\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 447.2563 - val_loss: 402.9700\n",
      "Epoch 17/185\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 437.0018 - val_loss: 342.4146\n",
      "Epoch 18/185\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 438.7765 - val_loss: 387.9963\n",
      "Epoch 19/185\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 429.7712 - val_loss: 407.2722\n",
      "Epoch 20/185\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 409.2351 - val_loss: 324.0656\n",
      "Epoch 21/185\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 399.4481 - val_loss: 422.1949\n",
      "Epoch 22/185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 401.6214 - val_loss: 355.2207\n",
      "Epoch 23/185\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 384.1396 - val_loss: 413.3264\n",
      "Epoch 24/185\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 379.4362 - val_loss: 338.7459\n",
      "Epoch 25/185\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 371.6781 - val_loss: 343.8436\n",
      "Epoch 26/185\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 365.5403 - val_loss: 342.6321\n",
      "Epoch 27/185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 358.9243 - val_loss: 364.7983\n",
      "Epoch 28/185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 346.1965 - val_loss: 362.2500\n",
      "Epoch 29/185\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 343.2652 - val_loss: 356.7847\n",
      "Epoch 30/185\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 339.6507 - val_loss: 357.7452\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1167.2394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:13:08,881] Trial 36 finished with value: 1167.2393798828125 and parameters: {'head_size': 211, 'num_heads': 7, 'ff_dim': 19, 'num_transformer_blocks': 6, 'mlp_units': 119, 'mlp_dropout': 0.12104320029098922, 'dropout': 0.15351251124522952, 'learning_rate': 0.00022396911999477965, 'n_epochs': 185}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_418 (L  (None, 8, 5)                 10        ['input_38[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_209 (  (None, 8, 5)                 10907     ['layer_normalization_418[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_418[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_455 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_209[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_418 (  (None, 8, 5)                 0         ['dropout_455[0][0]',         \n",
      " TFOpLambda)                                                         'input_38[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_419 (L  (None, 8, 5)                 10        ['tf.__operators__.add_418[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_418 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_419[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_456 (Dropout)       (None, 8, 16)                0         ['conv1d_418[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_419 (Conv1D)         (None, 8, 5)                 85        ['dropout_456[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_419 (  (None, 8, 5)                 0         ['conv1d_419[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_418[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_420 (L  (None, 8, 5)                 10        ['tf.__operators__.add_419[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_210 (  (None, 8, 5)                 10907     ['layer_normalization_420[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_420[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_457 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_210[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_420 (  (None, 8, 5)                 0         ['dropout_457[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_419[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_421 (L  (None, 8, 5)                 10        ['tf.__operators__.add_420[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_420 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_421[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_458 (Dropout)       (None, 8, 16)                0         ['conv1d_420[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_421 (Conv1D)         (None, 8, 5)                 85        ['dropout_458[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_421 (  (None, 8, 5)                 0         ['conv1d_421[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_420[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_422 (L  (None, 8, 5)                 10        ['tf.__operators__.add_421[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_211 (  (None, 8, 5)                 10907     ['layer_normalization_422[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_422[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_459 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_211[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_422 (  (None, 8, 5)                 0         ['dropout_459[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_421[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_423 (L  (None, 8, 5)                 10        ['tf.__operators__.add_422[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_422 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_423[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_460 (Dropout)       (None, 8, 16)                0         ['conv1d_422[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_423 (Conv1D)         (None, 8, 5)                 85        ['dropout_460[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_423 (  (None, 8, 5)                 0         ['conv1d_423[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_422[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_424 (L  (None, 8, 5)                 10        ['tf.__operators__.add_423[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_212 (  (None, 8, 5)                 10907     ['layer_normalization_424[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_424[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_461 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_212[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_424 (  (None, 8, 5)                 0         ['dropout_461[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_423[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_425 (L  (None, 8, 5)                 10        ['tf.__operators__.add_424[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_424 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_425[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_462 (Dropout)       (None, 8, 16)                0         ['conv1d_424[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_425 (Conv1D)         (None, 8, 5)                 85        ['dropout_462[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_425 (  (None, 8, 5)                 0         ['conv1d_425[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_424[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_426 (L  (None, 8, 5)                 10        ['tf.__operators__.add_425[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_213 (  (None, 8, 5)                 10907     ['layer_normalization_426[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_426[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_463 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_213[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_426 (  (None, 8, 5)                 0         ['dropout_463[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_425[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_427 (L  (None, 8, 5)                 10        ['tf.__operators__.add_426[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_426 (Conv1D)         (None, 8, 16)                96        ['layer_normalization_427[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_464 (Dropout)       (None, 8, 16)                0         ['conv1d_426[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_427 (Conv1D)         (None, 8, 5)                 85        ['dropout_464[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_427 (  (None, 8, 5)                 0         ['conv1d_427[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_426[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_427[0][\n",
      " 7 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_111 (Dense)           (None, 111)                  999       ['global_average_pooling1d_37[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_465 (Dropout)       (None, 111)                  0         ['dense_111[0][0]']           \n",
      "                                                                                                  \n",
      " dense_112 (Dense)           (None, 16)                   1792      ['dropout_465[0][0]']         \n",
      "                                                                                                  \n",
      " dense_113 (Dense)           (None, 8)                    136       ['dense_112[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 58467 (228.39 KB)\n",
      "Trainable params: 58467 (228.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/141\n",
      "20/20 [==============================] - 2s 40ms/step - loss: 2898.2976 - val_loss: 8233.4492\n",
      "Epoch 2/141\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2743.5657 - val_loss: 7708.3706\n",
      "Epoch 3/141\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 2622.3645 - val_loss: 7315.4336\n",
      "Epoch 4/141\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 2500.6968 - val_loss: 6988.6904\n",
      "Epoch 5/141\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 2395.6602 - val_loss: 6712.6934\n",
      "Epoch 6/141\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 2321.3887 - val_loss: 6424.2227\n",
      "Epoch 7/141\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2233.3240 - val_loss: 6151.2461\n",
      "Epoch 8/141\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2169.2788 - val_loss: 5897.1582\n",
      "Epoch 9/141\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2094.0715 - val_loss: 5600.8594\n",
      "Epoch 10/141\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2013.5812 - val_loss: 5322.1021\n",
      "Epoch 11/141\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 1945.2651 - val_loss: 5062.7339\n",
      "Epoch 12/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1872.3613 - val_loss: 4786.9038\n",
      "Epoch 13/141\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 1790.8256 - val_loss: 4520.0601\n",
      "Epoch 14/141\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1709.4421 - val_loss: 4219.6333\n",
      "Epoch 15/141\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1638.0692 - val_loss: 3919.9888\n",
      "Epoch 16/141\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1546.1101 - val_loss: 3637.7146\n",
      "Epoch 17/141\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1458.1542 - val_loss: 3323.3206\n",
      "Epoch 18/141\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 1371.3071 - val_loss: 3010.7100\n",
      "Epoch 19/141\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1292.1625 - val_loss: 2689.5564\n",
      "Epoch 20/141\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1189.3035 - val_loss: 2377.7964\n",
      "Epoch 21/141\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 1117.0911 - val_loss: 2078.7612\n",
      "Epoch 22/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1039.6326 - val_loss: 1830.9811\n",
      "Epoch 23/141\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 957.2982 - val_loss: 1612.2412\n",
      "Epoch 24/141\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 896.9529 - val_loss: 1399.2471\n",
      "Epoch 25/141\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 835.8868 - val_loss: 1197.6416\n",
      "Epoch 26/141\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 773.4800 - val_loss: 1027.1924\n",
      "Epoch 27/141\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 725.8032 - val_loss: 977.5578\n",
      "Epoch 28/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 672.5753 - val_loss: 853.9815\n",
      "Epoch 29/141\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 638.8589 - val_loss: 776.7794\n",
      "Epoch 30/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 597.7606 - val_loss: 685.2239\n",
      "Epoch 31/141\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 582.0172 - val_loss: 527.8328\n",
      "Epoch 32/141\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 552.4300 - val_loss: 468.6320\n",
      "Epoch 33/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 540.0981 - val_loss: 441.6207\n",
      "Epoch 34/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 538.5020 - val_loss: 435.7487\n",
      "Epoch 35/141\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 519.3112 - val_loss: 372.8264\n",
      "Epoch 36/141\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 513.5441 - val_loss: 303.0784\n",
      "Epoch 37/141\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 505.2216 - val_loss: 328.3897\n",
      "Epoch 38/141\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 500.0556 - val_loss: 297.0514\n",
      "Epoch 39/141\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 493.2658 - val_loss: 318.6770\n",
      "Epoch 40/141\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 493.9773 - val_loss: 291.5544\n",
      "Epoch 41/141\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 486.4402 - val_loss: 324.6861\n",
      "Epoch 42/141\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 481.3114 - val_loss: 303.1779\n",
      "Epoch 43/141\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 479.8288 - val_loss: 268.6467\n",
      "Epoch 44/141\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 485.7250 - val_loss: 295.3748\n",
      "Epoch 45/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 462.1232 - val_loss: 277.1856\n",
      "Epoch 46/141\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 469.6858 - val_loss: 289.1241\n",
      "Epoch 47/141\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 457.7755 - val_loss: 292.6973\n",
      "Epoch 48/141\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 468.0707 - val_loss: 314.0079\n",
      "Epoch 49/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 462.6793 - val_loss: 290.3015\n",
      "Epoch 50/141\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 450.4359 - val_loss: 320.2697\n",
      "Epoch 51/141\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 459.7133 - val_loss: 273.6273\n",
      "Epoch 52/141\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 451.4776 - val_loss: 331.9109\n",
      "Epoch 53/141\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 444.9936 - val_loss: 294.3797\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1151.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:13:50,849] Trial 37 finished with value: 1151.10009765625 and parameters: {'head_size': 79, 'num_heads': 6, 'ff_dim': 16, 'num_transformer_blocks': 5, 'mlp_units': 111, 'mlp_dropout': 0.13700151481602954, 'dropout': 0.1788347213150578, 'learning_rate': 6.007154163861995e-05, 'n_epochs': 141}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_428 (L  (None, 8, 5)                 10        ['input_39[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_214 (  (None, 8, 5)                 15300     ['layer_normalization_428[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_428[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_466 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_214[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_428 (  (None, 8, 5)                 0         ['dropout_466[0][0]',         \n",
      " TFOpLambda)                                                         'input_39[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_429 (L  (None, 8, 5)                 10        ['tf.__operators__.add_428[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_428 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_429[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_467 (Dropout)       (None, 8, 20)                0         ['conv1d_428[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_429 (Conv1D)         (None, 8, 5)                 105       ['dropout_467[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_429 (  (None, 8, 5)                 0         ['conv1d_429[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_428[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_430 (L  (None, 8, 5)                 10        ['tf.__operators__.add_429[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_215 (  (None, 8, 5)                 15300     ['layer_normalization_430[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_430[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_468 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_215[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_430 (  (None, 8, 5)                 0         ['dropout_468[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_429[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_431 (L  (None, 8, 5)                 10        ['tf.__operators__.add_430[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_430 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_431[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_469 (Dropout)       (None, 8, 20)                0         ['conv1d_430[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_431 (Conv1D)         (None, 8, 5)                 105       ['dropout_469[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_431 (  (None, 8, 5)                 0         ['conv1d_431[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_430[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_432 (L  (None, 8, 5)                 10        ['tf.__operators__.add_431[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_216 (  (None, 8, 5)                 15300     ['layer_normalization_432[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_432[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_470 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_216[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_432 (  (None, 8, 5)                 0         ['dropout_470[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_431[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_433 (L  (None, 8, 5)                 10        ['tf.__operators__.add_432[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_432 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_433[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_471 (Dropout)       (None, 8, 20)                0         ['conv1d_432[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_433 (Conv1D)         (None, 8, 5)                 105       ['dropout_471[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_433 (  (None, 8, 5)                 0         ['conv1d_433[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_432[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_434 (L  (None, 8, 5)                 10        ['tf.__operators__.add_433[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_217 (  (None, 8, 5)                 15300     ['layer_normalization_434[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_434[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_472 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_217[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_434 (  (None, 8, 5)                 0         ['dropout_472[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_433[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_435 (L  (None, 8, 5)                 10        ['tf.__operators__.add_434[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_434 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_435[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_473 (Dropout)       (None, 8, 20)                0         ['conv1d_434[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_435 (Conv1D)         (None, 8, 5)                 105       ['dropout_473[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_435 (  (None, 8, 5)                 0         ['conv1d_435[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_434[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_436 (L  (None, 8, 5)                 10        ['tf.__operators__.add_435[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_218 (  (None, 8, 5)                 15300     ['layer_normalization_436[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_436[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_474 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_218[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_436 (  (None, 8, 5)                 0         ['dropout_474[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_435[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_437 (L  (None, 8, 5)                 10        ['tf.__operators__.add_436[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_436 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_437[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_475 (Dropout)       (None, 8, 20)                0         ['conv1d_436[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_437 (Conv1D)         (None, 8, 5)                 105       ['dropout_475[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_437 (  (None, 8, 5)                 0         ['conv1d_437[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_436[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_438 (L  (None, 8, 5)                 10        ['tf.__operators__.add_437[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_219 (  (None, 8, 5)                 15300     ['layer_normalization_438[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_438[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_476 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_219[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_438 (  (None, 8, 5)                 0         ['dropout_476[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_437[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_439 (L  (None, 8, 5)                 10        ['tf.__operators__.add_438[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_438 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_439[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_477 (Dropout)       (None, 8, 20)                0         ['conv1d_438[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_439 (Conv1D)         (None, 8, 5)                 105       ['dropout_477[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_439 (  (None, 8, 5)                 0         ['conv1d_439[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_438[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_440 (L  (None, 8, 5)                 10        ['tf.__operators__.add_439[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_220 (  (None, 8, 5)                 15300     ['layer_normalization_440[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_440[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_478 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_220[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_440 (  (None, 8, 5)                 0         ['dropout_478[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_439[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_441 (L  (None, 8, 5)                 10        ['tf.__operators__.add_440[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_440 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_441[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_479 (Dropout)       (None, 8, 20)                0         ['conv1d_440[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_441 (Conv1D)         (None, 8, 5)                 105       ['dropout_479[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_441 (  (None, 8, 5)                 0         ['conv1d_441[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_440[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_442 (L  (None, 8, 5)                 10        ['tf.__operators__.add_441[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_221 (  (None, 8, 5)                 15300     ['layer_normalization_442[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_442[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_480 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_221[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_442 (  (None, 8, 5)                 0         ['dropout_480[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_441[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_443 (L  (None, 8, 5)                 10        ['tf.__operators__.add_442[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_442 (Conv1D)         (None, 8, 20)                120       ['layer_normalization_443[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_481 (Dropout)       (None, 8, 20)                0         ['conv1d_442[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_443 (Conv1D)         (None, 8, 5)                 105       ['dropout_481[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_443 (  (None, 8, 5)                 0         ['conv1d_443[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_442[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_443[0][\n",
      " 8 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_114 (Dense)           (None, 121)                  1089      ['global_average_pooling1d_38[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_482 (Dropout)       (None, 121)                  0         ['dense_114[0][0]']           \n",
      "                                                                                                  \n",
      " dense_115 (Dense)           (None, 16)                   1952      ['dropout_482[0][0]']         \n",
      "                                                                                                  \n",
      " dense_116 (Dense)           (None, 8)                    136       ['dense_115[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127537 (498.19 KB)\n",
      "Trainable params: 127537 (498.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/192\n",
      "20/20 [==============================] - 3s 54ms/step - loss: 2490.8120 - val_loss: 5375.6968\n",
      "Epoch 2/192\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1775.3201 - val_loss: 4500.4180\n",
      "Epoch 3/192\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1363.7347 - val_loss: 3943.3818\n",
      "Epoch 4/192\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1098.5010 - val_loss: 3427.6436\n",
      "Epoch 5/192\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 947.7679 - val_loss: 2839.2019\n",
      "Epoch 6/192\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 842.9005 - val_loss: 2363.2419\n",
      "Epoch 7/192\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 790.1717 - val_loss: 1937.2805\n",
      "Epoch 8/192\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 735.5089 - val_loss: 1675.4408\n",
      "Epoch 9/192\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 696.6363 - val_loss: 1432.2939\n",
      "Epoch 10/192\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 640.9270 - val_loss: 1126.1239\n",
      "Epoch 11/192\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 605.4675 - val_loss: 898.2228\n",
      "Epoch 12/192\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 560.5250 - val_loss: 805.2946\n",
      "Epoch 13/192\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 523.3025 - val_loss: 664.0444\n",
      "Epoch 14/192\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 498.9738 - val_loss: 552.2253\n",
      "Epoch 15/192\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 481.0253 - val_loss: 506.4003\n",
      "Epoch 16/192\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 468.3306 - val_loss: 488.5149\n",
      "Epoch 17/192\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 445.8620 - val_loss: 408.5269\n",
      "Epoch 18/192\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 454.1669 - val_loss: 371.1850\n",
      "Epoch 19/192\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 436.7127 - val_loss: 444.5256\n",
      "Epoch 20/192\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 425.7180 - val_loss: 420.4859\n",
      "Epoch 21/192\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 428.3664 - val_loss: 487.1819\n",
      "Epoch 22/192\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 409.7625 - val_loss: 434.2467\n",
      "Epoch 23/192\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 402.0320 - val_loss: 390.5652\n",
      "Epoch 24/192\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 398.0930 - val_loss: 406.3557\n",
      "Epoch 25/192\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 383.1814 - val_loss: 355.4115\n",
      "Epoch 26/192\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 367.3730 - val_loss: 414.9524\n",
      "Epoch 27/192\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 366.3849 - val_loss: 435.1982\n",
      "Epoch 28/192\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 357.8375 - val_loss: 445.1025\n",
      "Epoch 29/192\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 359.1768 - val_loss: 425.8127\n",
      "Epoch 30/192\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 345.0753 - val_loss: 481.7158\n",
      "Epoch 31/192\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 340.7657 - val_loss: 383.8683\n",
      "Epoch 32/192\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 326.0388 - val_loss: 369.8697\n",
      "Epoch 33/192\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 323.1493 - val_loss: 384.6643\n",
      "Epoch 34/192\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 316.3849 - val_loss: 346.2357\n",
      "Epoch 35/192\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 305.8121 - val_loss: 402.3738\n",
      "Epoch 36/192\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 306.9788 - val_loss: 375.3176\n",
      "Epoch 37/192\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 300.5535 - val_loss: 407.6572\n",
      "Epoch 38/192\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 295.2760 - val_loss: 362.3994\n",
      "Epoch 39/192\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 291.7727 - val_loss: 337.9651\n",
      "Epoch 40/192\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 285.7953 - val_loss: 359.8928\n",
      "Epoch 41/192\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 278.8764 - val_loss: 310.1849\n",
      "Epoch 42/192\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 276.6495 - val_loss: 377.5617\n",
      "Epoch 43/192\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 270.6365 - val_loss: 372.2342\n",
      "Epoch 44/192\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 266.9381 - val_loss: 360.6613\n",
      "Epoch 45/192\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 263.7348 - val_loss: 397.5936\n",
      "Epoch 46/192\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 255.3422 - val_loss: 338.3524\n",
      "Epoch 47/192\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 257.9607 - val_loss: 305.4322\n",
      "Epoch 48/192\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 253.2391 - val_loss: 375.8975\n",
      "Epoch 49/192\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 245.0451 - val_loss: 363.6660\n",
      "Epoch 50/192\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 243.6095 - val_loss: 334.6013\n",
      "Epoch 51/192\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 236.7485 - val_loss: 362.1553\n",
      "Epoch 52/192\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 237.7184 - val_loss: 356.1180\n",
      "Epoch 53/192\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 232.3604 - val_loss: 338.1714\n",
      "Epoch 54/192\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 227.1176 - val_loss: 339.3630\n",
      "Epoch 55/192\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 228.4865 - val_loss: 310.4768\n",
      "Epoch 56/192\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 221.9558 - val_loss: 364.7633\n",
      "Epoch 57/192\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 222.2383 - val_loss: 329.8799\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 877.9949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:14:55,921] Trial 38 finished with value: 877.994873046875 and parameters: {'head_size': 133, 'num_heads': 5, 'ff_dim': 20, 'num_transformer_blocks': 8, 'mlp_units': 121, 'mlp_dropout': 0.14300318410476964, 'dropout': 0.1335086233089689, 'learning_rate': 0.00024866867588620216, 'n_epochs': 192}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_39\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_40 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_444 (L  (None, 8, 5)                 10        ['input_40[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_222 (  (None, 8, 5)                 27789     ['layer_normalization_444[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_444[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_483 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_222[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_444 (  (None, 8, 5)                 0         ['dropout_483[0][0]',         \n",
      " TFOpLambda)                                                         'input_40[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_445 (L  (None, 8, 5)                 10        ['tf.__operators__.add_444[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_444 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_445[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_484 (Dropout)       (None, 8, 13)                0         ['conv1d_444[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_445 (Conv1D)         (None, 8, 5)                 70        ['dropout_484[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_445 (  (None, 8, 5)                 0         ['conv1d_445[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_444[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_446 (L  (None, 8, 5)                 10        ['tf.__operators__.add_445[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_223 (  (None, 8, 5)                 27789     ['layer_normalization_446[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_446[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_485 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_223[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_446 (  (None, 8, 5)                 0         ['dropout_485[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_445[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_447 (L  (None, 8, 5)                 10        ['tf.__operators__.add_446[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_446 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_447[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_486 (Dropout)       (None, 8, 13)                0         ['conv1d_446[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_447 (Conv1D)         (None, 8, 5)                 70        ['dropout_486[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_447 (  (None, 8, 5)                 0         ['conv1d_447[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_446[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_448 (L  (None, 8, 5)                 10        ['tf.__operators__.add_447[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_224 (  (None, 8, 5)                 27789     ['layer_normalization_448[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_448[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_487 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_224[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_448 (  (None, 8, 5)                 0         ['dropout_487[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_447[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_449 (L  (None, 8, 5)                 10        ['tf.__operators__.add_448[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_448 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_449[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_488 (Dropout)       (None, 8, 13)                0         ['conv1d_448[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_449 (Conv1D)         (None, 8, 5)                 70        ['dropout_488[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_449 (  (None, 8, 5)                 0         ['conv1d_449[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_448[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_450 (L  (None, 8, 5)                 10        ['tf.__operators__.add_449[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_225 (  (None, 8, 5)                 27789     ['layer_normalization_450[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_450[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_489 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_225[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_450 (  (None, 8, 5)                 0         ['dropout_489[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_449[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_451 (L  (None, 8, 5)                 10        ['tf.__operators__.add_450[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_450 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_451[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_490 (Dropout)       (None, 8, 13)                0         ['conv1d_450[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_451 (Conv1D)         (None, 8, 5)                 70        ['dropout_490[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_451 (  (None, 8, 5)                 0         ['conv1d_451[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_450[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_452 (L  (None, 8, 5)                 10        ['tf.__operators__.add_451[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_226 (  (None, 8, 5)                 27789     ['layer_normalization_452[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_452[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_491 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_226[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_452 (  (None, 8, 5)                 0         ['dropout_491[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_451[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_453 (L  (None, 8, 5)                 10        ['tf.__operators__.add_452[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_452 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_453[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_492 (Dropout)       (None, 8, 13)                0         ['conv1d_452[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_453 (Conv1D)         (None, 8, 5)                 70        ['dropout_492[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_453 (  (None, 8, 5)                 0         ['conv1d_453[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_452[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_454 (L  (None, 8, 5)                 10        ['tf.__operators__.add_453[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_227 (  (None, 8, 5)                 27789     ['layer_normalization_454[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_454[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_493 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_227[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_454 (  (None, 8, 5)                 0         ['dropout_493[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_453[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_455 (L  (None, 8, 5)                 10        ['tf.__operators__.add_454[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_454 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_455[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_494 (Dropout)       (None, 8, 13)                0         ['conv1d_454[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_455 (Conv1D)         (None, 8, 5)                 70        ['dropout_494[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_455 (  (None, 8, 5)                 0         ['conv1d_455[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_454[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 8)                    0         ['tf.__operators__.add_455[0][\n",
      " 9 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_117 (Dense)           (None, 99)                   891       ['global_average_pooling1d_39[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_495 (Dropout)       (None, 99)                   0         ['dense_117[0][0]']           \n",
      "                                                                                                  \n",
      " dense_118 (Dense)           (None, 16)                   1600      ['dropout_495[0][0]']         \n",
      "                                                                                                  \n",
      " dense_119 (Dense)           (None, 8)                    136       ['dense_118[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 170369 (665.50 KB)\n",
      "Trainable params: 170369 (665.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/110\n",
      "20/20 [==============================] - 3s 69ms/step - loss: 2977.6807 - val_loss: 7222.9595\n",
      "Epoch 2/110\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 2471.1975 - val_loss: 5965.6001\n",
      "Epoch 3/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 2081.3567 - val_loss: 5414.1851\n",
      "Epoch 4/110\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 1850.9850 - val_loss: 4837.4663\n",
      "Epoch 5/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 1663.4718 - val_loss: 4212.3164\n",
      "Epoch 6/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 1502.3733 - val_loss: 3755.5505\n",
      "Epoch 7/110\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 1342.5824 - val_loss: 3341.7390\n",
      "Epoch 8/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 1193.7048 - val_loss: 3110.7979\n",
      "Epoch 9/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 1091.9926 - val_loss: 2866.4905\n",
      "Epoch 10/110\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 1013.2432 - val_loss: 2577.6626\n",
      "Epoch 11/110\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 930.4118 - val_loss: 2362.1113\n",
      "Epoch 12/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 857.0983 - val_loss: 2172.4509\n",
      "Epoch 13/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 778.4757 - val_loss: 1973.3595\n",
      "Epoch 14/110\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 736.4813 - val_loss: 1680.2458\n",
      "Epoch 15/110\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 707.5399 - val_loss: 1484.3141\n",
      "Epoch 16/110\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 669.0109 - val_loss: 1291.1595\n",
      "Epoch 17/110\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 631.3080 - val_loss: 1097.9380\n",
      "Epoch 18/110\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 612.4595 - val_loss: 955.9137\n",
      "Epoch 19/110\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 585.6358 - val_loss: 809.7855\n",
      "Epoch 20/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 570.5225 - val_loss: 738.6743\n",
      "Epoch 21/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 553.9220 - val_loss: 637.0844\n",
      "Epoch 22/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 541.3647 - val_loss: 620.8977\n",
      "Epoch 23/110\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 523.0009 - val_loss: 598.6578\n",
      "Epoch 24/110\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 511.3279 - val_loss: 517.2151\n",
      "Epoch 25/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 511.9749 - val_loss: 490.6678\n",
      "Epoch 26/110\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 498.2286 - val_loss: 500.5540\n",
      "Epoch 27/110\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 503.4382 - val_loss: 485.4638\n",
      "Epoch 28/110\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 488.3330 - val_loss: 436.4553\n",
      "Epoch 29/110\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 486.3806 - val_loss: 422.3867\n",
      "Epoch 30/110\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 478.9820 - val_loss: 426.9548\n",
      "Epoch 31/110\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 460.8087 - val_loss: 416.1569\n",
      "Epoch 32/110\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 447.9445 - val_loss: 405.1523\n",
      "Epoch 33/110\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 453.6422 - val_loss: 396.5285\n",
      "Epoch 34/110\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 442.5780 - val_loss: 359.6314\n",
      "Epoch 35/110\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 441.3992 - val_loss: 330.2468\n",
      "Epoch 36/110\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 443.6394 - val_loss: 415.2290\n",
      "Epoch 37/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 438.8704 - val_loss: 377.0723\n",
      "Epoch 38/110\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 429.9452 - val_loss: 388.1266\n",
      "Epoch 39/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 419.9131 - val_loss: 402.7810\n",
      "Epoch 40/110\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 411.7348 - val_loss: 403.2690\n",
      "Epoch 41/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 407.1678 - val_loss: 337.0498\n",
      "Epoch 42/110\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 397.9731 - val_loss: 356.8202\n",
      "Epoch 43/110\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 401.4376 - val_loss: 349.2554\n",
      "Epoch 44/110\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 390.7923 - val_loss: 359.0799\n",
      "Epoch 45/110\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 388.8881 - val_loss: 391.8326\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1236.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:15:49,326] Trial 39 finished with value: 1236.0050048828125 and parameters: {'head_size': 151, 'num_heads': 8, 'ff_dim': 13, 'num_transformer_blocks': 6, 'mlp_units': 99, 'mlp_dropout': 0.11454990559799817, 'dropout': 0.15205445674327484, 'learning_rate': 0.00014290289489032673, 'n_epochs': 110}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_456 (L  (None, 8, 5)                 10        ['input_41[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_228 (  (None, 8, 5)                 38001     ['layer_normalization_456[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_456[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_496 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_228[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_456 (  (None, 8, 5)                 0         ['dropout_496[0][0]',         \n",
      " TFOpLambda)                                                         'input_41[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_457 (L  (None, 8, 5)                 10        ['tf.__operators__.add_456[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_456 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_457[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_497 (Dropout)       (None, 8, 17)                0         ['conv1d_456[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_457 (Conv1D)         (None, 8, 5)                 90        ['dropout_497[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_457 (  (None, 8, 5)                 0         ['conv1d_457[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_456[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_458 (L  (None, 8, 5)                 10        ['tf.__operators__.add_457[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_229 (  (None, 8, 5)                 38001     ['layer_normalization_458[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_458[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_498 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_229[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_458 (  (None, 8, 5)                 0         ['dropout_498[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_457[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_459 (L  (None, 8, 5)                 10        ['tf.__operators__.add_458[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_458 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_459[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_499 (Dropout)       (None, 8, 17)                0         ['conv1d_458[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_459 (Conv1D)         (None, 8, 5)                 90        ['dropout_499[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_459 (  (None, 8, 5)                 0         ['conv1d_459[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_458[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_460 (L  (None, 8, 5)                 10        ['tf.__operators__.add_459[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_230 (  (None, 8, 5)                 38001     ['layer_normalization_460[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_460[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_500 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_230[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_460 (  (None, 8, 5)                 0         ['dropout_500[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_459[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_461 (L  (None, 8, 5)                 10        ['tf.__operators__.add_460[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_460 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_461[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_501 (Dropout)       (None, 8, 17)                0         ['conv1d_460[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_461 (Conv1D)         (None, 8, 5)                 90        ['dropout_501[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_461 (  (None, 8, 5)                 0         ['conv1d_461[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_460[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_462 (L  (None, 8, 5)                 10        ['tf.__operators__.add_461[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_231 (  (None, 8, 5)                 38001     ['layer_normalization_462[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_462[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_502 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_231[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_462 (  (None, 8, 5)                 0         ['dropout_502[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_461[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_463 (L  (None, 8, 5)                 10        ['tf.__operators__.add_462[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_462 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_463[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_503 (Dropout)       (None, 8, 17)                0         ['conv1d_462[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_463 (Conv1D)         (None, 8, 5)                 90        ['dropout_503[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_463 (  (None, 8, 5)                 0         ['conv1d_463[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_462[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_464 (L  (None, 8, 5)                 10        ['tf.__operators__.add_463[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_232 (  (None, 8, 5)                 38001     ['layer_normalization_464[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_464[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_504 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_232[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_464 (  (None, 8, 5)                 0         ['dropout_504[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_463[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_465 (L  (None, 8, 5)                 10        ['tf.__operators__.add_464[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_464 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_465[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_505 (Dropout)       (None, 8, 17)                0         ['conv1d_464[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_465 (Conv1D)         (None, 8, 5)                 90        ['dropout_505[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_465 (  (None, 8, 5)                 0         ['conv1d_465[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_464[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_466 (L  (None, 8, 5)                 10        ['tf.__operators__.add_465[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_233 (  (None, 8, 5)                 38001     ['layer_normalization_466[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_466[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_506 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_233[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_466 (  (None, 8, 5)                 0         ['dropout_506[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_465[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_467 (L  (None, 8, 5)                 10        ['tf.__operators__.add_466[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_466 (Conv1D)         (None, 8, 17)                102       ['layer_normalization_467[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_507 (Dropout)       (None, 8, 17)                0         ['conv1d_466[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_467 (Conv1D)         (None, 8, 5)                 90        ['dropout_507[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_467 (  (None, 8, 5)                 0         ['conv1d_467[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_466[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_467[0][\n",
      " 0 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_120 (Dense)           (None, 117)                  1053      ['global_average_pooling1d_40[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_508 (Dropout)       (None, 117)                  0         ['dense_120[0][0]']           \n",
      "                                                                                                  \n",
      " dense_121 (Dense)           (None, 16)                   1888      ['dropout_508[0][0]']         \n",
      "                                                                                                  \n",
      " dense_122 (Dense)           (None, 8)                    136       ['dense_121[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 232355 (907.64 KB)\n",
      "Trainable params: 232355 (907.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/199\n",
      "20/20 [==============================] - 3s 66ms/step - loss: 1880.3877 - val_loss: 3552.3528\n",
      "Epoch 2/199\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 1000.8817 - val_loss: 2639.0859\n",
      "Epoch 3/199\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 801.4496 - val_loss: 1553.5518\n",
      "Epoch 4/199\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 706.5617 - val_loss: 1020.9677\n",
      "Epoch 5/199\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 639.0701 - val_loss: 880.1788\n",
      "Epoch 6/199\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 591.6193 - val_loss: 736.3847\n",
      "Epoch 7/199\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 573.9890 - val_loss: 521.3878\n",
      "Epoch 8/199\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 540.6198 - val_loss: 360.3207\n",
      "Epoch 9/199\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 517.1807 - val_loss: 471.4662\n",
      "Epoch 10/199\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 493.5630 - val_loss: 450.5999\n",
      "Epoch 11/199\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 487.3000 - val_loss: 408.4771\n",
      "Epoch 12/199\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 461.9386 - val_loss: 424.4373\n",
      "Epoch 13/199\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 430.4355 - val_loss: 413.1162\n",
      "Epoch 14/199\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 424.9629 - val_loss: 390.0648\n",
      "Epoch 15/199\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 411.8559 - val_loss: 377.6531\n",
      "Epoch 16/199\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 396.7702 - val_loss: 471.9718\n",
      "Epoch 17/199\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 383.3347 - val_loss: 366.5111\n",
      "Epoch 18/199\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 366.6471 - val_loss: 427.6773\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1383.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:16:11,204] Trial 40 finished with value: 1383.1171875 and parameters: {'head_size': 236, 'num_heads': 7, 'ff_dim': 17, 'num_transformer_blocks': 6, 'mlp_units': 117, 'mlp_dropout': 0.18886672384166975, 'dropout': 0.18261390693888505, 'learning_rate': 0.00041133016216667577, 'n_epochs': 199}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_41\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_468 (L  (None, 8, 5)                 10        ['input_42[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_234 (  (None, 8, 5)                 27375     ['layer_normalization_468[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_468[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_509 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_234[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_468 (  (None, 8, 5)                 0         ['dropout_509[0][0]',         \n",
      " TFOpLambda)                                                         'input_42[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_469 (L  (None, 8, 5)                 10        ['tf.__operators__.add_468[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_468 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_469[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_510 (Dropout)       (None, 8, 15)                0         ['conv1d_468[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_469 (Conv1D)         (None, 8, 5)                 80        ['dropout_510[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_469 (  (None, 8, 5)                 0         ['conv1d_469[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_468[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_470 (L  (None, 8, 5)                 10        ['tf.__operators__.add_469[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_235 (  (None, 8, 5)                 27375     ['layer_normalization_470[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_470[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_511 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_235[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_470 (  (None, 8, 5)                 0         ['dropout_511[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_469[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_471 (L  (None, 8, 5)                 10        ['tf.__operators__.add_470[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_470 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_471[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_512 (Dropout)       (None, 8, 15)                0         ['conv1d_470[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_471 (Conv1D)         (None, 8, 5)                 80        ['dropout_512[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_471 (  (None, 8, 5)                 0         ['conv1d_471[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_470[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_472 (L  (None, 8, 5)                 10        ['tf.__operators__.add_471[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_236 (  (None, 8, 5)                 27375     ['layer_normalization_472[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_472[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_513 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_236[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_472 (  (None, 8, 5)                 0         ['dropout_513[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_471[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_473 (L  (None, 8, 5)                 10        ['tf.__operators__.add_472[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_472 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_473[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_514 (Dropout)       (None, 8, 15)                0         ['conv1d_472[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_473 (Conv1D)         (None, 8, 5)                 80        ['dropout_514[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_473 (  (None, 8, 5)                 0         ['conv1d_473[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_472[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_474 (L  (None, 8, 5)                 10        ['tf.__operators__.add_473[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_237 (  (None, 8, 5)                 27375     ['layer_normalization_474[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_474[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_515 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_237[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_474 (  (None, 8, 5)                 0         ['dropout_515[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_473[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_475 (L  (None, 8, 5)                 10        ['tf.__operators__.add_474[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_474 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_475[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_516 (Dropout)       (None, 8, 15)                0         ['conv1d_474[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_475 (Conv1D)         (None, 8, 5)                 80        ['dropout_516[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_475 (  (None, 8, 5)                 0         ['conv1d_475[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_474[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_475[0][\n",
      " 1 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_123 (Dense)           (None, 110)                  990       ['global_average_pooling1d_41[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_517 (Dropout)       (None, 110)                  0         ['dense_123[0][0]']           \n",
      "                                                                                                  \n",
      " dense_124 (Dense)           (None, 16)                   1776      ['dropout_517[0][0]']         \n",
      "                                                                                                  \n",
      " dense_125 (Dense)           (None, 8)                    136       ['dense_124[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 113162 (442.04 KB)\n",
      "Trainable params: 113162 (442.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/172\n",
      "20/20 [==============================] - 2s 44ms/step - loss: 2248.1348 - val_loss: 4349.5591\n",
      "Epoch 2/172\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 1355.2190 - val_loss: 2872.3010\n",
      "Epoch 3/172\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 820.6602 - val_loss: 746.4621\n",
      "Epoch 4/172\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 666.1089 - val_loss: 695.4823\n",
      "Epoch 5/172\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 606.5789 - val_loss: 640.7391\n",
      "Epoch 6/172\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 554.6166 - val_loss: 533.0963\n",
      "Epoch 7/172\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 518.2429 - val_loss: 535.7081\n",
      "Epoch 8/172\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 484.1466 - val_loss: 575.8409\n",
      "Epoch 9/172\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 450.2505 - val_loss: 494.2668\n",
      "Epoch 10/172\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 422.4641 - val_loss: 388.0870\n",
      "Epoch 11/172\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 401.5483 - val_loss: 550.0856\n",
      "Epoch 12/172\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 384.6650 - val_loss: 514.8047\n",
      "Epoch 13/172\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 354.2807 - val_loss: 513.9971\n",
      "Epoch 14/172\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 342.5732 - val_loss: 511.8531\n",
      "Epoch 15/172\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 320.1161 - val_loss: 420.5929\n",
      "Epoch 16/172\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 305.9836 - val_loss: 464.8222\n",
      "Epoch 17/172\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 298.3709 - val_loss: 542.3857\n",
      "Epoch 18/172\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 281.9830 - val_loss: 474.6840\n",
      "Epoch 19/172\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 264.0567 - val_loss: 455.4618\n",
      "Epoch 20/172\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 259.4181 - val_loss: 465.4131\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1412.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:16:27,378] Trial 41 finished with value: 1412.4036865234375 and parameters: {'head_size': 170, 'num_heads': 7, 'ff_dim': 15, 'num_transformer_blocks': 4, 'mlp_units': 110, 'mlp_dropout': 0.154723900202282, 'dropout': 0.1167627023995682, 'learning_rate': 0.0006716992599506747, 'n_epochs': 172}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_43 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_476 (L  (None, 8, 5)                 10        ['input_43[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_238 (  (None, 8, 5)                 30273     ['layer_normalization_476[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_476[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_518 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_238[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_476 (  (None, 8, 5)                 0         ['dropout_518[0][0]',         \n",
      " TFOpLambda)                                                         'input_43[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_477 (L  (None, 8, 5)                 10        ['tf.__operators__.add_476[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_476 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_477[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_519 (Dropout)       (None, 8, 13)                0         ['conv1d_476[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_477 (Conv1D)         (None, 8, 5)                 70        ['dropout_519[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_477 (  (None, 8, 5)                 0         ['conv1d_477[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_476[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_478 (L  (None, 8, 5)                 10        ['tf.__operators__.add_477[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_239 (  (None, 8, 5)                 30273     ['layer_normalization_478[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_478[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_520 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_239[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_478 (  (None, 8, 5)                 0         ['dropout_520[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_477[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_479 (L  (None, 8, 5)                 10        ['tf.__operators__.add_478[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_478 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_479[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_521 (Dropout)       (None, 8, 13)                0         ['conv1d_478[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_479 (Conv1D)         (None, 8, 5)                 70        ['dropout_521[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_479 (  (None, 8, 5)                 0         ['conv1d_479[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_478[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_480 (L  (None, 8, 5)                 10        ['tf.__operators__.add_479[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_240 (  (None, 8, 5)                 30273     ['layer_normalization_480[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_480[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_522 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_240[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_480 (  (None, 8, 5)                 0         ['dropout_522[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_479[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_481 (L  (None, 8, 5)                 10        ['tf.__operators__.add_480[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_480 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_481[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_523 (Dropout)       (None, 8, 13)                0         ['conv1d_480[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_481 (Conv1D)         (None, 8, 5)                 70        ['dropout_523[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_481 (  (None, 8, 5)                 0         ['conv1d_481[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_480[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_482 (L  (None, 8, 5)                 10        ['tf.__operators__.add_481[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_241 (  (None, 8, 5)                 30273     ['layer_normalization_482[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_482[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_524 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_241[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_482 (  (None, 8, 5)                 0         ['dropout_524[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_481[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_483 (L  (None, 8, 5)                 10        ['tf.__operators__.add_482[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_482 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_483[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_525 (Dropout)       (None, 8, 13)                0         ['conv1d_482[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_483 (Conv1D)         (None, 8, 5)                 70        ['dropout_525[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_483 (  (None, 8, 5)                 0         ['conv1d_483[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_482[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_484 (L  (None, 8, 5)                 10        ['tf.__operators__.add_483[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_242 (  (None, 8, 5)                 30273     ['layer_normalization_484[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_484[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_526 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_242[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_484 (  (None, 8, 5)                 0         ['dropout_526[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_483[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_485 (L  (None, 8, 5)                 10        ['tf.__operators__.add_484[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_484 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_485[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_527 (Dropout)       (None, 8, 13)                0         ['conv1d_484[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_485 (Conv1D)         (None, 8, 5)                 70        ['dropout_527[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_485 (  (None, 8, 5)                 0         ['conv1d_485[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_484[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_485[0][\n",
      " 2 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_126 (Dense)           (None, 105)                  945       ['global_average_pooling1d_42[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_528 (Dropout)       (None, 105)                  0         ['dense_126[0][0]']           \n",
      "                                                                                                  \n",
      " dense_127 (Dense)           (None, 16)                   1696      ['dropout_528[0][0]']         \n",
      "                                                                                                  \n",
      " dense_128 (Dense)           (None, 8)                    136       ['dense_127[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 154982 (605.40 KB)\n",
      "Trainable params: 154982 (605.40 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/182\n",
      "20/20 [==============================] - 2s 56ms/step - loss: 1787.8457 - val_loss: 3671.1108\n",
      "Epoch 2/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 947.2537 - val_loss: 1143.1841\n",
      "Epoch 3/182\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 650.6501 - val_loss: 1205.6726\n",
      "Epoch 4/182\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 564.6998 - val_loss: 615.2264\n",
      "Epoch 5/182\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 508.4162 - val_loss: 488.3588\n",
      "Epoch 6/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 468.9840 - val_loss: 520.9954\n",
      "Epoch 7/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 445.8040 - val_loss: 517.6544\n",
      "Epoch 8/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 412.8536 - val_loss: 603.2708\n",
      "Epoch 9/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 388.2397 - val_loss: 476.1213\n",
      "Epoch 10/182\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 370.9605 - val_loss: 348.9570\n",
      "Epoch 11/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 350.8248 - val_loss: 559.8250\n",
      "Epoch 12/182\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 339.7629 - val_loss: 387.8103\n",
      "Epoch 13/182\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 318.4287 - val_loss: 376.5647\n",
      "Epoch 14/182\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 305.6525 - val_loss: 382.6727\n",
      "Epoch 15/182\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 297.3392 - val_loss: 346.0554\n",
      "Epoch 16/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 276.4033 - val_loss: 364.2497\n",
      "Epoch 17/182\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 277.0388 - val_loss: 456.1832\n",
      "Epoch 18/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 255.5220 - val_loss: 418.6281\n",
      "Epoch 19/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 256.2968 - val_loss: 443.6533\n",
      "Epoch 20/182\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 241.8851 - val_loss: 347.9632\n",
      "Epoch 21/182\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 236.8086 - val_loss: 320.6840\n",
      "Epoch 22/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 227.9928 - val_loss: 296.3350\n",
      "Epoch 23/182\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 229.2565 - val_loss: 396.8430\n",
      "Epoch 24/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 213.5465 - val_loss: 324.5053\n",
      "Epoch 25/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 215.3635 - val_loss: 298.1516\n",
      "Epoch 26/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 212.6630 - val_loss: 285.0198\n",
      "Epoch 27/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 197.9416 - val_loss: 319.4887\n",
      "Epoch 28/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 198.3369 - val_loss: 282.6142\n",
      "Epoch 29/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 194.4455 - val_loss: 319.5719\n",
      "Epoch 30/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 188.5093 - val_loss: 341.7144\n",
      "Epoch 31/182\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 189.6410 - val_loss: 374.8075\n",
      "Epoch 32/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 183.5753 - val_loss: 272.5706\n",
      "Epoch 33/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 175.0564 - val_loss: 349.8405\n",
      "Epoch 34/182\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 175.6179 - val_loss: 265.7711\n",
      "Epoch 35/182\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 173.3325 - val_loss: 268.6183\n",
      "Epoch 36/182\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 173.2666 - val_loss: 324.3787\n",
      "Epoch 37/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 165.8031 - val_loss: 283.0616\n",
      "Epoch 38/182\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 164.2459 - val_loss: 299.9990\n",
      "Epoch 39/182\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 162.9806 - val_loss: 370.5846\n",
      "Epoch 40/182\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 160.5285 - val_loss: 296.3791\n",
      "Epoch 41/182\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 163.0787 - val_loss: 350.0297\n",
      "Epoch 42/182\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 156.2937 - val_loss: 286.9548\n",
      "Epoch 43/182\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 153.0268 - val_loss: 350.2499\n",
      "Epoch 44/182\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 156.6407 - val_loss: 390.2157\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 850.5519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:17:13,213] Trial 42 finished with value: 850.5518798828125 and parameters: {'head_size': 188, 'num_heads': 7, 'ff_dim': 13, 'num_transformer_blocks': 5, 'mlp_units': 105, 'mlp_dropout': 0.14478292606517748, 'dropout': 0.10081370394167138, 'learning_rate': 0.0006461267088669076, 'n_epochs': 182}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_43\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_44 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_486 (L  (None, 8, 5)                 10        ['input_44[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_243 (  (None, 8, 5)                 26501     ['layer_normalization_486[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_486[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_529 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_243[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_486 (  (None, 8, 5)                 0         ['dropout_529[0][0]',         \n",
      " TFOpLambda)                                                         'input_44[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_487 (L  (None, 8, 5)                 10        ['tf.__operators__.add_486[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_486 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_487[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_530 (Dropout)       (None, 8, 13)                0         ['conv1d_486[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_487 (Conv1D)         (None, 8, 5)                 70        ['dropout_530[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_487 (  (None, 8, 5)                 0         ['conv1d_487[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_486[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_488 (L  (None, 8, 5)                 10        ['tf.__operators__.add_487[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_244 (  (None, 8, 5)                 26501     ['layer_normalization_488[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_488[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_531 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_244[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_488 (  (None, 8, 5)                 0         ['dropout_531[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_487[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_489 (L  (None, 8, 5)                 10        ['tf.__operators__.add_488[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_488 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_489[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_532 (Dropout)       (None, 8, 13)                0         ['conv1d_488[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_489 (Conv1D)         (None, 8, 5)                 70        ['dropout_532[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_489 (  (None, 8, 5)                 0         ['conv1d_489[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_488[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_490 (L  (None, 8, 5)                 10        ['tf.__operators__.add_489[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_245 (  (None, 8, 5)                 26501     ['layer_normalization_490[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_490[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_533 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_245[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_490 (  (None, 8, 5)                 0         ['dropout_533[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_489[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_491 (L  (None, 8, 5)                 10        ['tf.__operators__.add_490[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_490 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_491[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_534 (Dropout)       (None, 8, 13)                0         ['conv1d_490[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_491 (Conv1D)         (None, 8, 5)                 70        ['dropout_534[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_491 (  (None, 8, 5)                 0         ['conv1d_491[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_490[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_492 (L  (None, 8, 5)                 10        ['tf.__operators__.add_491[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_246 (  (None, 8, 5)                 26501     ['layer_normalization_492[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_492[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_535 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_246[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_492 (  (None, 8, 5)                 0         ['dropout_535[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_491[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_493 (L  (None, 8, 5)                 10        ['tf.__operators__.add_492[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_492 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_493[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_536 (Dropout)       (None, 8, 13)                0         ['conv1d_492[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_493 (Conv1D)         (None, 8, 5)                 70        ['dropout_536[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_493 (  (None, 8, 5)                 0         ['conv1d_493[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_492[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_494 (L  (None, 8, 5)                 10        ['tf.__operators__.add_493[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_247 (  (None, 8, 5)                 26501     ['layer_normalization_494[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_494[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_537 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_247[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_494 (  (None, 8, 5)                 0         ['dropout_537[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_493[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_495 (L  (None, 8, 5)                 10        ['tf.__operators__.add_494[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_494 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_495[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_538 (Dropout)       (None, 8, 13)                0         ['conv1d_494[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_495 (Conv1D)         (None, 8, 5)                 70        ['dropout_538[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_495 (  (None, 8, 5)                 0         ['conv1d_495[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_494[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_495[0][\n",
      " 3 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_129 (Dense)           (None, 114)                  1026      ['global_average_pooling1d_43[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_539 (Dropout)       (None, 114)                  0         ['dense_129[0][0]']           \n",
      "                                                                                                  \n",
      " dense_130 (Dense)           (None, 16)                   1840      ['dropout_539[0][0]']         \n",
      "                                                                                                  \n",
      " dense_131 (Dense)           (None, 8)                    136       ['dense_130[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136347 (532.61 KB)\n",
      "Trainable params: 136347 (532.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/183\n",
      "20/20 [==============================] - 2s 54ms/step - loss: 1664.8267 - val_loss: 3346.2668\n",
      "Epoch 2/183\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 1066.1091 - val_loss: 2169.6499\n",
      "Epoch 3/183\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 706.9061 - val_loss: 846.7656\n",
      "Epoch 4/183\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 568.3028 - val_loss: 688.8000\n",
      "Epoch 5/183\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 508.1436 - val_loss: 656.2262\n",
      "Epoch 6/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 483.0124 - val_loss: 408.1778\n",
      "Epoch 7/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 450.2776 - val_loss: 478.0236\n",
      "Epoch 8/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 425.5575 - val_loss: 450.9628\n",
      "Epoch 9/183\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 401.1417 - val_loss: 565.1993\n",
      "Epoch 10/183\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 376.1091 - val_loss: 365.5105\n",
      "Epoch 11/183\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 360.4354 - val_loss: 367.3518\n",
      "Epoch 12/183\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 339.3828 - val_loss: 389.5226\n",
      "Epoch 13/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 332.5553 - val_loss: 310.4752\n",
      "Epoch 14/183\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 310.0816 - val_loss: 454.6079\n",
      "Epoch 15/183\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 292.3687 - val_loss: 295.8823\n",
      "Epoch 16/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 286.8537 - val_loss: 299.3527\n",
      "Epoch 17/183\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 278.3418 - val_loss: 402.9143\n",
      "Epoch 18/183\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 267.8811 - val_loss: 304.0551\n",
      "Epoch 19/183\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 255.5231 - val_loss: 463.3214\n",
      "Epoch 20/183\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 249.0750 - val_loss: 323.5869\n",
      "Epoch 21/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 242.2524 - val_loss: 319.1198\n",
      "Epoch 22/183\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 230.4584 - val_loss: 305.6240\n",
      "Epoch 23/183\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 227.3613 - val_loss: 343.5624\n",
      "Epoch 24/183\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 218.7217 - val_loss: 324.4319\n",
      "Epoch 25/183\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 212.0412 - val_loss: 354.6414\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 975.4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:17:37,657] Trial 43 finished with value: 975.4404907226562 and parameters: {'head_size': 192, 'num_heads': 6, 'ff_dim': 13, 'num_transformer_blocks': 5, 'mlp_units': 114, 'mlp_dropout': 0.12616096189935577, 'dropout': 0.13049369472765435, 'learning_rate': 0.0006285156344617336, 'n_epochs': 183}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_44\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_496 (L  (None, 8, 5)                 10        ['input_45[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_248 (  (None, 8, 5)                 25949     ['layer_normalization_496[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_496[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_540 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_248[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_496 (  (None, 8, 5)                 0         ['dropout_540[0][0]',         \n",
      " TFOpLambda)                                                         'input_45[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_497 (L  (None, 8, 5)                 10        ['tf.__operators__.add_496[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_496 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_497[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_541 (Dropout)       (None, 8, 12)                0         ['conv1d_496[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_497 (Conv1D)         (None, 8, 5)                 65        ['dropout_541[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_497 (  (None, 8, 5)                 0         ['conv1d_497[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_496[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_498 (L  (None, 8, 5)                 10        ['tf.__operators__.add_497[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_249 (  (None, 8, 5)                 25949     ['layer_normalization_498[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_498[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_542 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_249[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_498 (  (None, 8, 5)                 0         ['dropout_542[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_497[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_499 (L  (None, 8, 5)                 10        ['tf.__operators__.add_498[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_498 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_499[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_543 (Dropout)       (None, 8, 12)                0         ['conv1d_498[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_499 (Conv1D)         (None, 8, 5)                 65        ['dropout_543[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_499 (  (None, 8, 5)                 0         ['conv1d_499[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_498[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_500 (L  (None, 8, 5)                 10        ['tf.__operators__.add_499[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_250 (  (None, 8, 5)                 25949     ['layer_normalization_500[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_500[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_544 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_250[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_500 (  (None, 8, 5)                 0         ['dropout_544[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_499[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_501 (L  (None, 8, 5)                 10        ['tf.__operators__.add_500[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_500 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_501[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_545 (Dropout)       (None, 8, 12)                0         ['conv1d_500[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_501 (Conv1D)         (None, 8, 5)                 65        ['dropout_545[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_501 (  (None, 8, 5)                 0         ['conv1d_501[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_500[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_502 (L  (None, 8, 5)                 10        ['tf.__operators__.add_501[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_251 (  (None, 8, 5)                 25949     ['layer_normalization_502[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_502[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_546 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_251[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_502 (  (None, 8, 5)                 0         ['dropout_546[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_501[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_503 (L  (None, 8, 5)                 10        ['tf.__operators__.add_502[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_502 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_503[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_547 (Dropout)       (None, 8, 12)                0         ['conv1d_502[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_503 (Conv1D)         (None, 8, 5)                 65        ['dropout_547[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_503 (  (None, 8, 5)                 0         ['conv1d_503[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_502[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_504 (L  (None, 8, 5)                 10        ['tf.__operators__.add_503[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_252 (  (None, 8, 5)                 25949     ['layer_normalization_504[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_504[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_548 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_252[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_504 (  (None, 8, 5)                 0         ['dropout_548[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_503[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_505 (L  (None, 8, 5)                 10        ['tf.__operators__.add_504[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_504 (Conv1D)         (None, 8, 12)                72        ['layer_normalization_505[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_549 (Dropout)       (None, 8, 12)                0         ['conv1d_504[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_505 (Conv1D)         (None, 8, 5)                 65        ['dropout_549[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_505 (  (None, 8, 5)                 0         ['conv1d_505[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_504[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_505[0][\n",
      " 4 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_132 (Dense)           (None, 106)                  954       ['global_average_pooling1d_44[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_550 (Dropout)       (None, 106)                  0         ['dense_132[0][0]']           \n",
      "                                                                                                  \n",
      " dense_133 (Dense)           (None, 16)                   1712      ['dropout_550[0][0]']         \n",
      "                                                                                                  \n",
      " dense_134 (Dense)           (None, 8)                    136       ['dense_133[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 133332 (520.83 KB)\n",
      "Trainable params: 133332 (520.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/194\n",
      "20/20 [==============================] - 2s 54ms/step - loss: 3172.7112 - val_loss: 6124.5781\n",
      "Epoch 2/194\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1905.9146 - val_loss: 3468.1938\n",
      "Epoch 3/194\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 1243.2244 - val_loss: 2297.0725\n",
      "Epoch 4/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 845.9830 - val_loss: 1304.2631\n",
      "Epoch 5/194\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 712.9039 - val_loss: 652.6605\n",
      "Epoch 6/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 653.0643 - val_loss: 713.2133\n",
      "Epoch 7/194\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 623.2917 - val_loss: 615.1964\n",
      "Epoch 8/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 585.9912 - val_loss: 664.7563\n",
      "Epoch 9/194\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 564.1523 - val_loss: 569.9839\n",
      "Epoch 10/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 552.3400 - val_loss: 539.3854\n",
      "Epoch 11/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 517.9011 - val_loss: 505.5582\n",
      "Epoch 12/194\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 485.2144 - val_loss: 624.3349\n",
      "Epoch 13/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 479.4697 - val_loss: 546.3929\n",
      "Epoch 14/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 447.2746 - val_loss: 505.2591\n",
      "Epoch 15/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 438.5797 - val_loss: 620.3903\n",
      "Epoch 16/194\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 420.9713 - val_loss: 547.4517\n",
      "Epoch 17/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 399.7457 - val_loss: 580.4984\n",
      "Epoch 18/194\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 387.6722 - val_loss: 592.4847\n",
      "Epoch 19/194\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 367.9405 - val_loss: 608.2433\n",
      "Epoch 20/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 357.9171 - val_loss: 482.9945\n",
      "Epoch 21/194\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 348.4600 - val_loss: 439.3887\n",
      "Epoch 22/194\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 344.1884 - val_loss: 443.1703\n",
      "Epoch 23/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 319.3044 - val_loss: 477.7693\n",
      "Epoch 24/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 320.2731 - val_loss: 566.1368\n",
      "Epoch 25/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 305.8737 - val_loss: 430.8713\n",
      "Epoch 26/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 292.3257 - val_loss: 446.9372\n",
      "Epoch 27/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 287.2096 - val_loss: 415.4332\n",
      "Epoch 28/194\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 276.3818 - val_loss: 460.9457\n",
      "Epoch 29/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 270.1753 - val_loss: 424.6228\n",
      "Epoch 30/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 266.4373 - val_loss: 412.3713\n",
      "Epoch 31/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 261.3958 - val_loss: 526.7737\n",
      "Epoch 32/194\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 251.9790 - val_loss: 473.8370\n",
      "Epoch 33/194\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 252.7272 - val_loss: 403.8443\n",
      "Epoch 34/194\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 240.3042 - val_loss: 364.2434\n",
      "Epoch 35/194\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 235.7308 - val_loss: 486.8555\n",
      "Epoch 36/194\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 232.9256 - val_loss: 382.1494\n",
      "Epoch 37/194\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 221.5504 - val_loss: 402.9255\n",
      "Epoch 38/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 216.8693 - val_loss: 323.5024\n",
      "Epoch 39/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 218.9623 - val_loss: 424.2976\n",
      "Epoch 40/194\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 216.1213 - val_loss: 370.6154\n",
      "Epoch 41/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 206.3270 - val_loss: 471.7913\n",
      "Epoch 42/194\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 208.0613 - val_loss: 382.7075\n",
      "Epoch 43/194\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 201.8889 - val_loss: 384.4481\n",
      "Epoch 44/194\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 201.8132 - val_loss: 343.0899\n",
      "Epoch 45/194\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 200.7233 - val_loss: 329.9658\n",
      "Epoch 46/194\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 201.7554 - val_loss: 353.2526\n",
      "Epoch 47/194\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 192.3831 - val_loss: 377.0995\n",
      "Epoch 48/194\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 194.1801 - val_loss: 378.2805\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 946.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:18:24,391] Trial 44 finished with value: 946.8920288085938 and parameters: {'head_size': 141, 'num_heads': 8, 'ff_dim': 12, 'num_transformer_blocks': 5, 'mlp_units': 106, 'mlp_dropout': 0.16661332836631643, 'dropout': 0.1097544456884142, 'learning_rate': 0.000497471422370795, 'n_epochs': 194}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_45\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_46 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_506 (L  (None, 8, 5)                 10        ['input_46[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_253 (  (None, 8, 5)                 27881     ['layer_normalization_506[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_506[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_551 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_253[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_506 (  (None, 8, 5)                 0         ['dropout_551[0][0]',         \n",
      " TFOpLambda)                                                         'input_46[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_507 (L  (None, 8, 5)                 10        ['tf.__operators__.add_506[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_506 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_507[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_552 (Dropout)       (None, 8, 15)                0         ['conv1d_506[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_507 (Conv1D)         (None, 8, 5)                 80        ['dropout_552[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_507 (  (None, 8, 5)                 0         ['conv1d_507[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_506[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_508 (L  (None, 8, 5)                 10        ['tf.__operators__.add_507[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_254 (  (None, 8, 5)                 27881     ['layer_normalization_508[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_508[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_553 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_254[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_508 (  (None, 8, 5)                 0         ['dropout_553[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_507[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_509 (L  (None, 8, 5)                 10        ['tf.__operators__.add_508[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_508 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_509[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_554 (Dropout)       (None, 8, 15)                0         ['conv1d_508[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_509 (Conv1D)         (None, 8, 5)                 80        ['dropout_554[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_509 (  (None, 8, 5)                 0         ['conv1d_509[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_508[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_510 (L  (None, 8, 5)                 10        ['tf.__operators__.add_509[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_255 (  (None, 8, 5)                 27881     ['layer_normalization_510[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_510[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_555 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_255[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_510 (  (None, 8, 5)                 0         ['dropout_555[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_509[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_511 (L  (None, 8, 5)                 10        ['tf.__operators__.add_510[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_510 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_511[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_556 (Dropout)       (None, 8, 15)                0         ['conv1d_510[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_511 (Conv1D)         (None, 8, 5)                 80        ['dropout_556[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_511 (  (None, 8, 5)                 0         ['conv1d_511[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_510[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_512 (L  (None, 8, 5)                 10        ['tf.__operators__.add_511[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_256 (  (None, 8, 5)                 27881     ['layer_normalization_512[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_512[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_557 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_256[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_512 (  (None, 8, 5)                 0         ['dropout_557[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_511[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_513 (L  (None, 8, 5)                 10        ['tf.__operators__.add_512[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_512 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_513[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_558 (Dropout)       (None, 8, 15)                0         ['conv1d_512[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_513 (Conv1D)         (None, 8, 5)                 80        ['dropout_558[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_513 (  (None, 8, 5)                 0         ['conv1d_513[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_512[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_514 (L  (None, 8, 5)                 10        ['tf.__operators__.add_513[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_257 (  (None, 8, 5)                 27881     ['layer_normalization_514[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_514[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_559 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_257[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_514 (  (None, 8, 5)                 0         ['dropout_559[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_513[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_515 (L  (None, 8, 5)                 10        ['tf.__operators__.add_514[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_514 (Conv1D)         (None, 8, 15)                90        ['layer_normalization_515[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_560 (Dropout)       (None, 8, 15)                0         ['conv1d_514[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_515 (Conv1D)         (None, 8, 5)                 80        ['dropout_560[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_515 (  (None, 8, 5)                 0         ['conv1d_515[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_514[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_515[0][\n",
      " 5 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_135 (Dense)           (None, 98)                   882       ['global_average_pooling1d_45[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_561 (Dropout)       (None, 98)                   0         ['dense_135[0][0]']           \n",
      "                                                                                                  \n",
      " dense_136 (Dense)           (None, 16)                   1584      ['dropout_561[0][0]']         \n",
      "                                                                                                  \n",
      " dense_137 (Dense)           (None, 8)                    136       ['dense_136[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142957 (558.43 KB)\n",
      "Trainable params: 142957 (558.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/66\n",
      "20/20 [==============================] - 2s 55ms/step - loss: 2186.6516 - val_loss: 3856.1155\n",
      "Epoch 2/66\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1604.6910 - val_loss: 3025.5959\n",
      "Epoch 3/66\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 1220.2025 - val_loss: 2291.7793\n",
      "Epoch 4/66\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 891.8136 - val_loss: 1446.4733\n",
      "Epoch 5/66\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 645.9570 - val_loss: 1164.8833\n",
      "Epoch 6/66\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 571.1932 - val_loss: 1096.6758\n",
      "Epoch 7/66\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 507.9798 - val_loss: 924.6031\n",
      "Epoch 8/66\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 479.4828 - val_loss: 692.5074\n",
      "Epoch 9/66\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 459.5349 - val_loss: 576.5984\n",
      "Epoch 10/66\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 439.2195 - val_loss: 508.0063\n",
      "Epoch 11/66\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 435.2019 - val_loss: 445.6381\n",
      "Epoch 12/66\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 418.1852 - val_loss: 433.2655\n",
      "Epoch 13/66\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 399.7938 - val_loss: 373.0353\n",
      "Epoch 14/66\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 398.3419 - val_loss: 465.5005\n",
      "Epoch 15/66\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 383.1884 - val_loss: 466.0994\n",
      "Epoch 16/66\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 372.7575 - val_loss: 417.2143\n",
      "Epoch 17/66\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 366.1420 - val_loss: 401.3874\n",
      "Epoch 18/66\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 353.2420 - val_loss: 445.8665\n",
      "Epoch 19/66\n",
      "20/20 [==============================] - 86s 5s/step - loss: 344.1418 - val_loss: 468.9866\n",
      "Epoch 20/66\n",
      "20/20 [==============================] - 2s 80ms/step - loss: 335.9706 - val_loss: 392.0123\n",
      "Epoch 21/66\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 332.3080 - val_loss: 383.6550\n",
      "Epoch 22/66\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 319.3588 - val_loss: 460.5349\n",
      "Epoch 23/66\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 310.0972 - val_loss: 455.8563\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1214.3204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:20:12,892] Trial 45 finished with value: 1214.3204345703125 and parameters: {'head_size': 202, 'num_heads': 6, 'ff_dim': 15, 'num_transformer_blocks': 5, 'mlp_units': 98, 'mlp_dropout': 0.11029466384230618, 'dropout': 0.1417487923353768, 'learning_rate': 0.0003534065131311236, 'n_epochs': 66}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_46\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_47 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_516 (L  (None, 8, 5)                 10        ['input_47[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_258 (  (None, 8, 5)                 25926     ['layer_normalization_516[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_516[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_562 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_258[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_516 (  (None, 8, 5)                 0         ['dropout_562[0][0]',         \n",
      " TFOpLambda)                                                         'input_47[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_517 (L  (None, 8, 5)                 10        ['tf.__operators__.add_516[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_516 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_517[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_563 (Dropout)       (None, 8, 10)                0         ['conv1d_516[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_517 (Conv1D)         (None, 8, 5)                 55        ['dropout_563[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_517 (  (None, 8, 5)                 0         ['conv1d_517[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_516[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_518 (L  (None, 8, 5)                 10        ['tf.__operators__.add_517[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_259 (  (None, 8, 5)                 25926     ['layer_normalization_518[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_518[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_564 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_259[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_518 (  (None, 8, 5)                 0         ['dropout_564[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_517[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_519 (L  (None, 8, 5)                 10        ['tf.__operators__.add_518[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_518 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_519[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_565 (Dropout)       (None, 8, 10)                0         ['conv1d_518[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_519 (Conv1D)         (None, 8, 5)                 55        ['dropout_565[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_519 (  (None, 8, 5)                 0         ['conv1d_519[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_518[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_520 (L  (None, 8, 5)                 10        ['tf.__operators__.add_519[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_260 (  (None, 8, 5)                 25926     ['layer_normalization_520[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_520[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_566 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_260[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_520 (  (None, 8, 5)                 0         ['dropout_566[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_519[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_521 (L  (None, 8, 5)                 10        ['tf.__operators__.add_520[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_520 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_521[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_567 (Dropout)       (None, 8, 10)                0         ['conv1d_520[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_521 (Conv1D)         (None, 8, 5)                 55        ['dropout_567[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_521 (  (None, 8, 5)                 0         ['conv1d_521[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_520[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_522 (L  (None, 8, 5)                 10        ['tf.__operators__.add_521[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_261 (  (None, 8, 5)                 25926     ['layer_normalization_522[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_522[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_568 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_261[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_522 (  (None, 8, 5)                 0         ['dropout_568[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_521[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_523 (L  (None, 8, 5)                 10        ['tf.__operators__.add_522[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_522 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_523[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_569 (Dropout)       (None, 8, 10)                0         ['conv1d_522[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_523 (Conv1D)         (None, 8, 5)                 55        ['dropout_569[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_523 (  (None, 8, 5)                 0         ['conv1d_523[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_522[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_524 (L  (None, 8, 5)                 10        ['tf.__operators__.add_523[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_262 (  (None, 8, 5)                 25926     ['layer_normalization_524[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_524[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_570 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_262[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_524 (  (None, 8, 5)                 0         ['dropout_570[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_523[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_525 (L  (None, 8, 5)                 10        ['tf.__operators__.add_524[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_524 (Conv1D)         (None, 8, 10)                60        ['layer_normalization_525[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_571 (Dropout)       (None, 8, 10)                0         ['conv1d_524[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_525 (Conv1D)         (None, 8, 5)                 55        ['dropout_571[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_525 (  (None, 8, 5)                 0         ['conv1d_525[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_524[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_525[0][\n",
      " 6 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_138 (Dense)           (None, 101)                  909       ['global_average_pooling1d_46[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_572 (Dropout)       (None, 101)                  0         ['dense_138[0][0]']           \n",
      "                                                                                                  \n",
      " dense_139 (Dense)           (None, 16)                   1632      ['dropout_572[0][0]']         \n",
      "                                                                                                  \n",
      " dense_140 (Dense)           (None, 8)                    136       ['dense_139[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132982 (519.46 KB)\n",
      "Trainable params: 132982 (519.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/158\n",
      "20/20 [==============================] - 3s 42ms/step - loss: 2048.1787 - val_loss: 4081.3171\n",
      "Epoch 2/158\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 1026.4315 - val_loss: 1488.4758\n",
      "Epoch 3/158\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 644.9729 - val_loss: 641.7128\n",
      "Epoch 4/158\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 544.3962 - val_loss: 619.6645\n",
      "Epoch 5/158\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 510.0039 - val_loss: 566.7867\n",
      "Epoch 6/158\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 466.3115 - val_loss: 629.5978\n",
      "Epoch 7/158\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 436.2715 - val_loss: 574.5485\n",
      "Epoch 8/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 410.0168 - val_loss: 541.2618\n",
      "Epoch 9/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 380.6169 - val_loss: 485.4531\n",
      "Epoch 10/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 367.4960 - val_loss: 515.2269\n",
      "Epoch 11/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 346.7152 - val_loss: 473.9754\n",
      "Epoch 12/158\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 329.5627 - val_loss: 493.4289\n",
      "Epoch 13/158\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 312.1985 - val_loss: 396.0179\n",
      "Epoch 14/158\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 298.4243 - val_loss: 581.5334\n",
      "Epoch 15/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 289.2221 - val_loss: 436.2759\n",
      "Epoch 16/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 273.2454 - val_loss: 413.4610\n",
      "Epoch 17/158\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 262.5015 - val_loss: 429.9676\n",
      "Epoch 18/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 262.5883 - val_loss: 487.0125\n",
      "Epoch 19/158\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 245.7981 - val_loss: 461.2417\n",
      "Epoch 20/158\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 242.8440 - val_loss: 495.8902\n",
      "Epoch 21/158\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 231.5909 - val_loss: 472.6520\n",
      "Epoch 22/158\n",
      "20/20 [==============================] - 1s 38ms/step - loss: 224.3137 - val_loss: 420.9272\n",
      "Epoch 23/158\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 216.8219 - val_loss: 519.4916\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1236.6830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:20:32,238] Trial 46 finished with value: 1236.6829833984375 and parameters: {'head_size': 161, 'num_heads': 7, 'ff_dim': 10, 'num_transformer_blocks': 5, 'mlp_units': 101, 'mlp_dropout': 0.13950749219972927, 'dropout': 0.1265045041254224, 'learning_rate': 0.0006989130620882346, 'n_epochs': 158}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_47\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_48 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_526 (L  (None, 8, 5)                 10        ['input_48[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_263 (  (None, 8, 5)                 40485     ['layer_normalization_526[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_526[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_573 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_263[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_526 (  (None, 8, 5)                 0         ['dropout_573[0][0]',         \n",
      " TFOpLambda)                                                         'input_48[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_527 (L  (None, 8, 5)                 10        ['tf.__operators__.add_526[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_526 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_527[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_574 (Dropout)       (None, 8, 9)                 0         ['conv1d_526[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_527 (Conv1D)         (None, 8, 5)                 50        ['dropout_574[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_527 (  (None, 8, 5)                 0         ['conv1d_527[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_526[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_528 (L  (None, 8, 5)                 10        ['tf.__operators__.add_527[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_264 (  (None, 8, 5)                 40485     ['layer_normalization_528[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_528[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_575 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_264[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_528 (  (None, 8, 5)                 0         ['dropout_575[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_527[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_529 (L  (None, 8, 5)                 10        ['tf.__operators__.add_528[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_528 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_529[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_576 (Dropout)       (None, 8, 9)                 0         ['conv1d_528[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_529 (Conv1D)         (None, 8, 5)                 50        ['dropout_576[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_529 (  (None, 8, 5)                 0         ['conv1d_529[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_528[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_530 (L  (None, 8, 5)                 10        ['tf.__operators__.add_529[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_265 (  (None, 8, 5)                 40485     ['layer_normalization_530[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_530[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_577 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_265[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_530 (  (None, 8, 5)                 0         ['dropout_577[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_529[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_531 (L  (None, 8, 5)                 10        ['tf.__operators__.add_530[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_530 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_531[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_578 (Dropout)       (None, 8, 9)                 0         ['conv1d_530[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_531 (Conv1D)         (None, 8, 5)                 50        ['dropout_578[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_531 (  (None, 8, 5)                 0         ['conv1d_531[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_530[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_532 (L  (None, 8, 5)                 10        ['tf.__operators__.add_531[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_266 (  (None, 8, 5)                 40485     ['layer_normalization_532[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_532[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_579 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_266[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_532 (  (None, 8, 5)                 0         ['dropout_579[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_531[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_533 (L  (None, 8, 5)                 10        ['tf.__operators__.add_532[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_532 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_533[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_580 (Dropout)       (None, 8, 9)                 0         ['conv1d_532[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_533 (Conv1D)         (None, 8, 5)                 50        ['dropout_580[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_533 (  (None, 8, 5)                 0         ['conv1d_533[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_532[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_534 (L  (None, 8, 5)                 10        ['tf.__operators__.add_533[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_267 (  (None, 8, 5)                 40485     ['layer_normalization_534[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_534[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_581 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_267[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_534 (  (None, 8, 5)                 0         ['dropout_581[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_533[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_535 (L  (None, 8, 5)                 10        ['tf.__operators__.add_534[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_534 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_535[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_582 (Dropout)       (None, 8, 9)                 0         ['conv1d_534[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_535 (Conv1D)         (None, 8, 5)                 50        ['dropout_582[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_535 (  (None, 8, 5)                 0         ['conv1d_535[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_534[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_536 (L  (None, 8, 5)                 10        ['tf.__operators__.add_535[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_268 (  (None, 8, 5)                 40485     ['layer_normalization_536[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_536[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_583 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_268[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_536 (  (None, 8, 5)                 0         ['dropout_583[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_535[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_537 (L  (None, 8, 5)                 10        ['tf.__operators__.add_536[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_536 (Conv1D)         (None, 8, 9)                 54        ['layer_normalization_537[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_584 (Dropout)       (None, 8, 9)                 0         ['conv1d_536[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_537 (Conv1D)         (None, 8, 5)                 50        ['dropout_584[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_537 (  (None, 8, 5)                 0         ['conv1d_537[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_536[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_537[0][\n",
      " 7 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_141 (Dense)           (None, 123)                  1107      ['global_average_pooling1d_47[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_585 (Dropout)       (None, 123)                  0         ['dense_141[0][0]']           \n",
      "                                                                                                  \n",
      " dense_142 (Dense)           (None, 16)                   1984      ['dropout_585[0][0]']         \n",
      "                                                                                                  \n",
      " dense_143 (Dense)           (None, 8)                    136       ['dense_142[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 246881 (964.38 KB)\n",
      "Trainable params: 246881 (964.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/180\n",
      "20/20 [==============================] - 3s 61ms/step - loss: 1573.9979 - val_loss: 3709.1025\n",
      "Epoch 2/180\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1156.5334 - val_loss: 3317.3677\n",
      "Epoch 3/180\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 967.1727 - val_loss: 2776.3140\n",
      "Epoch 4/180\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 815.7672 - val_loss: 2180.0107\n",
      "Epoch 5/180\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 710.2709 - val_loss: 1728.2589\n",
      "Epoch 6/180\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 628.7166 - val_loss: 1224.1499\n",
      "Epoch 7/180\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 584.9597 - val_loss: 881.6561\n",
      "Epoch 8/180\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 525.7084 - val_loss: 688.1437\n",
      "Epoch 9/180\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 515.9417 - val_loss: 605.8250\n",
      "Epoch 10/180\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 490.0103 - val_loss: 551.9286\n",
      "Epoch 11/180\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 475.2443 - val_loss: 483.3885\n",
      "Epoch 12/180\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 455.4522 - val_loss: 442.0404\n",
      "Epoch 13/180\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 440.7024 - val_loss: 590.3088\n",
      "Epoch 14/180\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 426.3108 - val_loss: 585.8999\n",
      "Epoch 15/180\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 419.4029 - val_loss: 489.6328\n",
      "Epoch 16/180\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 411.9510 - val_loss: 530.6420\n",
      "Epoch 17/180\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 390.3264 - val_loss: 501.1578\n",
      "Epoch 18/180\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 386.7369 - val_loss: 494.9768\n",
      "Epoch 19/180\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 374.8653 - val_loss: 546.8913\n",
      "Epoch 20/180\n",
      "20/20 [==============================] - 1s 63ms/step - loss: 364.2831 - val_loss: 566.0122\n",
      "Epoch 21/180\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 357.2501 - val_loss: 494.6112\n",
      "Epoch 22/180\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 345.9557 - val_loss: 531.1023\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1612.5380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:20:59,125] Trial 47 finished with value: 1612.5379638671875 and parameters: {'head_size': 220, 'num_heads': 8, 'ff_dim': 9, 'num_transformer_blocks': 6, 'mlp_units': 123, 'mlp_dropout': 0.14434358894200106, 'dropout': 0.20758157095445517, 'learning_rate': 0.00026359777390528997, 'n_epochs': 180}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_48\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_538 (L  (None, 8, 5)                 10        ['input_49[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_269 (  (None, 8, 5)                 14334     ['layer_normalization_538[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_538[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_586 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_269[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_538 (  (None, 8, 5)                 0         ['dropout_586[0][0]',         \n",
      " TFOpLambda)                                                         'input_49[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_539 (L  (None, 8, 5)                 10        ['tf.__operators__.add_538[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_538 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_539[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_587 (Dropout)       (None, 8, 13)                0         ['conv1d_538[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_539 (Conv1D)         (None, 8, 5)                 70        ['dropout_587[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_539 (  (None, 8, 5)                 0         ['conv1d_539[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_538[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_540 (L  (None, 8, 5)                 10        ['tf.__operators__.add_539[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_270 (  (None, 8, 5)                 14334     ['layer_normalization_540[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_540[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_588 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_270[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_540 (  (None, 8, 5)                 0         ['dropout_588[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_539[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_541 (L  (None, 8, 5)                 10        ['tf.__operators__.add_540[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_540 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_541[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_589 (Dropout)       (None, 8, 13)                0         ['conv1d_540[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_541 (Conv1D)         (None, 8, 5)                 70        ['dropout_589[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_541 (  (None, 8, 5)                 0         ['conv1d_541[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_540[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_542 (L  (None, 8, 5)                 10        ['tf.__operators__.add_541[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_271 (  (None, 8, 5)                 14334     ['layer_normalization_542[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_542[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_590 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_271[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_542 (  (None, 8, 5)                 0         ['dropout_590[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_541[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_543 (L  (None, 8, 5)                 10        ['tf.__operators__.add_542[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_542 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_543[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_591 (Dropout)       (None, 8, 13)                0         ['conv1d_542[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_543 (Conv1D)         (None, 8, 5)                 70        ['dropout_591[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_543 (  (None, 8, 5)                 0         ['conv1d_543[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_542[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_544 (L  (None, 8, 5)                 10        ['tf.__operators__.add_543[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_272 (  (None, 8, 5)                 14334     ['layer_normalization_544[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_544[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_592 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_272[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_544 (  (None, 8, 5)                 0         ['dropout_592[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_543[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_545 (L  (None, 8, 5)                 10        ['tf.__operators__.add_544[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_544 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_545[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_593 (Dropout)       (None, 8, 13)                0         ['conv1d_544[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_545 (Conv1D)         (None, 8, 5)                 70        ['dropout_593[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_545 (  (None, 8, 5)                 0         ['conv1d_545[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_544[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_546 (L  (None, 8, 5)                 10        ['tf.__operators__.add_545[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_273 (  (None, 8, 5)                 14334     ['layer_normalization_546[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_546[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_594 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_273[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_546 (  (None, 8, 5)                 0         ['dropout_594[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_545[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_547 (L  (None, 8, 5)                 10        ['tf.__operators__.add_546[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_546 (Conv1D)         (None, 8, 13)                78        ['layer_normalization_547[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_595 (Dropout)       (None, 8, 13)                0         ['conv1d_546[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_547 (Conv1D)         (None, 8, 5)                 70        ['dropout_595[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_547 (  (None, 8, 5)                 0         ['conv1d_547[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_546[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_547[0][\n",
      " 8 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_144 (Dense)           (None, 113)                  1017      ['global_average_pooling1d_48[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_596 (Dropout)       (None, 113)                  0         ['dense_144[0][0]']           \n",
      "                                                                                                  \n",
      " dense_145 (Dense)           (None, 16)                   1824      ['dropout_596[0][0]']         \n",
      "                                                                                                  \n",
      " dense_146 (Dense)           (None, 8)                    136       ['dense_145[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 75487 (294.87 KB)\n",
      "Trainable params: 75487 (294.87 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/172\n",
      "20/20 [==============================] - 2s 38ms/step - loss: 2205.9468 - val_loss: 3374.7034\n",
      "Epoch 2/172\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 978.8594 - val_loss: 1174.1388\n",
      "Epoch 3/172\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 701.8507 - val_loss: 792.4333\n",
      "Epoch 4/172\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 615.9387 - val_loss: 527.6810\n",
      "Epoch 5/172\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 562.3932 - val_loss: 382.3313\n",
      "Epoch 6/172\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 508.3260 - val_loss: 614.1636\n",
      "Epoch 7/172\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 485.9703 - val_loss: 302.2900\n",
      "Epoch 8/172\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 449.7867 - val_loss: 423.1495\n",
      "Epoch 9/172\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 426.3464 - val_loss: 228.1936\n",
      "Epoch 10/172\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 404.3598 - val_loss: 469.9612\n",
      "Epoch 11/172\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 381.3439 - val_loss: 333.7300\n",
      "Epoch 12/172\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 363.8268 - val_loss: 353.5551\n",
      "Epoch 13/172\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 338.7455 - val_loss: 355.7980\n",
      "Epoch 14/172\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 321.1161 - val_loss: 325.3719\n",
      "Epoch 15/172\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 311.5970 - val_loss: 401.1182\n",
      "Epoch 16/172\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 295.7803 - val_loss: 283.1230\n",
      "Epoch 17/172\n",
      "20/20 [==============================] - 1s 35ms/step - loss: 291.3257 - val_loss: 263.9892\n",
      "Epoch 18/172\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 282.9123 - val_loss: 334.3211\n",
      "Epoch 19/172\n",
      "20/20 [==============================] - 1s 33ms/step - loss: 261.7581 - val_loss: 321.2352\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 839.6962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:21:12,639] Trial 48 finished with value: 839.6962280273438 and parameters: {'head_size': 89, 'num_heads': 7, 'ff_dim': 13, 'num_transformer_blocks': 5, 'mlp_units': 113, 'mlp_dropout': 0.16673133551068944, 'dropout': 0.1111895055810657, 'learning_rate': 0.0007379411708346279, 'n_epochs': 172}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_49\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_50 (InputLayer)       [(None, 8, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_548 (L  (None, 8, 5)                 10        ['input_50[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_274 (  (None, 8, 5)                 10355     ['layer_normalization_548[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_548[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_597 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_274[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_548 (  (None, 8, 5)                 0         ['dropout_597[0][0]',         \n",
      " TFOpLambda)                                                         'input_50[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_549 (L  (None, 8, 5)                 10        ['tf.__operators__.add_548[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_548 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_549[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_598 (Dropout)       (None, 8, 22)                0         ['conv1d_548[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_549 (Conv1D)         (None, 8, 5)                 115       ['dropout_598[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_549 (  (None, 8, 5)                 0         ['conv1d_549[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_548[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_550 (L  (None, 8, 5)                 10        ['tf.__operators__.add_549[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_275 (  (None, 8, 5)                 10355     ['layer_normalization_550[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_550[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_599 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_275[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_550 (  (None, 8, 5)                 0         ['dropout_599[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_549[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_551 (L  (None, 8, 5)                 10        ['tf.__operators__.add_550[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_550 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_551[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_600 (Dropout)       (None, 8, 22)                0         ['conv1d_550[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_551 (Conv1D)         (None, 8, 5)                 115       ['dropout_600[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_551 (  (None, 8, 5)                 0         ['conv1d_551[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_550[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_552 (L  (None, 8, 5)                 10        ['tf.__operators__.add_551[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_276 (  (None, 8, 5)                 10355     ['layer_normalization_552[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_552[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_601 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_276[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_552 (  (None, 8, 5)                 0         ['dropout_601[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_551[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_553 (L  (None, 8, 5)                 10        ['tf.__operators__.add_552[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_552 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_553[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_602 (Dropout)       (None, 8, 22)                0         ['conv1d_552[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_553 (Conv1D)         (None, 8, 5)                 115       ['dropout_602[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_553 (  (None, 8, 5)                 0         ['conv1d_553[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_552[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_554 (L  (None, 8, 5)                 10        ['tf.__operators__.add_553[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_277 (  (None, 8, 5)                 10355     ['layer_normalization_554[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_554[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_603 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_277[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_554 (  (None, 8, 5)                 0         ['dropout_603[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_553[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_555 (L  (None, 8, 5)                 10        ['tf.__operators__.add_554[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_554 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_555[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_604 (Dropout)       (None, 8, 22)                0         ['conv1d_554[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_555 (Conv1D)         (None, 8, 5)                 115       ['dropout_604[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_555 (  (None, 8, 5)                 0         ['conv1d_555[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_554[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_556 (L  (None, 8, 5)                 10        ['tf.__operators__.add_555[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_278 (  (None, 8, 5)                 10355     ['layer_normalization_556[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_556[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_605 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_278[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_556 (  (None, 8, 5)                 0         ['dropout_605[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_555[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_557 (L  (None, 8, 5)                 10        ['tf.__operators__.add_556[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_556 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_557[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_606 (Dropout)       (None, 8, 22)                0         ['conv1d_556[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_557 (Conv1D)         (None, 8, 5)                 115       ['dropout_606[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_557 (  (None, 8, 5)                 0         ['conv1d_557[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_556[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_558 (L  (None, 8, 5)                 10        ['tf.__operators__.add_557[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_279 (  (None, 8, 5)                 10355     ['layer_normalization_558[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_558[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_607 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_279[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_558 (  (None, 8, 5)                 0         ['dropout_607[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_557[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_559 (L  (None, 8, 5)                 10        ['tf.__operators__.add_558[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_558 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_559[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_608 (Dropout)       (None, 8, 22)                0         ['conv1d_558[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_559 (Conv1D)         (None, 8, 5)                 115       ['dropout_608[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_559 (  (None, 8, 5)                 0         ['conv1d_559[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_558[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_560 (L  (None, 8, 5)                 10        ['tf.__operators__.add_559[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " multi_head_attention_280 (  (None, 8, 5)                 10355     ['layer_normalization_560[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_560[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_609 (Dropout)       (None, 8, 5)                 0         ['multi_head_attention_280[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_560 (  (None, 8, 5)                 0         ['dropout_609[0][0]',         \n",
      " TFOpLambda)                                                         'tf.__operators__.add_559[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " layer_normalization_561 (L  (None, 8, 5)                 10        ['tf.__operators__.add_560[0][\n",
      " ayerNormalization)                                                 0]']                          \n",
      "                                                                                                  \n",
      " conv1d_560 (Conv1D)         (None, 8, 22)                132       ['layer_normalization_561[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_610 (Dropout)       (None, 8, 22)                0         ['conv1d_560[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_561 (Conv1D)         (None, 8, 5)                 115       ['dropout_610[0][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_561 (  (None, 8, 5)                 0         ['conv1d_561[0][0]',          \n",
      " TFOpLambda)                                                         'tf.__operators__.add_560[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 8)                    0         ['tf.__operators__.add_561[0][\n",
      " 9 (GlobalAveragePooling1D)                                         0]']                          \n",
      "                                                                                                  \n",
      " dense_147 (Dense)           (None, 112)                  1008      ['global_average_pooling1d_49[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " dropout_611 (Dropout)       (None, 112)                  0         ['dense_147[0][0]']           \n",
      "                                                                                                  \n",
      " dense_148 (Dense)           (None, 16)                   1808      ['dropout_611[0][0]']         \n",
      "                                                                                                  \n",
      " dense_149 (Dense)           (None, 8)                    136       ['dense_148[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 77306 (301.98 KB)\n",
      "Trainable params: 77306 (301.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/130\n",
      "20/20 [==============================] - 3s 48ms/step - loss: 2061.1553 - val_loss: 2103.7708\n",
      "Epoch 2/130\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 950.8203 - val_loss: 1800.7886\n",
      "Epoch 3/130\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 723.2625 - val_loss: 1418.1146\n",
      "Epoch 4/130\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 601.1207 - val_loss: 692.6801\n",
      "Epoch 5/130\n",
      "20/20 [==============================] - 1s 36ms/step - loss: 525.3340 - val_loss: 530.6149\n",
      "Epoch 6/130\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 476.6869 - val_loss: 565.3167\n",
      "Epoch 7/130\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 428.5332 - val_loss: 465.7400\n",
      "Epoch 8/130\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 403.4916 - val_loss: 536.2223\n",
      "Epoch 9/130\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 364.3707 - val_loss: 462.5631\n",
      "Epoch 10/130\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 341.3919 - val_loss: 396.5300\n",
      "Epoch 11/130\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 320.2592 - val_loss: 379.3755\n",
      "Epoch 12/130\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 293.5717 - val_loss: 392.0328\n",
      "Epoch 13/130\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 284.5950 - val_loss: 406.1082\n",
      "Epoch 14/130\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 262.7446 - val_loss: 371.2390\n",
      "Epoch 15/130\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 250.6520 - val_loss: 425.2565\n",
      "Epoch 16/130\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 243.6428 - val_loss: 385.8304\n",
      "Epoch 17/130\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 229.4611 - val_loss: 406.0529\n",
      "Epoch 18/130\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 220.7943 - val_loss: 368.0039\n",
      "Epoch 19/130\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 214.4102 - val_loss: 322.8027\n",
      "Epoch 20/130\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 209.9832 - val_loss: 330.4775\n",
      "Epoch 21/130\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 200.2564 - val_loss: 403.2989\n",
      "Epoch 22/130\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 201.2061 - val_loss: 288.6293\n",
      "Epoch 23/130\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 198.3141 - val_loss: 357.7781\n",
      "Epoch 24/130\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 190.9352 - val_loss: 386.6489\n",
      "Epoch 25/130\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 182.1881 - val_loss: 367.0088\n",
      "Epoch 26/130\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 181.8024 - val_loss: 277.6598\n",
      "Epoch 27/130\n",
      "20/20 [==============================] - 1s 39ms/step - loss: 178.6557 - val_loss: 350.6963\n",
      "Epoch 28/130\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 176.4003 - val_loss: 293.6632\n",
      "Epoch 29/130\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 172.3249 - val_loss: 461.3838\n",
      "Epoch 30/130\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 170.9686 - val_loss: 283.4253\n",
      "Epoch 31/130\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 163.8650 - val_loss: 337.4405\n",
      "Epoch 32/130\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 162.2458 - val_loss: 398.5642\n",
      "Epoch 33/130\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 166.1887 - val_loss: 320.5966\n",
      "Epoch 34/130\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 164.0948 - val_loss: 314.1621\n",
      "Epoch 35/130\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 160.6674 - val_loss: 287.0710\n",
      "Epoch 36/130\n",
      "20/20 [==============================] - 1s 42ms/step - loss: 154.4331 - val_loss: 356.6996\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 849.5533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-16 17:21:48,376] Trial 49 finished with value: 849.5532836914062 and parameters: {'head_size': 90, 'num_heads': 5, 'ff_dim': 22, 'num_transformer_blocks': 7, 'mlp_units': 112, 'mlp_dropout': 0.17022693703974662, 'dropout': 0.24813766400735707, 'learning_rate': 0.000999319839821239, 'n_epochs': 130}. Best is trial 34 with value: 656.3905029296875.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlePrpfKBgtB"
   },
   "source": [
    "# Viewing our Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKYq7TBFBAx4",
    "outputId": "bc40acca-e0e5-4b6a-bb67-48c3b7ca6e0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    head_size  num_heads  ff_dim  num_transformer_blocks  mlp_units  \\\n",
      "0         122          8      23                       4        125   \n",
      "1          83          7      16                       6         71   \n",
      "2         140          7      20                       8        101   \n",
      "3         167          7      16                       5        118   \n",
      "4         193          5      23                       5         99   \n",
      "5          79          4      22                       6         77   \n",
      "6         143          6      13                       6        121   \n",
      "7          92          4      21                       4         87   \n",
      "8         199          5      21                       4         65   \n",
      "9         132          6      24                       7        124   \n",
      "10        250          8       9                       4        111   \n",
      "11        252          8       8                       4        108   \n",
      "12        254          8       8                       5        113   \n",
      "13        240          8       8                       5        112   \n",
      "14        223          8      11                       5         89   \n",
      "15        215          7      12                       5        108   \n",
      "16        255          8      10                       7        114   \n",
      "17        181          7      14                       4        105   \n",
      "18        173          7      14                       5         90   \n",
      "19        105          6      18                       7        103   \n",
      "20        104          5      18                       7         94   \n",
      "21        113          6      18                       8        104   \n",
      "22         67          6      18                       7        104   \n",
      "23        179          7      14                       6        118   \n",
      "24        149          7      15                       6        119   \n",
      "25        229          6      19                       7        116   \n",
      "26        157          6      17                       6        128   \n",
      "27        204          5      11                       8         94   \n",
      "28        117          7      12                       7         83   \n",
      "29        182          8      15                       6        123   \n",
      "30        101          7      10                       6        128   \n",
      "31        180          7      14                       4         99   \n",
      "32        129          8      17                       5        108   \n",
      "33        190          7      14                       4        103   \n",
      "34        156          7      15                       5        116   \n",
      "35        161          6      16                       5        115   \n",
      "36        211          7      19                       6        119   \n",
      "37         79          6      16                       5        111   \n",
      "38        133          5      20                       8        121   \n",
      "39        151          8      13                       6         99   \n",
      "40        236          7      17                       6        117   \n",
      "41        170          7      15                       4        110   \n",
      "42        188          7      13                       5        105   \n",
      "43        192          6      13                       5        114   \n",
      "44        141          8      12                       5        106   \n",
      "45        202          6      15                       5         98   \n",
      "46        161          7      10                       5        101   \n",
      "47        220          8       9                       6        123   \n",
      "48         89          7      13                       5        113   \n",
      "49         90          5      22                       7        112   \n",
      "\n",
      "    mlp_dropout   dropout  learning_rate  n_epochs         value  \n",
      "0      0.100443  0.258834       0.000052       163   1294.032471  \n",
      "1      0.191881  0.264681       0.000003        72  12303.410156  \n",
      "2      0.303580  0.138592       0.000087        65   1333.905884  \n",
      "3      0.113761  0.172781       0.000014       152   2364.010010  \n",
      "4      0.137743  0.222440       0.000001       188   8521.475586  \n",
      "5      0.284594  0.335667       0.000001        75   6085.980957  \n",
      "6      0.329423  0.247162       0.000020       134   1876.027710  \n",
      "7      0.207090  0.198328       0.000008       114   4985.630859  \n",
      "8      0.151104  0.193011       0.000095        96   2131.491943  \n",
      "9      0.346858  0.201138       0.000014       109   2643.272217  \n",
      "10     0.100278  0.292863       0.000700       190    895.738220  \n",
      "11     0.103314  0.300618       0.000898       197    938.586670  \n",
      "12     0.165441  0.313133       0.000959       198    845.503540  \n",
      "13     0.167721  0.337096       0.000869       175    986.806152  \n",
      "14     0.240391  0.301801       0.000292       179   1094.122192  \n",
      "15     0.132713  0.294604       0.000323       198   1233.630981  \n",
      "16     0.173248  0.336180       0.000329       149    938.359924  \n",
      "17     0.151838  0.101151       0.000891       167    870.481567  \n",
      "18     0.227937  0.120991       0.000179       166   1415.592407  \n",
      "19     0.164045  0.108573       0.000518       137    826.896973  \n",
      "20     0.192147  0.163927       0.000145       135   1180.487427  \n",
      "21     0.156451  0.102286       0.000544       148   1080.125732  \n",
      "22     0.136406  0.104221       0.000978       166    992.694824  \n",
      "23     0.175812  0.137364       0.000458       123    802.083923  \n",
      "24     0.178411  0.145700       0.000376       119   1236.908325  \n",
      "25     0.201027  0.134189       0.000485        93    971.144470  \n",
      "26     0.179052  0.164246       0.000195       124   1182.747070  \n",
      "27     0.218922  0.121758       0.000561       102   1295.331909  \n",
      "28     0.126174  0.145771       0.000270        85   1120.099243  \n",
      "29     0.160938  0.231538       0.000044       126   1440.086426  \n",
      "30     0.147176  0.122156       0.000484       140    968.787415  \n",
      "31     0.154121  0.110383       0.000656       157   1288.370605  \n",
      "32     0.182842  0.101696       0.000939       176   1161.716187  \n",
      "33     0.167759  0.118215       0.000429       162    892.895569  \n",
      "34     0.117268  0.127685       0.000647       187    656.390503  \n",
      "35     0.120268  0.134449       0.000114       187   1367.780762  \n",
      "36     0.121043  0.153513       0.000224       185   1167.239380  \n",
      "37     0.137002  0.178835       0.000060       141   1151.100098  \n",
      "38     0.143003  0.133509       0.000249       192    877.994873  \n",
      "39     0.114550  0.152054       0.000143       110   1236.005005  \n",
      "40     0.188867  0.182614       0.000411       199   1383.117188  \n",
      "41     0.154724  0.116763       0.000672       172   1412.403687  \n",
      "42     0.144783  0.100814       0.000646       182    850.551880  \n",
      "43     0.126161  0.130494       0.000629       183    975.440491  \n",
      "44     0.166613  0.109754       0.000497       194    946.892029  \n",
      "45     0.110295  0.141749       0.000353        66   1214.320435  \n",
      "46     0.139507  0.126505       0.000699       158   1236.682983  \n",
      "47     0.144344  0.207582       0.000264       180   1612.537964  \n",
      "48     0.166731  0.111190       0.000738       172    839.696228  \n",
      "49     0.170227  0.248138       0.000999       130    849.553284  \n"
     ]
    }
   ],
   "source": [
    "all_trials = study.trials\n",
    "\n",
    "trial_results = []\n",
    "\n",
    "for trial in all_trials:\n",
    "    trial_params = trial.params\n",
    "    trial_value = trial.value\n",
    "    trial_result = {**trial_params, \"value\": trial_value}\n",
    "    trial_results.append(trial_result)\n",
    "\n",
    "trial_results_df = pd.DataFrame(trial_results)\n",
    "\n",
    "print(trial_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXpOYvXG8vfm"
   },
   "source": [
    "# Retrieving the best parameters from our search and building our model with these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ln9Rq8Fg7Ld2",
    "outputId": "82c37d1d-f303-4dfd-b50a-0c8be2d55f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'head_size': 156, 'num_heads': 7, 'ff_dim': 15, 'num_transformer_blocks': 5, 'mlp_units': 116, 'mlp_dropout': 0.11726807327113829, 'dropout': 0.12768472299347952, 'learning_rate': 0.0006473993344407467, 'n_epochs': 187}\n",
      "Best loss: 656.3905029296875\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_loss = study.best_value\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "c39kP3208-qo"
   },
   "outputs": [],
   "source": [
    "best_model = build_model(\n",
    "    input_shape=x_train.shape[1:],\n",
    "    head_size=best_params[\"head_size\"],\n",
    "    num_heads=best_params[\"num_heads\"],\n",
    "    ff_dim=best_params[\"ff_dim\"],\n",
    "    num_transformer_blocks=best_params[\"num_transformer_blocks\"],\n",
    "    mlp_units=[best_params[\"mlp_units\"]],\n",
    "    mlp_dropout=best_params[\"mlp_dropout\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yH2uxzJx8-rx"
   },
   "outputs": [],
   "source": [
    "best_model.compile(\n",
    "    loss=\"mean_absolute_error\",\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=best_params['learning_rate'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6DrQx4x9bl-",
    "outputId": "399fa5c5-8951-4045-8eb4-0f3858f1c305",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/187\n",
      "18/18 [==============================] - 2s 45ms/step - loss: 2112.1836 - val_loss: 4494.3574\n",
      "Epoch 2/187\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 972.0182 - val_loss: 1506.4160\n",
      "Epoch 3/187\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 559.0244 - val_loss: 599.7822\n",
      "Epoch 4/187\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 488.2434 - val_loss: 566.9741\n",
      "Epoch 5/187\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 477.1525 - val_loss: 405.1788\n",
      "Epoch 6/187\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 450.2514 - val_loss: 407.4417\n",
      "Epoch 7/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 422.7819 - val_loss: 372.1998\n",
      "Epoch 8/187\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 405.5941 - val_loss: 456.7125\n",
      "Epoch 9/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 381.5315 - val_loss: 365.5364\n",
      "Epoch 10/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 364.7542 - val_loss: 447.5941\n",
      "Epoch 11/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 353.9270 - val_loss: 444.7235\n",
      "Epoch 12/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 328.3618 - val_loss: 466.0831\n",
      "Epoch 13/187\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 320.9402 - val_loss: 430.4502\n",
      "Epoch 14/187\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 300.0476 - val_loss: 436.0649\n",
      "Epoch 15/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 288.1080 - val_loss: 418.5357\n",
      "Epoch 16/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 280.3596 - val_loss: 476.8951\n",
      "Epoch 17/187\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 267.7753 - val_loss: 323.7383\n",
      "Epoch 18/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 257.9153 - val_loss: 314.3297\n",
      "Epoch 19/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 251.2508 - val_loss: 358.4420\n",
      "Epoch 20/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 239.0754 - val_loss: 289.2159\n",
      "Epoch 21/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 230.5659 - val_loss: 314.8628\n",
      "Epoch 22/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 222.0391 - val_loss: 355.3353\n",
      "Epoch 23/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 211.4032 - val_loss: 311.4870\n",
      "Epoch 24/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 207.7894 - val_loss: 314.8580\n",
      "Epoch 25/187\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 202.3049 - val_loss: 340.5654\n",
      "Epoch 26/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 197.0672 - val_loss: 291.4319\n",
      "Epoch 27/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 193.5704 - val_loss: 279.2023\n",
      "Epoch 28/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 184.2402 - val_loss: 269.3840\n",
      "Epoch 29/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 183.3193 - val_loss: 299.6659\n",
      "Epoch 30/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 176.9150 - val_loss: 324.6301\n",
      "Epoch 31/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 168.6467 - val_loss: 363.5493\n",
      "Epoch 32/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 166.6664 - val_loss: 282.6341\n",
      "Epoch 33/187\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 167.2324 - val_loss: 294.9935\n",
      "Epoch 34/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 160.4025 - val_loss: 286.5209\n",
      "Epoch 35/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 157.9167 - val_loss: 247.3580\n",
      "Epoch 36/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 152.4847 - val_loss: 329.1848\n",
      "Epoch 37/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 150.9143 - val_loss: 314.8108\n",
      "Epoch 38/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 146.8842 - val_loss: 267.0114\n",
      "Epoch 39/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 147.2378 - val_loss: 283.9333\n",
      "Epoch 40/187\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 143.2540 - val_loss: 275.7038\n",
      "Epoch 41/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 138.9784 - val_loss: 285.2018\n",
      "Epoch 42/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 139.9497 - val_loss: 266.5489\n",
      "Epoch 43/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 134.1060 - val_loss: 263.7192\n",
      "Epoch 44/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 136.7096 - val_loss: 287.6017\n",
      "Epoch 45/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 134.7820 - val_loss: 275.7349\n",
      "Epoch 46/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 129.0780 - val_loss: 270.5928\n",
      "Epoch 47/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 132.2368 - val_loss: 257.3857\n",
      "Epoch 48/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 130.7624 - val_loss: 263.6362\n",
      "Epoch 49/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 129.2201 - val_loss: 293.5344\n",
      "Epoch 50/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 128.3162 - val_loss: 292.2149\n",
      "Epoch 51/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 123.5903 - val_loss: 288.4828\n",
      "Epoch 52/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 120.6103 - val_loss: 270.7262\n",
      "Epoch 53/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 120.7749 - val_loss: 256.3436\n",
      "Epoch 54/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 120.2256 - val_loss: 276.0773\n",
      "Epoch 55/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 120.0357 - val_loss: 308.5350\n",
      "Epoch 56/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 120.2364 - val_loss: 273.2292\n",
      "Epoch 57/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 116.7701 - val_loss: 257.5055\n",
      "Epoch 58/187\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 118.7825 - val_loss: 247.2940\n",
      "Epoch 59/187\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 113.8678 - val_loss: 269.3686\n",
      "Epoch 60/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 111.2790 - val_loss: 242.3972\n",
      "Epoch 61/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 114.2906 - val_loss: 293.7764\n",
      "Epoch 62/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 109.7394 - val_loss: 252.6271\n",
      "Epoch 63/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 111.0332 - val_loss: 243.8709\n",
      "Epoch 64/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 111.0276 - val_loss: 256.6451\n",
      "Epoch 65/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 106.1076 - val_loss: 236.1189\n",
      "Epoch 66/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 108.8350 - val_loss: 248.1944\n",
      "Epoch 67/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 107.5111 - val_loss: 287.3828\n",
      "Epoch 68/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 101.9764 - val_loss: 255.7877\n",
      "Epoch 69/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 111.4075 - val_loss: 287.9797\n",
      "Epoch 70/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 105.6157 - val_loss: 246.3623\n",
      "Epoch 71/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 106.6656 - val_loss: 263.0547\n",
      "Epoch 72/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 108.1698 - val_loss: 259.3735\n",
      "Epoch 73/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 106.6342 - val_loss: 296.9519\n",
      "Epoch 74/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 102.5930 - val_loss: 247.8273\n",
      "Epoch 75/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 101.5528 - val_loss: 246.7896\n",
      "Epoch 76/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 99.4881 - val_loss: 244.3604\n",
      "Epoch 77/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 100.8927 - val_loss: 270.1480\n",
      "Epoch 78/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 104.2598 - val_loss: 314.3500\n",
      "Epoch 79/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 100.6893 - val_loss: 230.9835\n",
      "Epoch 80/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 100.4083 - val_loss: 237.9799\n",
      "Epoch 81/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 100.0792 - val_loss: 243.3851\n",
      "Epoch 82/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 98.1129 - val_loss: 258.5304\n",
      "Epoch 83/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 98.6516 - val_loss: 233.2370\n",
      "Epoch 84/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 98.4865 - val_loss: 272.5070\n",
      "Epoch 85/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 97.9096 - val_loss: 262.1739\n",
      "Epoch 86/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 98.9039 - val_loss: 284.8485\n",
      "Epoch 87/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 97.5041 - val_loss: 273.7543\n",
      "Epoch 88/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 96.2004 - val_loss: 250.2126\n",
      "Epoch 89/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 96.1132 - val_loss: 239.1440\n",
      "Epoch 90/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 93.8947 - val_loss: 313.5793\n",
      "Epoch 91/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 95.1602 - val_loss: 287.3868\n",
      "Epoch 92/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 94.6656 - val_loss: 246.5213\n",
      "Epoch 93/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 95.1695 - val_loss: 248.6836\n",
      "Epoch 94/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 96.0012 - val_loss: 245.0079\n",
      "Epoch 95/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 96.7928 - val_loss: 277.2918\n",
      "Epoch 96/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 92.9391 - val_loss: 245.3468\n",
      "Epoch 97/187\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 92.6986 - val_loss: 240.2047\n",
      "Epoch 98/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 94.1280 - val_loss: 234.9857\n",
      "Epoch 99/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 94.2863 - val_loss: 273.3671\n",
      "Epoch 100/187\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 89.4367 - val_loss: 244.3755\n",
      "Epoch 101/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 94.0364 - val_loss: 244.9954\n",
      "Epoch 102/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 89.9821 - val_loss: 259.8141\n",
      "Epoch 103/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 90.7307 - val_loss: 248.6676\n",
      "Epoch 104/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 93.1073 - val_loss: 254.2131\n",
      "Epoch 105/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 90.6005 - val_loss: 222.0663\n",
      "Epoch 106/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 88.7620 - val_loss: 266.8717\n",
      "Epoch 107/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 89.8971 - val_loss: 247.7619\n",
      "Epoch 108/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 91.0566 - val_loss: 251.2364\n",
      "Epoch 109/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 86.9495 - val_loss: 298.4276\n",
      "Epoch 110/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 90.1024 - val_loss: 286.2675\n",
      "Epoch 111/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 90.9911 - val_loss: 221.5218\n",
      "Epoch 112/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 91.0645 - val_loss: 280.6958\n",
      "Epoch 113/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 90.7219 - val_loss: 230.4089\n",
      "Epoch 114/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 87.6820 - val_loss: 228.6051\n",
      "Epoch 115/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 87.7139 - val_loss: 267.4532\n",
      "Epoch 116/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 90.1519 - val_loss: 235.3735\n",
      "Epoch 117/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 87.6325 - val_loss: 266.5104\n",
      "Epoch 118/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 92.8526 - val_loss: 294.2074\n",
      "Epoch 119/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 89.7267 - val_loss: 244.7610\n",
      "Epoch 120/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 89.1017 - val_loss: 238.7508\n",
      "Epoch 121/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 88.6351 - val_loss: 270.9473\n",
      "Epoch 122/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 87.1082 - val_loss: 228.1885\n",
      "Epoch 123/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 86.9016 - val_loss: 252.9319\n",
      "Epoch 124/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 88.7226 - val_loss: 259.2661\n",
      "Epoch 125/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 93.5491 - val_loss: 271.0255\n",
      "Epoch 126/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 90.4866 - val_loss: 239.6844\n",
      "Epoch 127/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 86.5746 - val_loss: 350.9738\n",
      "Epoch 128/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 88.4343 - val_loss: 219.4790\n",
      "Epoch 129/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 86.9394 - val_loss: 265.6076\n",
      "Epoch 130/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 86.3478 - val_loss: 274.5716\n",
      "Epoch 131/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 86.9301 - val_loss: 253.3419\n",
      "Epoch 132/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 88.3712 - val_loss: 213.8737\n",
      "Epoch 133/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 85.6460 - val_loss: 249.8468\n",
      "Epoch 134/187\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 85.8768 - val_loss: 233.0746\n",
      "Epoch 135/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 84.3725 - val_loss: 257.7893\n",
      "Epoch 136/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 84.1291 - val_loss: 276.3291\n",
      "Epoch 137/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 85.7627 - val_loss: 226.8468\n",
      "Epoch 138/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 85.0421 - val_loss: 202.6515\n",
      "Epoch 139/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 86.0251 - val_loss: 241.3580\n",
      "Epoch 140/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 85.0325 - val_loss: 223.8097\n",
      "Epoch 141/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 86.0629 - val_loss: 216.9394\n",
      "Epoch 142/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 83.2895 - val_loss: 206.1880\n",
      "Epoch 143/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 85.1398 - val_loss: 216.1888\n",
      "Epoch 144/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 83.6253 - val_loss: 246.4567\n",
      "Epoch 145/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 85.9571 - val_loss: 237.9333\n",
      "Epoch 146/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 85.4809 - val_loss: 225.6998\n",
      "Epoch 147/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 88.1281 - val_loss: 328.9728\n",
      "Epoch 148/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 88.7478 - val_loss: 222.2899\n",
      "Epoch 149/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 85.6088 - val_loss: 296.5927\n",
      "Epoch 150/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 84.8100 - val_loss: 222.1832\n",
      "Epoch 151/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 85.3396 - val_loss: 227.5581\n",
      "Epoch 152/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 84.4897 - val_loss: 237.5797\n",
      "Epoch 153/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 84.6595 - val_loss: 230.9227\n",
      "Epoch 154/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 81.4116 - val_loss: 217.7326\n",
      "Epoch 155/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 80.4617 - val_loss: 256.1146\n",
      "Epoch 156/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 84.4304 - val_loss: 235.7419\n",
      "Epoch 157/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 85.4841 - val_loss: 226.4262\n",
      "Epoch 158/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 87.3378 - val_loss: 251.1850\n",
      "Epoch 159/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 84.9203 - val_loss: 232.7148\n",
      "Epoch 160/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 81.0201 - val_loss: 235.8946\n",
      "Epoch 161/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 83.8824 - val_loss: 289.1544\n",
      "Epoch 162/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 83.6904 - val_loss: 251.6913\n",
      "Epoch 163/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 83.2653 - val_loss: 238.6575\n",
      "Epoch 164/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 86.0267 - val_loss: 259.1423\n",
      "Epoch 165/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 83.3278 - val_loss: 237.4990\n",
      "Epoch 166/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 85.1870 - val_loss: 244.6788\n",
      "Epoch 167/187\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 84.3739 - val_loss: 227.9191\n",
      "Epoch 168/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 81.9296 - val_loss: 230.4175\n",
      "Epoch 169/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 84.7487 - val_loss: 221.9981\n",
      "Epoch 170/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 85.1452 - val_loss: 260.5822\n",
      "Epoch 171/187\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 82.7255 - val_loss: 259.5507\n",
      "Epoch 172/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 84.7444 - val_loss: 321.4414\n",
      "Epoch 173/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 85.9202 - val_loss: 210.1369\n",
      "Epoch 174/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 83.3117 - val_loss: 215.7729\n",
      "Epoch 175/187\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 85.5334 - val_loss: 271.3239\n",
      "Epoch 176/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 85.0992 - val_loss: 269.3852\n",
      "Epoch 177/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 80.2853 - val_loss: 230.2843\n",
      "Epoch 178/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 83.4075 - val_loss: 205.2088\n",
      "Epoch 179/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 85.0219 - val_loss: 215.5668\n",
      "Epoch 180/187\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 80.7959 - val_loss: 218.7431\n",
      "Epoch 181/187\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 81.3019 - val_loss: 225.9773\n",
      "Epoch 182/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 86.5116 - val_loss: 207.1634\n",
      "Epoch 183/187\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 82.8264 - val_loss: 222.2416\n",
      "Epoch 184/187\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 85.3013 - val_loss: 248.3148\n",
      "Epoch 185/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 83.2988 - val_loss: 263.9492\n",
      "Epoch 186/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 83.7133 - val_loss: 215.5816\n",
      "Epoch 187/187\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 83.4557 - val_loss: 226.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x162312e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "\n",
    "best_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=best_params['n_epochs'],\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    #callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model and its weights for future usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wout_vp/venv-metal/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "save_weights_at = os.path.join(\n",
    "    'Transformers_Best_Models_and_weights',\n",
    "    f'best_model_weights_{best_loss:.4f}.hdf5'\n",
    ")\n",
    "\n",
    "best_model.save(save_weights_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the saved model of choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_model_path = 'Transformers_Best_Models_and_weights/TestSet_Multivariate_monthly_best_model_weights_656.3905.hdf5'\n",
    "saved_model = load_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcx99VJ_hF-7"
   },
   "source": [
    "# Forecasting Validation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "ZXHjqYH5Jjl-",
    "outputId": "dffd0a67-0d09-4024-d968-bbec2312d0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 8ms/step\n",
      "                      MAE    RMSE  MAPE     R2\n",
      "Feature cement_co2  642.3  681.91  1.02 -151.2\n",
      "----------------------------------------\n",
      "                MAE    RMSE  MAPE    R2\n",
      "Feature co2  935.16  949.45  0.03  0.92\n",
      "----------------------------------------\n",
      "                     MAE     RMSE  MAPE    R2\n",
      "Feature coal_co2  968.42  1048.89  0.07  0.76\n",
      "----------------------------------------\n",
      "                       MAE  RMSE  MAPE    R2\n",
      "Feature flaring_co2  47.65  54.1  0.15 -1.15\n",
      "----------------------------------------\n",
      "                     MAE     RMSE  MAPE   R2\n",
      "Feature gas_co2  1355.89  1390.45  0.27 -4.3\n",
      "----------------------------------------\n",
      "                                MAE    RMSE  MAPE     R2\n",
      "Feature land_use_change_co2  900.45  954.96  0.16 -12.81\n",
      "----------------------------------------\n",
      "                     MAE     RMSE  MAPE    R2\n",
      "Feature oil_co2  1363.47  1451.93   0.1 -1.26\n",
      "----------------------------------------\n",
      "                               MAE    RMSE   MAPE     R2\n",
      "Feature other_industry_co2  370.28  381.85  10.55 -34.98\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pred = saved_model.predict(x_test)\n",
    "\n",
    "feature_columns = test_set.columns\n",
    "\n",
    "for col_idx, column_name in enumerate(feature_columns):\n",
    "    feature_pred = pred[:, col_idx]\n",
    "    feature_actual = y_test[:, col_idx]\n",
    "\n",
    "    metrics_df = print_metrics(feature_pred, feature_actual, f'Feature {column_name}')\n",
    "\n",
    "    print(metrics_df)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lC_iLaBJN6eQ",
    "outputId": "6a406bb9-067b-4223-e4d2-2a79e0e20d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cement_co2                        co2                    coal_co2  \\\n",
      "            Prediction       Actual    Prediction        Actual    Prediction   \n",
      "year                                                                            \n",
      "2000-07-01  515.518127   738.129500  25040.093750  25560.836000   9781.655273   \n",
      "2000-08-01  515.957764   740.829917  25056.294922  25578.704833   9785.941406   \n",
      "2000-09-01  516.395508   743.530333  25072.490234  25596.573667   9790.231445   \n",
      "2000-10-01  516.835693   746.230750  25088.693359  25614.442500   9794.520508   \n",
      "2000-11-01  517.275208   748.931167  25104.892578  25632.311333   9798.808594   \n",
      "...                ...          ...           ...           ...           ...   \n",
      "2020-09-01  684.656555  1660.907000  36030.539062  37116.969833  16535.253906   \n",
      "2020-10-01  685.786499  1663.828250  36025.976562  37118.690375  16538.875000   \n",
      "2020-11-01  686.915588  1666.749500  36021.417969  37120.410917  16542.501953   \n",
      "2020-12-01  688.042114  1669.670750  36016.859375  37122.131458  16546.126953   \n",
      "2021-01-01  689.171875  1672.592000  36012.292969  37123.852000  16549.744141   \n",
      "\n",
      "                         flaring_co2                  gas_co2               \\\n",
      "                  Actual  Prediction      Actual   Prediction       Actual   \n",
      "year                                                                         \n",
      "2000-07-01   9229.187000  315.272308  280.028000  3886.891357  4752.438000   \n",
      "2000-08-01   9233.532000  314.562531  280.211333  3889.873535  4755.962500   \n",
      "2000-09-01   9237.877000  313.852997  280.394667  3892.858643  4759.487000   \n",
      "2000-10-01   9242.222000  313.144104  280.578000  3895.839844  4763.011500   \n",
      "2000-11-01   9246.567000  312.436615  280.761333  3898.824463  4766.536000   \n",
      "...                  ...         ...         ...          ...          ...   \n",
      "2020-09-01  14711.253333  413.349915  413.545333  5869.231934  7799.983333   \n",
      "2020-10-01  14778.339500  401.947205  414.290500  5884.524902  7830.445000   \n",
      "2020-11-01  14845.425667  390.548523  415.035667  5899.819824  7860.906667   \n",
      "2020-12-01  14912.511833  379.145508  415.780833  5915.112793  7891.368333   \n",
      "2021-01-01  14979.598000  367.742462  416.526000  5930.405762  7921.830000   \n",
      "\n",
      "           land_use_change_co2                    oil_co2                \\\n",
      "                    Prediction       Actual    Prediction        Actual   \n",
      "year                                                                      \n",
      "2000-07-01         5378.831055  4927.469500  11012.102539  10346.380000   \n",
      "2000-08-01         5348.528809  4896.060917  11020.646484  10353.761500   \n",
      "2000-09-01         5318.226074  4864.652333  11029.187500  10361.143000   \n",
      "2000-10-01         5287.920898  4833.243750  11037.732422  10368.524500   \n",
      "2000-11-01         5257.619141  4801.835167  11046.279297  10375.906000   \n",
      "...                        ...          ...           ...           ...   \n",
      "2020-09-01         5521.355469  3978.615333  13954.645508  11622.042333   \n",
      "2020-10-01         5475.534668  3968.722500  13953.903320  11675.821500   \n",
      "2020-11-01         5429.718262  3958.829667  13953.161133  11729.600667   \n",
      "2020-12-01         5383.891602  3948.936833  13952.418945  11783.379833   \n",
      "2021-01-01         5338.077148  3939.044000  13951.673828  11837.159000   \n",
      "\n",
      "           other_industry_co2             \n",
      "                   Prediction     Actual  \n",
      "year                                      \n",
      "2000-07-01          27.385826  214.67250  \n",
      "2000-08-01          26.430147  214.40675  \n",
      "2000-09-01          25.473255  214.14100  \n",
      "2000-10-01          24.518665  213.87525  \n",
      "2000-11-01          23.562943  213.60950  \n",
      "...                       ...        ...  \n",
      "2020-09-01        -172.911301  296.19800  \n",
      "2020-10-01        -175.192978  296.18500  \n",
      "2020-11-01        -177.474640  296.17200  \n",
      "2020-12-01        -179.755524  296.15900  \n",
      "2021-01-01        -182.036301  296.14600  \n",
      "\n",
      "[247 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "results_of_features_df = []\n",
    "\n",
    "for idx, column in enumerate(feature_columns):\n",
    "    pred_for_feature = pred[:, idx]\n",
    "    y_test_for_feature = y_test[:, idx]\n",
    "\n",
    "    result_df = pd.DataFrame({'Prediction': pred_for_feature, 'Actual': y_test_for_feature})\n",
    "    result_df.index = test_set.index[-247:]\n",
    "\n",
    "    results_of_features_df.append(result_df)\n",
    "\n",
    "# combining the dataframes together\n",
    "combined_results_df = pd.concat(results_of_features_df, axis=1, keys=feature_columns)\n",
    "\n",
    "print(combined_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "2IK1HkOfGXn7",
    "outputId": "c538fee4-6018-437e-e16c-c6a23f0cb2c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 10))\n",
    "\n",
    "for idx, column in enumerate(feature_columns):\n",
    "    result_df = results_of_features_df[idx]\n",
    "    \n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    ax.plot(result_df.index, result_df['Prediction'], label='Prediction')\n",
    "    ax.plot(result_df.index, result_df['Actual'], label='Actual')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Predicted vs Actual Values for {column}')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__bD63WVhODI",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Forecasting Validation on the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "GDIuZlCVgC_7",
    "outputId": "4a529972-711d-4b66-f308-f5066ef3c7b5"
   },
   "outputs": [],
   "source": [
    "val_pred = saved_model.predict(x_val)\n",
    "\n",
    "val_feature_columns = validate_set.columns\n",
    "\n",
    "for col_idx, column_name in enumerate(val_feature_columns):\n",
    "    feature_pred = val_pred[:, col_idx]\n",
    "    feature_actual = y_val[:, col_idx]\n",
    "\n",
    "    metrics_df = print_metrics(feature_pred, feature_actual, f'Feature {column_name}')\n",
    "\n",
    "    print(metrics_df)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuitQMzLhNG9",
    "outputId": "c6cc0eb4-bfad-4fa4-b91b-09f5a83e35d0"
   },
   "outputs": [],
   "source": [
    "results_of_features_val = []\n",
    "\n",
    "for idx, column in enumerate(feature_columns):\n",
    "    pred_val_for_feature = val_pred[:, idx]\n",
    "    y_val_for_feature = y_val[:, idx]\n",
    "\n",
    "    result_df_val = pd.DataFrame({'Prediction': pred_val_for_feature, 'Actual': y_val_for_feature})\n",
    "    result_df_val.index = validate_set.index[-5:]\n",
    "\n",
    "    results_of_features_val.append(result_df_val)\n",
    "\n",
    "# combining the dataframes together for the validation set\n",
    "combined_results_df_val = pd.concat(results_of_features_val, axis=1, keys=feature_columns)\n",
    "\n",
    "print(combined_results_df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "DUmIMhEIhNIT",
    "outputId": "5812e7b7-c0b8-4fce-961b-5da385673f1f"
   },
   "outputs": [],
   "source": [
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 10))\n",
    "\n",
    "for idx, column in enumerate(feature_columns):\n",
    "    result_df = results_of_features_val[idx]\n",
    "    \n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    ax.plot(result_df.index, result_df['Prediction'], label='Prediction')\n",
    "    ax.plot(result_df.index, result_df['Actual'], label='Actual')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Predicted vs Actual Values for {column}')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zee9DSG4hNJm"
   },
   "source": [
    "# Ensembling of the 2 Best Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = load_model('Transformers_Best_Models_and_weights/Multivariate_best_model_weights_939.8892.hdf5')\n",
    "model_b = load_model('Transformers_Best_Models_and_weights/Multivariate_best_model_weights_727.0049.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = model_a.predict(x_test)\n",
    "pred_b = model_b.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_a = model_a.predict(x_val)\n",
    "val_pred_b = model_b.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_model_a = 0.4\n",
    "weight_model_b = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred = (weight_model_a * pred_a) + (weight_model_b * pred_b)\n",
    "\n",
    "ensemble_pred_val = ((weight_model_a * val_pred_a) + (weight_model_b * val_pred_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting using the Ensembled Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(ensemble_pred, y_test, 'Transformer_Ensembled_Scores_TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(ensemble_pred_val, y_val, 'Transformer_Ensembled_Scores_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensembled_flat = ensemble_pred.flatten()\n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "# Create a DataFrame to compare predictions and actual values\n",
    "ensembled_result_df = pd.DataFrame({'Prediction': pred_ensembled_flat, 'Actual': y_test_flat})\n",
    "\n",
    "ensembled_result_df.index = test_set.index[-6:]\n",
    "\n",
    "print(\"TEST Set:\")\n",
    "print(\"\\n\")\n",
    "print(ensembled_result_df)\n",
    "\n",
    "pred_ensembled_val_flat = ensemble_pred_val.flatten()\n",
    "y_val_flat = y_val.flatten()\n",
    "\n",
    "# Create a DataFrame to compare predictions and actual values\n",
    "ensembled_result_val_df = pd.DataFrame({'Prediction': pred_ensembled_val_flat, 'Actual': y_val_flat})\n",
    "\n",
    "ensembled_result_val_df.index = test_set.index[-5:]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"VAL Set:\")\n",
    "print(\"\\n\")\n",
    "print(ensembled_result_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(result_df.index, ensembled_result_df['Prediction'], label='Prediction')\n",
    "plt.plot(result_df.index, ensembled_result_df['Actual'], label='Actual')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test Set: Predicted vs Actual Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(result_df_val.index, ensembled_result_val_df['Prediction'], label='Prediction')\n",
    "plt.plot(result_df_val.index, ensembled_result_val_df['Actual'], label='Actual')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Val set: Predicted vs Actual Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_QA2gkAI7yqR",
    "bFmhe3p171U-",
    "0JRtDG_sdx7D",
    "ZRwUSHvpdp0O",
    "Tgq1ukssXFX9",
    "huSns1vJdlkO",
    "nlePrpfKBgtB"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
